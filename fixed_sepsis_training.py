#!/usr/bin/env python3

import os
# è§£å†³macOSä¸Šçš„OpenMPå†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from collections import defaultdict
import pickle
import warnings
warnings.filterwarnings('ignore')

class EnhancedSepsisLSTMAgent(nn.Module):
    """å¢å¼ºçš„è´¥è¡€ç—‡LSTMæ™ºèƒ½ä½“ - åŒ…å«åŒ»å­¦çº¦æŸ"""
    
    def __init__(self, input_dim, hidden_dim=128, num_actions=20, sequence_length=7):
        super(EnhancedSepsisLSTMAgent, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_actions = num_actions
        self.sequence_length = sequence_length
        
        # å¤šå±‚LSTMç”¨äºæ›´å¥½çš„æ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=False
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºå…³æ³¨é‡è¦çš„æ—¶é—´æ­¥
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # Layer normalization for stability
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        
        # åŒ»å­¦ç‰¹å¾æå–å™¨
        self.medical_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim//2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim//2),
            nn.Dropout(0.1)
        )
        
        # å¢å¼ºçš„Actorç½‘ç»œ
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, num_actions)
        )
        
        # å¢å¼ºçš„Criticç½‘ç»œ
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, 1)
        )
        
        # åŒ»å­¦çº¦æŸå±‚
        self.medical_constraint = nn.Linear(num_actions, num_actions)
        
        # ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                module.bias.data.fill_(0.01)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    torch.nn.init.xavier_uniform_(param.data, gain=0.5)
                elif 'weight_hh' in name:
                    torch.nn.init.orthogonal_(param.data, gain=0.5)
                elif 'bias' in name:
                    param.data.fill_(0.01)
    
    def forward(self, sequences, sequence_lengths=None):
        """å‰å‘ä¼ æ’­"""
        batch_size = sequences.size(0)
        
        # LSTMå¤„ç†æ—¶åºæ•°æ®
        lstm_output, (h_n, c_n) = self.lstm(sequences)
        
        # Layer normalization
        lstm_output = self.layer_norm1(lstm_output)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attended_output, attention_weights = self.attention(
            lstm_output, lstm_output, lstm_output
        )
        
        # ç»“åˆLSTMå’Œæ³¨æ„åŠ›è¾“å‡º
        combined_output = lstm_output + attended_output
        combined_output = self.layer_norm2(combined_output)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        if sequence_lengths is not None:
            last_outputs = []
            for i, length in enumerate(sequence_lengths):
                last_outputs.append(combined_output[i, length-1, :])
            temporal_context = torch.stack(last_outputs)
        else:
            temporal_context = combined_output[:, -1, :]
        
        # åŒ»å­¦ç‰¹å¾æå–ï¼ˆä½¿ç”¨æœ€æ–°çš„åŒ»å­¦æ•°æ®ï¼‰
        current_medical_state = sequences[:, -1, :]
        medical_features = self.medical_feature_extractor(current_medical_state)
        
        # èåˆæ—¶åºå’ŒåŒ»å­¦ç‰¹å¾
        combined_features = torch.cat([temporal_context, medical_features], dim=1)
        
        # Actorå’ŒCriticè¾“å‡º
        raw_action_logits = self.actor(combined_features)
        
        # åº”ç”¨åŒ»å­¦çº¦æŸ
        constrained_action_logits = self.medical_constraint(raw_action_logits)
        
        state_values = self.critic(combined_features)
        
        return constrained_action_logits, state_values, combined_output, attention_weights

class ClinicalRewardCalculator:
    """ä¸´åºŠç›¸å…³çš„å¥–åŠ±è®¡ç®—å™¨ - åŸºäºåŒ»å­¦æŒ‡æ ‡çš„å¥–åŠ±"""
    
    def __init__(self):
        # é‡æ–°è®¾è®¡çš„ä¸´åºŠæŒ‡æ ‡æƒé‡ - æ›´åŠ æ³¨é‡SOFAè¯„åˆ†å’Œç”Ÿå‘½ä½“å¾æ”¹å–„
        self.vital_signs_weight = 0.4  # å¢åŠ ç”Ÿå‘½ä½“å¾æƒé‡
        self.sofa_weight = 0.5         # å¢åŠ SOFAè¯„åˆ†æƒé‡
        self.stability_weight = 0.15   # é€‚å½“é™ä½ç¨³å®šæ€§æƒé‡
        self.safety_weight = 0.05      # é™ä½å®‰å…¨æƒé‡ï¼Œé¿å…è¿‡åº¦ä¿å®ˆ
        
        # æ­£å¸¸å€¼èŒƒå›´ (åŸºäºä¸´åºŠæ ‡å‡†) - æ‰©å¤§ä¸€äº›èŒƒå›´ä»¥å¢åŠ æ•æ„Ÿåº¦
        self.normal_ranges = {
            'temperature': (35.5, 38.0),   # ç¨å¾®æ‰©å¤§æ¸©åº¦èŒƒå›´
            'heart_rate': (50, 110),       # æ‰©å¤§å¿ƒç‡èŒƒå›´
            'respiratory_rate': (10, 24),  # æ‰©å¤§å‘¼å¸é¢‘ç‡èŒƒå›´
            'arterial_blood_pressure_systolic': (85, 150),
            'arterial_blood_pressure_diastolic': (55, 95),
            'o2_saturation_pulseoxymetry': (92, 100),  # ç¨å¾®é™ä½æ°§é¥±å’Œåº¦ä¸‹é™
            'glucose_(serum)': (60, 200),  # æ‰©å¤§è¡€ç³–èŒƒå›´
            'gcs_-_eye_opening': (3, 4),
            'gcs_-_motor_response': (4, 6),  # æ‰©å¤§è¿åŠ¨ååº”èŒƒå›´
            'gcs_-_verbal_response': (3, 5)  # æ‰©å¤§è¨€è¯­ååº”èŒƒå›´
        }
        
    def calculate_vital_signs_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾çš„å¥–åŠ±è®¡ç®— - æ”¹è¿›ç‰ˆ"""
        batch_size = features.size(0)
        rewards = torch.zeros(batch_size)
        
        # ç‰¹å¾åç§°æ˜ å°„åˆ°ç´¢å¼•
        feature_names = ['temperature', 'heart_rate', 'respiratory_rate', 
                        'arterial_blood_pressure_systolic', 'arterial_blood_pressure_diastolic',
                        'o2_saturation_pulseoxymetry', 'glucose_(serum)', 'gcs_-_eye_opening',
                        'gcs_-_motor_response', 'gcs_-_verbal_response', 'age', 'sofa_score']
        
        # è®¡ç®—æ—¶åºæ”¹å–„å¥–åŠ± - å…³æ³¨ç”Ÿå‘½ä½“å¾çš„æ”¹å–„è¶‹åŠ¿
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            current_values = features[:, -1, :10]  # å½“å‰æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            previous_values = features[:, -2, :10]  # å‰ä¸€æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            
            improvement_reward = 0.0
            for i, feature_name in enumerate(feature_names[:10]):
                if feature_name in self.normal_ranges:
                    normal_min, normal_max = self.normal_ranges[feature_name]
                    normal_center = (normal_min + normal_max) / 2
                    
                    # è®¡ç®—å½“å‰å’Œä¹‹å‰è·ç¦»æ­£å¸¸ä¸­å¿ƒçš„è·ç¦»
                    current_distance = torch.abs(current_values[:, i] - normal_center)
                    previous_distance = torch.abs(previous_values[:, i] - normal_center)
                    
                    # æ”¹å–„å¥–åŠ± - è·ç¦»æ­£å¸¸å€¼æ›´è¿‘è·å¾—æ­£å¥–åŠ±
                    improvement = previous_distance - current_distance
                    normalized_improvement = improvement / (normal_max - normal_min)
                    improvement_reward += normalized_improvement * 0.3  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.3å¥–åŠ±
            
            rewards += improvement_reward
        
        # å½“å‰çŠ¶æ€çš„æ­£å¸¸æ€§å¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„è®¡ç®—æ–¹å¼
        current_state_reward = 0.0
        for i, feature_name in enumerate(feature_names[:10]):
            if feature_name in self.normal_ranges:
                normal_min, normal_max = self.normal_ranges[feature_name]
                values = features[:, -1, i]  # å½“å‰æ—¶é—´æ­¥çš„å€¼
                
                # ä½¿ç”¨åˆ†æ®µçº¿æ€§å‡½æ•°è€ŒéæŒ‡æ•°å‡½æ•°ï¼Œå¢åŠ æ•æ„Ÿåº¦
                in_range_mask = (values >= normal_min) & (values <= normal_max)
                below_range_mask = values < normal_min
                above_range_mask = values > normal_max
                
                # åœ¨æ­£å¸¸èŒƒå›´å†…ï¼šæ»¡åˆ†
                feature_reward = torch.zeros_like(values)
                feature_reward[in_range_mask] = 1.0
                
                # åœ¨èŒƒå›´å¤–ï¼šçº¿æ€§é€’å‡ï¼Œè€ŒéæŒ‡æ•°é€’å‡
                range_width = normal_max - normal_min
                feature_reward[below_range_mask] = torch.clamp(
                    1.0 - (normal_min - values[below_range_mask]) / range_width, 
                    min=0.0, max=1.0
                )
                feature_reward[above_range_mask] = torch.clamp(
                    1.0 - (values[above_range_mask] - normal_max) / range_width, 
                    min=0.0, max=1.0
                )
                
                current_state_reward += feature_reward * 0.15  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.15å¥–åŠ±
        
        rewards += current_state_reward
        return rewards
    
    def calculate_sofa_reward(self, features):
        """åŸºäºSOFAè¯„åˆ†çš„å¥–åŠ± - æ”¹è¿›ç‰ˆï¼Œæ›´æ³¨é‡æ”¹å–„è¶‹åŠ¿"""
        current_sofa = features[:, -1, -1]  # å½“å‰SOFAè¯„åˆ†
        
        # åŸºç¡€SOFAå¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„åˆ†æ®µå‡½æ•°
        base_sofa_reward = torch.zeros_like(current_sofa)
        
        # ä¼˜ç§€çŠ¶æ€ (SOFA 0-6): é«˜å¥–åŠ±
        excellent_mask = current_sofa <= 6
        base_sofa_reward[excellent_mask] = 2.0 - current_sofa[excellent_mask] / 6.0
        
        # ä¸­ç­‰çŠ¶æ€ (SOFA 7-12): ä¸­ç­‰å¥–åŠ±
        moderate_mask = (current_sofa > 6) & (current_sofa <= 12)
        base_sofa_reward[moderate_mask] = 1.0 - (current_sofa[moderate_mask] - 6) / 12.0
        
        # ä¸¥é‡çŠ¶æ€ (SOFA 13-18): ä½å¥–åŠ±
        severe_mask = (current_sofa > 12) & (current_sofa <= 18)
        base_sofa_reward[severe_mask] = 0.5 - (current_sofa[severe_mask] - 12) / 12.0
        
        # å±é‡çŠ¶æ€ (SOFA >18): æä½å¥–åŠ±
        critical_mask = current_sofa > 18
        base_sofa_reward[critical_mask] = 0.1
        
        # SOFAæ”¹å–„å¥–åŠ± - å…³æ³¨è¯„åˆ†çš„æ”¹å–„
        improvement_reward = torch.zeros_like(current_sofa)
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            previous_sofa = features[:, -2, -1]
            sofa_improvement = previous_sofa - current_sofa  # è¯„åˆ†é™ä½æ˜¯å¥½äº‹
            
            # æ”¹å–„å¥–åŠ±ï¼šæ¯é™ä½1åˆ†SOFAè¯„åˆ†è·å¾—0.5å¥–åŠ±
            improvement_reward = torch.clamp(sofa_improvement * 0.5, min=-1.0, max=2.0)
        
        total_sofa_reward = base_sofa_reward + improvement_reward
        return torch.clamp(total_sofa_reward, min=0.0, max=3.0)
    
    def calculate_stability_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾ç¨³å®šæ€§çš„å¥–åŠ±"""
        if features.size(1) < 2:  # åºåˆ—é•¿åº¦å°äº2ï¼Œæ— æ³•è®¡ç®—ç¨³å®šæ€§
            return torch.zeros(features.size(0))
        
        # è®¡ç®—ç”Ÿå‘½ä½“å¾çš„å˜åŒ–ç‡
        vital_signs = features[:, :, :10]  # å‰10ä¸ªç‰¹å¾æ˜¯ç”Ÿå‘½ä½“å¾
        changes = torch.abs(vital_signs[:, 1:] - vital_signs[:, :-1])
        
        # ç¨³å®šæ€§å¥–åŠ±ï¼šå˜åŒ–è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
        stability = torch.exp(-changes.mean(dim=(1, 2)))
        return stability
    
    def calculate_safety_constraint_reward(self, action_logits):
        """åŸºäºå®‰å…¨çº¦æŸçš„å¥–åŠ±"""
        action_probs = torch.softmax(action_logits, dim=-1)
        
        # é¼“åŠ±é€‚åº¦çš„åŠ¨ä½œé€‰æ‹©ï¼ˆé¿å…æç«¯æ²»ç–—ï¼‰
        entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        target_entropy = np.log(action_logits.size(-1)) * 0.6  # ç›®æ ‡ç†µ
        
        # ç†µæ¥è¿‘ç›®æ ‡å€¼æ—¶å¥–åŠ±æœ€é«˜
        entropy_reward = torch.exp(-torch.abs(entropy - target_entropy))
        return entropy_reward
    
    def calculate_reward(self, state_features, action_logits, episode_step=0):
        """
        ç»¼åˆä¸´åºŠå¥–åŠ±è®¡ç®— - æ”¹è¿›ç‰ˆï¼Œå…è®¸æ›´å¤§çš„å¥–åŠ±èŒƒå›´å’ŒåŠ¨æ€æ€§
        """
        # å„é¡¹å¥–åŠ±è®¡ç®—
        vital_reward = self.calculate_vital_signs_reward(state_features)
        sofa_reward = self.calculate_sofa_reward(state_features)
        stability_reward = self.calculate_stability_reward(state_features)
        safety_reward = self.calculate_safety_constraint_reward(action_logits)
        
        # åŠ æƒç»„åˆ - ä½¿ç”¨æ–°çš„æƒé‡
        total_reward = (
            self.vital_signs_weight * vital_reward +
            self.sofa_weight * sofa_reward +
            self.stability_weight * stability_reward +
            self.safety_weight * safety_reward
        )
        
        # æ”¹è¿›çš„ç”Ÿå­˜æ—¶é—´å¥–åŠ± - æ›´å…·åŠ¨æ€æ€§
        survival_bonus = torch.full_like(total_reward, min(episode_step * 0.02, 0.3))
        
        # æ·»åŠ éšæœºæ€§å¥–åŠ±ä»¥å¢åŠ æ¢ç´¢
        exploration_bonus = torch.randn_like(total_reward) * 0.05
        
        final_reward = total_reward + survival_bonus + exploration_bonus
        
        # æ‰©å¤§å¥–åŠ±èŒƒå›´ï¼Œç§»é™¤è¿‡äºä¸¥æ ¼çš„é™åˆ¶
        return torch.clamp(final_reward, min=-1.0, max=5.0)

class MedicalSafetyConstraints:
    """åŒ»å­¦å®‰å…¨çº¦æŸç±»"""
    
    def __init__(self, num_actions=20):
        self.num_actions = num_actions
        
        # å®šä¹‰å±é™©åŠ¨ä½œç»„åˆï¼ˆç¤ºä¾‹ï¼‰
        self.dangerous_combinations = [
            [0, 1],   # ç¤ºä¾‹ï¼šæŸäº›è¯ç‰©ç»„åˆ
            [5, 10],  # ç¤ºä¾‹ï¼šé«˜å‰‘é‡ç»„åˆ
        ]
        
        # æœ€å¤§å…è®¸å‰‘é‡
        self.max_dosage_actions = [15, 16, 17, 18, 19]  # ç¤ºä¾‹ï¼šé«˜å‰‚é‡åŠ¨ä½œ
        
    def apply_constraints(self, action_logits, medical_state=None):
        """åº”ç”¨åŒ»å­¦å®‰å…¨çº¦æŸ"""
        constrained_logits = action_logits.clone()
        
        # çº¦æŸ1ï¼šé˜²æ­¢å±é™©è¯ç‰©ç»„åˆ
        action_probs = torch.softmax(action_logits, dim=-1)
        
        for dangerous_combo in self.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            if combo_prob.max() > 0.1:  # å¦‚æœå±é™©ç»„åˆæ¦‚ç‡è¿‡é«˜
                for action_idx in dangerous_combo:
                    constrained_logits[:, action_idx] -= 2.0  # é™ä½æ¦‚ç‡
        
        # çº¦æŸ2ï¼šåŸºäºå½“å‰åŒ»å­¦çŠ¶æ€çš„çº¦æŸ
        if medical_state is not None:
            # å¦‚æœè¡€å‹è¿‡ä½ï¼Œé™åˆ¶é™å‹è¯ç‰©
            bp_systolic = medical_state[:, 3]  # æ”¶ç¼©å‹ç´¢å¼•
            low_bp_mask = bp_systolic < 90
            if low_bp_mask.any():
                constrained_logits[low_bp_mask, 10:15] -= 3.0  # é™åˆ¶é™å‹ç±»åŠ¨ä½œ
            
            # å¦‚æœå¿ƒç‡è¿‡é«˜ï¼Œé™åˆ¶å…´å¥‹å‰‚
            heart_rate = medical_state[:, 1]
            high_hr_mask = heart_rate > 120
            if high_hr_mask.any():
                constrained_logits[high_hr_mask, 5:10] -= 3.0  # é™åˆ¶å…´å¥‹å‰‚ç±»åŠ¨ä½œ
        
        return constrained_logits

class EnhancedTrainer:
    """å¢å¼ºçš„è®­ç»ƒå™¨ - åŒ…å«åŒ»å­¦å®‰å…¨çº¦æŸ"""
    
    def __init__(self, agent, reward_calculator, lr=5e-5, device='cpu'):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.agent = agent.to(device)
        self.reward_calculator = reward_calculator
        self.device = device
        self.safety_constraints = MedicalSafetyConstraints()
        
        # æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.AdamW(
            self.agent.parameters(), 
            lr=lr, 
            weight_decay=1e-4,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=3, verbose=True
        )
        
        self.mse_loss = nn.MSELoss()
        self.huber_loss = nn.SmoothL1Loss()  # æ›´ç¨³å®šçš„æŸå¤±å‡½æ•°
        self.global_step = 0
        
        # å¢å¼ºçš„å†å²è®°å½•
        self.history = {
            'train_total_loss': [],
            'train_actor_loss': [],
            'train_critic_loss': [],
            'train_reward': [],
            'val_loss': [],
            'val_reward': [],
            'learning_rate': [],
            'gradient_norm': [],
            'safety_violations': []
        }
    
    def prepare_batch(self, sequences, batch_size=8):
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡"""
        np.random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch_sequences = sequences[i:i + batch_size]
            
            # å †å ç‰¹å¾
            features = torch.tensor([seq['features'] for seq in batch_sequences], 
                                   dtype=torch.float32, device=self.device)
            
            # åºåˆ—é•¿åº¦
            seq_lengths = [seq['sequence_length'] for seq in batch_sequences]
            
            # æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼
            features = torch.clamp(features, min=-10, max=10)
            features = torch.where(torch.isnan(features), torch.zeros_like(features), features)
            
            yield features, seq_lengths
    
    def train_epoch(self, train_sequences, epoch):
        """å¢å¼ºçš„è®­ç»ƒepoch - åŒ…å«å®‰å…¨çº¦æŸå’Œæ”¹è¿›çš„æŸå¤±è®¡ç®—"""
        self.agent.train()
        epoch_actor_losses = []
        epoch_critic_losses = []
        epoch_rewards = []
        epoch_safety_violations = []
        epoch_gradient_norms = []
        
        for batch_idx, (features, seq_lengths) in enumerate(self.prepare_batch(train_sequences)):
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            action_logits, state_values, _, attention_weights = self.agent(features, seq_lengths)
            
            # åº”ç”¨åŒ»å­¦å®‰å…¨çº¦æŸ
            current_medical_state = features[:, -1, :]
            constrained_action_logits = self.safety_constraints.apply_constraints(
                action_logits, current_medical_state
            )
            
            # è®¡ç®—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            action_probs = torch.softmax(constrained_action_logits, dim=-1)
            
            # è®¡ç®—å¥–åŠ±
            rewards = self.reward_calculator.calculate_reward(
                features, constrained_action_logits, episode_step=self.global_step
            ).to(self.device)
            
            # æ›´ç¨³å®šçš„CriticæŸå¤±è®¡ç®—
            state_values_squeezed = state_values.squeeze()
            if state_values_squeezed.dim() == 0:
                state_values_squeezed = state_values_squeezed.unsqueeze(0)
            
            # ä½¿ç”¨HuberæŸå¤±æ›´ç¨³å®š
            critic_loss = self.huber_loss(state_values_squeezed, rewards)
            
            # è®¡ç®—advantageså¹¶æ ‡å‡†åŒ–
            advantages = rewards - state_values_squeezed.detach()
            if advantages.std() > 1e-6:
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # ç®€åŒ–çš„ActoræŸå¤±è®¡ç®— - æ ‡å‡†ç­–ç•¥æ¢¯åº¦
            action_dist = torch.distributions.Categorical(action_probs)
            sampled_actions = action_dist.sample()
            
            # è®¡ç®—é€‰ä¸­åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            log_probs = action_dist.log_prob(sampled_actions)
            
            # æ ‡å‡†ç­–ç•¥æ¢¯åº¦æŸå¤± (REINFORCE)
            policy_loss = -torch.mean(log_probs * advantages.detach())
            
            # ç†µæ­£åˆ™åŒ–
            entropy = action_dist.entropy().mean()
            entropy_bonus = 0.02 * entropy
            
            # æ³¨æ„åŠ›æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±å…³æ³¨é‡è¦æ—¶é—´æ­¥ï¼‰
            attention_penalty = 0.001 * torch.var(attention_weights).mean()
            
            # æœ€ç»ˆActoræŸå¤±
            actor_loss = policy_loss - entropy_bonus + attention_penalty
            
            # å®‰å…¨çº¦æŸæŸå¤±
            safety_violation = self.calculate_safety_violations(action_probs)
            safety_loss = 0.1 * safety_violation
            
            # æ€»æŸå¤±
            total_loss = critic_loss + actor_loss + safety_loss
            
            # æ£€æŸ¥NaN/Inf
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"NaN/Inf detected at epoch {epoch}, batch {batch_idx}. Skipping...")
                continue
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(self.agent.parameters(), max_norm=0.5)
            epoch_gradient_norms.append(grad_norm.item())
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            epoch_actor_losses.append(actor_loss.item())
            epoch_critic_losses.append(critic_loss.item())
            epoch_rewards.append(rewards.mean().item())
            epoch_safety_violations.append(safety_violation.item())
            
            self.global_step += 1
            
            if batch_idx % 5 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}: "
                      f"Actor Loss = {actor_loss.item():.6f}, "
                      f"Critic Loss = {critic_loss.item():.6f}, "
                      f"Avg Reward = {rewards.mean().item():.4f}, "
                      f"Entropy = {entropy.item():.4f}, "
                      f"Safety Violations = {safety_violation.item():.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_actor_loss = np.mean(epoch_actor_losses) if epoch_actor_losses else 0
        avg_critic_loss = np.mean(epoch_critic_losses) if epoch_critic_losses else 0
        avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0
        avg_safety_violations = np.mean(epoch_safety_violations) if epoch_safety_violations else 0
        avg_grad_norm = np.mean(epoch_gradient_norms) if epoch_gradient_norms else 0
        
        total_avg_loss = avg_actor_loss + avg_critic_loss
        
        # æ›´æ–°å†å²è®°å½•
        self.history['train_actor_loss'].append(avg_actor_loss)
        self.history['train_critic_loss'].append(avg_critic_loss)
        self.history['train_total_loss'].append(total_avg_loss)
        self.history['train_reward'].append(avg_reward)
        self.history['safety_violations'].append(avg_safety_violations)
        self.history['gradient_norm'].append(avg_grad_norm)
        self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])
        
        return total_avg_loss, avg_reward
    
    def calculate_safety_violations(self, action_probs):
        """è®¡ç®—å®‰å…¨è¿è§„æƒ…å†µ"""
        violations = torch.zeros(action_probs.size(0), device=self.device)
        
        # æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ
        for dangerous_combo in self.safety_constraints.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            violations += combo_prob  # å±é™©ç»„åˆçš„æ¦‚ç‡ä½œä¸ºè¿è§„æŒ‡æ ‡
        
        # æ£€æŸ¥é«˜å‰‚é‡åŠ¨ä½œ
        high_dose_probs = action_probs[:, self.safety_constraints.max_dosage_actions]
        violations += high_dose_probs.sum(dim=1) * 0.5  # é«˜å‰‚é‡åŠ¨ä½œæ€»æ¦‚ç‡
        
        return violations.mean()
    
    def validate(self, val_sequences):
        """å¢å¼ºçš„æ¨¡å‹éªŒè¯ - åŒ…å«ä¸´åºŠæŒ‡æ ‡"""
        self.agent.eval()
        val_losses = []
        val_rewards = []
        clinical_metrics = {
            'sofa_scores': [],
            'mortality_predictions': [],
            'treatment_effectiveness': [],
            'safety_scores': []
        }
        
        with torch.no_grad():
            for features, seq_lengths in self.prepare_batch(val_sequences):
                action_logits, state_values, _, attention_weights = self.agent(features, seq_lengths)
                
                # åº”ç”¨å®‰å…¨çº¦æŸ
                current_medical_state = features[:, -1, :]
                constrained_action_logits = self.safety_constraints.apply_constraints(
                    action_logits, current_medical_state
                )
                
                action_probs = torch.softmax(constrained_action_logits, dim=-1)
                
                rewards = self.reward_calculator.calculate_reward(
                    features, constrained_action_logits, episode_step=self.global_step
                ).to(self.device)
                
                state_values_squeezed = state_values.squeeze()
                if state_values_squeezed.dim() == 0:
                    state_values_squeezed = state_values_squeezed.unsqueeze(0)
                
                critic_loss = self.huber_loss(state_values_squeezed, rewards)
                
                # è®¡ç®—ActoræŸå¤±
                advantages = rewards - state_values_squeezed
                action_dist = torch.distributions.Categorical(action_probs)
                sampled_actions = action_dist.sample()
                log_probs = action_dist.log_prob(sampled_actions)
                actor_loss = -torch.mean(log_probs * advantages)
                
                total_loss = critic_loss.item() + actor_loss.item()
                
                if not (torch.isnan(torch.tensor(total_loss)) or torch.isinf(torch.tensor(total_loss))):
                    val_losses.append(total_loss)
                    val_rewards.append(rewards.mean().item())
                    
                    # æ”¶é›†ä¸´åºŠæŒ‡æ ‡
                    self.collect_clinical_metrics(features, action_probs, state_values, clinical_metrics)
        
        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')
        avg_val_reward = np.mean(val_rewards) if val_rewards else 0
        
        self.history['val_loss'].append(avg_val_loss)
        self.history['val_reward'].append(avg_val_reward)
        
        # è®¡ç®—ä¸´åºŠæŒ‡æ ‡
        clinical_summary = self.summarize_clinical_metrics(clinical_metrics)
        
        return avg_val_loss, avg_val_reward, clinical_summary
    
    def collect_clinical_metrics(self, features, action_probs, state_values, metrics):
        """æ”¶é›†ä¸´åºŠæŒ‡æ ‡"""
        # SOFAè¯„åˆ†å˜åŒ–
        sofa_scores = features[:, -1, -1]  # æœ€åä¸€ä¸ªç‰¹å¾æ˜¯SOFAè¯„åˆ†
        metrics['sofa_scores'].extend(sofa_scores.cpu().numpy())
        
        # æ­»äº¡é£é™©é¢„æµ‹ï¼ˆåŸºäºçŠ¶æ€å€¼ï¼‰
        mortality_risk = torch.sigmoid(-state_values.squeeze())  # çŠ¶æ€å€¼è¶Šä½ï¼Œæ­»äº¡é£é™©è¶Šé«˜
        metrics['mortality_predictions'].extend(mortality_risk.cpu().numpy())
        
        # æ²»ç–—æœ‰æ•ˆæ€§ï¼ˆåŸºäºåŠ¨ä½œé€‰æ‹©çš„é›†ä¸­åº¦ï¼‰
        action_entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        treatment_effectiveness = 1.0 / (1.0 + action_entropy)  # ç†µè¶Šä½ï¼Œæ²»ç–—è¶Šé›†ä¸­
        metrics['treatment_effectiveness'].extend(treatment_effectiveness.cpu().numpy())
        
        # å®‰å…¨è¯„åˆ†
        safety_scores = self.calculate_safety_scores(action_probs)
        metrics['safety_scores'].extend(safety_scores.cpu().numpy())
    
    def calculate_safety_scores(self, action_probs):
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        safety_scores = torch.ones(action_probs.size(0), device=self.device)
        
        # æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ
        for dangerous_combo in self.safety_constraints.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            safety_scores -= combo_prob  # å±é™©ç»„åˆæ¦‚ç‡è¶Šé«˜ï¼Œå®‰å…¨è¯„åˆ†è¶Šä½
        
        return torch.clamp(safety_scores, min=0.0, max=1.0)
    
    def summarize_clinical_metrics(self, metrics):
        """æ±‡æ€»ä¸´åºŠæŒ‡æ ‡"""
        summary = {}
        
        if metrics['sofa_scores']:
            sofa_scores = np.array(metrics['sofa_scores'])
            summary['avg_sofa_score'] = np.mean(sofa_scores)
            summary['sofa_improvement'] = max(0, 15 - np.mean(sofa_scores)) / 15  # æ ‡å‡†åŒ–æ”¹å–„æŒ‡æ ‡
        
        if metrics['mortality_predictions']:
            mortality_risk = np.array(metrics['mortality_predictions'])
            summary['mortality_risk'] = np.mean(mortality_risk)
        
        if metrics['treatment_effectiveness']:
            effectiveness = np.array(metrics['treatment_effectiveness'])
            summary['treatment_effectiveness'] = np.mean(effectiveness)
        
        if metrics['safety_scores']:
            safety = np.array(metrics['safety_scores'])
            summary['safety_score'] = np.mean(safety)
        
        return summary
    
    def train(self, train_sequences, val_sequences, epochs=30):
        """å¢å¼ºçš„è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆè´¥è¡€ç—‡è®­ç»ƒ...")
        print(f"è®­ç»ƒåºåˆ—: {len(train_sequences)}")
        print(f"éªŒè¯åºåˆ—: {len(val_sequences)}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        
        best_val_reward = -float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_reward = self.train_epoch(train_sequences, epoch)
            
            # éªŒè¯
            val_loss, val_reward, val_metrics = self.validate(val_sequences)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_reward)
            
            print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}:")
            print(f"  Train - Loss: {train_loss:.6f}, Reward: {train_reward:.4f}")
            print(f"  Val - Loss: {val_loss:.6f}, Reward: {val_reward:.4f}")
            print(f"  Safety Violations: {self.history['safety_violations'][-1]:.4f}")
            print(f"  Gradient Norm: {self.history['gradient_norm'][-1]:.4f}")
            print(f"  Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # è¾“å‡ºä¸´åºŠæŒ‡æ ‡
            if val_metrics:
                print(f"  Clinical Metrics - Mortality Risk: {val_metrics['mortality_risk']:.4f}, "
                      f"SOFA Improvement: {val_metrics['sofa_improvement']:.4f}")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_reward > best_val_reward:
                best_val_reward = val_reward
                patience_counter = 0
                torch.save(self.agent.state_dict(), 'models/enhanced_best_sepsis_agent.pth')
                print("  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = f'models/enhanced_checkpoint_epoch_{epoch+1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.agent.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_val_reward': best_val_reward,
                    'history': self.history
                }, checkpoint_path)
                print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        return self.history
    
    def save_training_report(self, filename='enhanced_training_report.txt'):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Enhanced Sepsis DRL Training Report\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Total Epochs: {len(self.history['train_reward'])}\n")
            f.write(f"Best Training Reward: {max(self.history['train_reward']):.4f}\n")
            f.write(f"Best Validation Reward: {max(self.history['val_reward']):.4f}\n")
            
            if self.history['safety_violations']:
                f.write(f"Final Safety Violations: {self.history['safety_violations'][-1]:.4f}\n")
            if self.history['gradient_norm']:
                f.write(f"Final Gradient Norm: {self.history['gradient_norm'][-1]:.4f}\n")
            if self.history['learning_rate']:
                f.write(f"Final Learning Rate: {self.history['learning_rate'][-1]:.2e}\n\n")
            
            f.write("Training Improvements:\n")
            f.write("- Enhanced LSTM architecture with attention mechanism\n")
            f.write("- Clinical reward function based on medical indicators\n")
            f.write("- Medical safety constraints to prevent dangerous combinations\n")
            f.write("- Improved training stability with gradient clipping and regularization\n")
            f.write("- Comprehensive clinical metrics evaluation\n")
        
        print(f"Training report saved to {filename}")
    
    def plot_training_history(self):
        """Generate individual training curve plots for better visualization and saving"""
        # Create plots directory if it doesn't exist
        Path('plots').mkdir(exist_ok=True)
        
        # Plot 1: Total Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_total_loss'], label='Train Total Loss', color='blue', linewidth=2)
        plt.plot(self.history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/total_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 2: Rewards
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_reward'], label='Train Reward', color='purple', linewidth=2)
        plt.plot(self.history['val_reward'], label='Validation Reward', color='brown', linewidth=2)
        plt.title('Clinical Rewards Over Training', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Reward')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/clinical_rewards.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 3: Actor vs Critic Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_actor_loss'], label='Actor Loss', color='green', linewidth=2)
        plt.plot(self.history['train_critic_loss'], label='Critic Loss', color='orange', linewidth=2)
        plt.title('Actor vs Critic Loss Comparison', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/actor_critic_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 4: Safety Violations
        if self.history['safety_violations']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['safety_violations'], label='Safety Violations', color='red', linewidth=2)
            plt.title('Safety Violations During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Violation Score')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/safety_violations.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 5: Gradient Norm
        if self.history['gradient_norm']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['gradient_norm'], label='Gradient Norm', color='cyan', linewidth=2)
            plt.title('Gradient Norm During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Norm')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/gradient_norm.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 6: Learning Rate Schedule
        if self.history['learning_rate']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['learning_rate'], label='Learning Rate', color='magenta', linewidth=2)
            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.yscale('log')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/learning_rate.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        print("ğŸ“Š Individual plots saved to 'plots/' directory:")

def load_sepsis_data(data_dir='preprocessed_sepsis_data'):
    """åŠ è½½é¢„å¤„ç†çš„è´¥è¡€ç—‡æ•°æ®"""
    data_dir = Path(data_dir)
    
    if not data_dir.exists():
        raise FileNotFoundError(f"Preprocessed data directory {data_dir} not found!")
    
    # åŠ è½½ç‰¹å¾ä¿¡æ¯
    feature_info = pd.read_csv(data_dir / 'feature_info.csv')
    feature_names = eval(feature_info['feature_names'].iloc[0])
    
    print(f"âœ… åŠ è½½ç‰¹å¾ä¿¡æ¯: {len(feature_names)} ä¸ªç‰¹å¾")
    
    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_features = np.load(data_dir / 'train_features.npy')
    val_features = np.load(data_dir / 'val_features.npy')
    train_metadata = pd.read_csv(data_dir / 'train_metadata.csv')
    val_metadata = pd.read_csv(data_dir / 'val_metadata.csv')
    
    print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {train_features.shape[0]} ä¸ªåºåˆ—")
    print(f"âœ… åŠ è½½éªŒè¯æ•°æ®: {val_features.shape[0]} ä¸ªåºåˆ—")
    
    # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼
    def create_sequences(features, metadata):
        sequences = []
        for i in range(len(features)):
            sequence_length = 7  # å‡è®¾å›ºå®šåºåˆ—é•¿åº¦
            sequences.append({
                'features': features[i].reshape(sequence_length, -1),
                'sequence_length': sequence_length
            })
        return sequences
    
    train_sequences = create_sequences(train_features, train_metadata)
    val_sequences = create_sequences(val_features, val_metadata)
    
    # æ•°æ®éªŒè¯
    valid_train_sequences = []
    for seq in train_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_train_sequences.append(seq)
    
    valid_val_sequences = []
    for seq in val_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_val_sequences.append(seq)
    
    print(f"âœ… æœ‰æ•ˆè®­ç»ƒåºåˆ—: {len(valid_train_sequences)}")
    print(f"âœ… æœ‰æ•ˆéªŒè¯åºåˆ—: {len(valid_val_sequences)}")
    
    input_dim = valid_train_sequences[0]['features'].shape[1] if valid_train_sequences else len(feature_names)
    
    return valid_train_sequences, valid_val_sequences, input_dim, feature_names

def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆ"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºmodelsç›®å½•
    Path('models').mkdir(exist_ok=True)
    
    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        train_sequences, val_sequences, input_dim, feature_names = load_sepsis_data()
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        agent = EnhancedSepsisLSTMAgent(
            input_dim=input_dim,
            hidden_dim=128,
            num_actions=20,
            sequence_length=7
        )
        
        # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
        reward_calculator = ClinicalRewardCalculator()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = EnhancedTrainer(
            agent=agent,
            reward_calculator=reward_calculator,
            lr=1e-4,  # æé«˜å­¦ä¹ ç‡ä»¥é€‚åº”æ–°çš„å¥–åŠ±å‡½æ•°
            device=device
        )
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(train_sequences, val_sequences, epochs=25)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        trainer.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        print("ğŸ“„ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        trainer.save_training_report()
        
        print("ğŸ‰ Enhanced training completed!")
        print("ğŸ“ Generated files:")
        print("  - models/enhanced_best_sepsis_agent.pth")
        print("  - enhanced_training_report.txt")
        print("  - models/enhanced_checkpoint_epoch_*.pth")
        print("  - plots/total_loss.png")
        print("  - plots/clinical_rewards.png")
        print("  - plots/actor_critic_loss.png")
        print("  - plots/safety_violations.png")
        print("  - plots/gradient_norm.png")
        print("  - plots/learning_rate.png")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 