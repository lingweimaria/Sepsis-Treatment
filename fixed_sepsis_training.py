#!/usr/bin/env python3

import os
# è§£å†³macOSä¸Šçš„OpenMPå†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from collections import defaultdict
import pickle
import warnings
warnings.filterwarnings('ignore')

class EnhancedSepsisLSTMAgent(nn.Module):
    """å¢å¼ºçš„è´¥è¡€ç—‡LSTMæ™ºèƒ½ä½“ - åŒ…å«åŒ»å­¦çº¦æŸå’Œç›‘ç£å­¦ä¹ æ¨¡ä»¿"""
    
    def __init__(self, input_dim, hidden_dim=128, num_actions=20, sequence_length=7, expert_data_path=None):
        super(EnhancedSepsisLSTMAgent, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_actions = num_actions
        self.sequence_length = sequence_length
        
        # å¤šå±‚LSTMç”¨äºæ›´å¥½çš„æ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=False
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºå…³æ³¨é‡è¦çš„æ—¶é—´æ­¥
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # Layer normalization for stability
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        
        # åŒ»å­¦ç‰¹å¾æå–å™¨
        self.medical_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim//2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim//2),
            nn.Dropout(0.1)
        )
        
        # å¢å¼ºçš„Actorç½‘ç»œ
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, num_actions)
        )
        
        # å¢å¼ºçš„Criticç½‘ç»œ
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, 1)
        )
        
        # åŒ»å­¦çº¦æŸå±‚
        self.medical_constraint = nn.Linear(num_actions, num_actions)
        
        # ç›‘ç£å­¦ä¹ æ¨¡ä»¿ä¸“å®¶è¡Œä¸ºçš„ç»„ä»¶
        self.expert_data_path = expert_data_path
        self.expert_imitation_net = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_actions)
        )
        
        # èåˆæƒé‡ - æ§åˆ¶RLå’ŒSLçš„å¹³è¡¡
        self.rl_weight = nn.Parameter(torch.tensor(0.7))  # å¯å­¦ä¹ çš„æƒé‡
        self.sl_weight = nn.Parameter(torch.tensor(0.3))
        
        # ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                module.bias.data.fill_(0.01)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    torch.nn.init.xavier_uniform_(param.data, gain=0.5)
                elif 'weight_hh' in name:
                    torch.nn.init.orthogonal_(param.data, gain=0.5)
                elif 'bias' in name:
                    param.data.fill_(0.01)
    
    def forward(self, sequences, sequence_lengths=None, use_expert_imitation=True):
        """å‰å‘ä¼ æ’­ - å¢åŠ ä¸“å®¶æ¨¡ä»¿åŠŸèƒ½"""
        batch_size = sequences.size(0)
        
        # LSTMå¤„ç†æ—¶åºæ•°æ®
        lstm_output, (h_n, c_n) = self.lstm(sequences)
        
        # Layer normalization
        lstm_output = self.layer_norm1(lstm_output)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attended_output, attention_weights = self.attention(
            lstm_output, lstm_output, lstm_output
        )
        
        # ç»“åˆLSTMå’Œæ³¨æ„åŠ›è¾“å‡º
        combined_output = lstm_output + attended_output
        combined_output = self.layer_norm2(combined_output)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        if sequence_lengths is not None:
            last_outputs = []
            for i, length in enumerate(sequence_lengths):
                last_outputs.append(combined_output[i, length-1, :])
            temporal_context = torch.stack(last_outputs)
        else:
            temporal_context = combined_output[:, -1, :]
        
        # åŒ»å­¦ç‰¹å¾æå–ï¼ˆä½¿ç”¨æœ€æ–°çš„åŒ»å­¦æ•°æ®ï¼‰
        current_medical_state = sequences[:, -1, :]
        medical_features = self.medical_feature_extractor(current_medical_state)
        
        # èåˆæ—¶åºå’ŒåŒ»å­¦ç‰¹å¾
        combined_features = torch.cat([temporal_context, medical_features], dim=1)
        
        # å¼ºåŒ–å­¦ä¹ Actorè¾“å‡º
        rl_action_logits = self.actor(combined_features)
        
        # ç›‘ç£å­¦ä¹ ä¸“å®¶æ¨¡ä»¿è¾“å‡º
        expert_action_logits = self.expert_imitation_net(combined_features)
        
        # èåˆRLå’ŒSLè¾“å‡º
        if use_expert_imitation:
            # å½’ä¸€åŒ–æƒé‡
            total_weight = torch.abs(self.rl_weight) + torch.abs(self.sl_weight)
            normalized_rl_weight = torch.abs(self.rl_weight) / total_weight
            normalized_sl_weight = torch.abs(self.sl_weight) / total_weight
            
            # åŠ æƒèåˆ
            fused_action_logits = (normalized_rl_weight * rl_action_logits + 
                                 normalized_sl_weight * expert_action_logits)
        else:
            fused_action_logits = rl_action_logits
        
        # åº”ç”¨åŒ»å­¦çº¦æŸ
        constrained_action_logits = self.medical_constraint(fused_action_logits)
        
        # Criticè¾“å‡º
        state_values = self.critic(combined_features)
        
        return (constrained_action_logits, state_values, combined_output, attention_weights,
                rl_action_logits, expert_action_logits)

class ClinicalRewardCalculator:
    """ä¸´åºŠç›¸å…³çš„å¥–åŠ±è®¡ç®—å™¨ - åŸºäºåŒ»å­¦æŒ‡æ ‡çš„å¥–åŠ±"""
    
    def __init__(self):
        # é‡æ–°è®¾è®¡çš„ä¸´åºŠæŒ‡æ ‡æƒé‡ - æ›´åŠ æ³¨é‡SOFAè¯„åˆ†å’Œç”Ÿå‘½ä½“å¾æ”¹å–„
        self.vital_signs_weight = 0.4  # å¢åŠ ç”Ÿå‘½ä½“å¾æƒé‡
        self.sofa_weight = 0.5         # å¢åŠ SOFAè¯„åˆ†æƒé‡
        self.stability_weight = 0.15   # é€‚å½“é™ä½ç¨³å®šæ€§æƒé‡
        self.safety_weight = 0.05      # é™ä½å®‰å…¨æƒé‡ï¼Œé¿å…è¿‡åº¦ä¿å®ˆ
        
        # æ­£å¸¸å€¼èŒƒå›´ (åŸºäºä¸´åºŠæ ‡å‡†) - æ‰©å¤§ä¸€äº›èŒƒå›´ä»¥å¢åŠ æ•æ„Ÿåº¦
        self.normal_ranges = {
            'temperature': (35.5, 38.0),   # ç¨å¾®æ‰©å¤§æ¸©åº¦èŒƒå›´
            'heart_rate': (50, 110),       # æ‰©å¤§å¿ƒç‡èŒƒå›´
            'respiratory_rate': (10, 24),  # æ‰©å¤§å‘¼å¸é¢‘ç‡èŒƒå›´
            'arterial_blood_pressure_systolic': (85, 150),
            'arterial_blood_pressure_diastolic': (55, 95),
            'o2_saturation_pulseoxymetry': (92, 100),  # ç¨å¾®é™ä½æ°§é¥±å’Œåº¦ä¸‹é™
            'glucose_(serum)': (60, 200),  # æ‰©å¤§è¡€ç³–èŒƒå›´
            'gcs_-_eye_opening': (3, 4),
            'gcs_-_motor_response': (4, 6),  # æ‰©å¤§è¿åŠ¨ååº”èŒƒå›´
            'gcs_-_verbal_response': (3, 5)  # æ‰©å¤§è¨€è¯­ååº”èŒƒå›´
        }
        
    def calculate_vital_signs_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾çš„å¥–åŠ±è®¡ç®— - æ”¹è¿›ç‰ˆ"""
        batch_size = features.size(0)
        rewards = torch.zeros(batch_size)
        
        # ç‰¹å¾åç§°æ˜ å°„åˆ°ç´¢å¼•
        feature_names = ['temperature', 'heart_rate', 'respiratory_rate', 
                        'arterial_blood_pressure_systolic', 'arterial_blood_pressure_diastolic',
                        'o2_saturation_pulseoxymetry', 'glucose_(serum)', 'gcs_-_eye_opening',
                        'gcs_-_motor_response', 'gcs_-_verbal_response', 'age', 'sofa_score']
        
        # è®¡ç®—æ—¶åºæ”¹å–„å¥–åŠ± - å…³æ³¨ç”Ÿå‘½ä½“å¾çš„æ”¹å–„è¶‹åŠ¿
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            current_values = features[:, -1, :10]  # å½“å‰æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            previous_values = features[:, -2, :10]  # å‰ä¸€æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            
            improvement_reward = 0.0
            for i, feature_name in enumerate(feature_names[:10]):
                if feature_name in self.normal_ranges:
                    normal_min, normal_max = self.normal_ranges[feature_name]
                    normal_center = (normal_min + normal_max) / 2
                    
                    # è®¡ç®—å½“å‰å’Œä¹‹å‰è·ç¦»æ­£å¸¸ä¸­å¿ƒçš„è·ç¦»
                    current_distance = torch.abs(current_values[:, i] - normal_center)
                    previous_distance = torch.abs(previous_values[:, i] - normal_center)
                    
                    # æ”¹å–„å¥–åŠ± - è·ç¦»æ­£å¸¸å€¼æ›´è¿‘è·å¾—æ­£å¥–åŠ±
                    improvement = previous_distance - current_distance
                    normalized_improvement = improvement / (normal_max - normal_min)
                    improvement_reward += normalized_improvement * 0.3  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.3å¥–åŠ±
            
            rewards += improvement_reward
        
        # å½“å‰çŠ¶æ€çš„æ­£å¸¸æ€§å¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„è®¡ç®—æ–¹å¼
        current_state_reward = 0.0
        for i, feature_name in enumerate(feature_names[:10]):
            if feature_name in self.normal_ranges:
                normal_min, normal_max = self.normal_ranges[feature_name]
                values = features[:, -1, i]  # å½“å‰æ—¶é—´æ­¥çš„å€¼
                
                # ä½¿ç”¨åˆ†æ®µçº¿æ€§å‡½æ•°è€ŒéæŒ‡æ•°å‡½æ•°ï¼Œå¢åŠ æ•æ„Ÿåº¦
                in_range_mask = (values >= normal_min) & (values <= normal_max)
                below_range_mask = values < normal_min
                above_range_mask = values > normal_max
                
                # åœ¨æ­£å¸¸èŒƒå›´å†…ï¼šæ»¡åˆ†
                feature_reward = torch.zeros_like(values)
                feature_reward[in_range_mask] = 1.0
                
                # åœ¨èŒƒå›´å¤–ï¼šçº¿æ€§é€’å‡ï¼Œè€ŒéæŒ‡æ•°é€’å‡
                range_width = normal_max - normal_min
                feature_reward[below_range_mask] = torch.clamp(
                    1.0 - (normal_min - values[below_range_mask]) / range_width, 
                    min=0.0, max=1.0
                )
                feature_reward[above_range_mask] = torch.clamp(
                    1.0 - (values[above_range_mask] - normal_max) / range_width, 
                    min=0.0, max=1.0
                )
                
                current_state_reward += feature_reward * 0.15  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.15å¥–åŠ±
        
        rewards += current_state_reward
        return rewards
    
    def calculate_sofa_reward(self, features):
        """åŸºäºSOFAè¯„åˆ†çš„å¥–åŠ± - æ”¹è¿›ç‰ˆï¼Œæ›´æ³¨é‡æ”¹å–„è¶‹åŠ¿"""
        current_sofa = features[:, -1, -1]  # å½“å‰SOFAè¯„åˆ†
        
        # åŸºç¡€SOFAå¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„åˆ†æ®µå‡½æ•°
        base_sofa_reward = torch.zeros_like(current_sofa)
        
        # ä¼˜ç§€çŠ¶æ€ (SOFA 0-6): é«˜å¥–åŠ±
        excellent_mask = current_sofa <= 6
        base_sofa_reward[excellent_mask] = 2.0 - current_sofa[excellent_mask] / 6.0
        
        # ä¸­ç­‰çŠ¶æ€ (SOFA 7-12): ä¸­ç­‰å¥–åŠ±
        moderate_mask = (current_sofa > 6) & (current_sofa <= 12)
        base_sofa_reward[moderate_mask] = 1.0 - (current_sofa[moderate_mask] - 6) / 12.0
        
        # ä¸¥é‡çŠ¶æ€ (SOFA 13-18): ä½å¥–åŠ±
        severe_mask = (current_sofa > 12) & (current_sofa <= 18)
        base_sofa_reward[severe_mask] = 0.5 - (current_sofa[severe_mask] - 12) / 12.0
        
        # å±é‡çŠ¶æ€ (SOFA >18): æä½å¥–åŠ±
        critical_mask = current_sofa > 18
        base_sofa_reward[critical_mask] = 0.1
        
        # SOFAæ”¹å–„å¥–åŠ± - å…³æ³¨è¯„åˆ†çš„æ”¹å–„
        improvement_reward = torch.zeros_like(current_sofa)
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            previous_sofa = features[:, -2, -1]
            sofa_improvement = previous_sofa - current_sofa  # è¯„åˆ†é™ä½æ˜¯å¥½äº‹
            
            # æ”¹å–„å¥–åŠ±ï¼šæ¯é™ä½1åˆ†SOFAè¯„åˆ†è·å¾—0.5å¥–åŠ±
            improvement_reward = torch.clamp(sofa_improvement * 0.5, min=-1.0, max=2.0)
        
        total_sofa_reward = base_sofa_reward + improvement_reward
        return torch.clamp(total_sofa_reward, min=0.0, max=3.0)
    
    def calculate_stability_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾ç¨³å®šæ€§çš„å¥–åŠ±"""
        if features.size(1) < 2:  # åºåˆ—é•¿åº¦å°äº2ï¼Œæ— æ³•è®¡ç®—ç¨³å®šæ€§
            return torch.zeros(features.size(0))
        
        # è®¡ç®—ç”Ÿå‘½ä½“å¾çš„å˜åŒ–ç‡
        vital_signs = features[:, :, :10]  # å‰10ä¸ªç‰¹å¾æ˜¯ç”Ÿå‘½ä½“å¾
        changes = torch.abs(vital_signs[:, 1:] - vital_signs[:, :-1])
        
        # ç¨³å®šæ€§å¥–åŠ±ï¼šå˜åŒ–è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
        stability = torch.exp(-changes.mean(dim=(1, 2)))
        return stability
    
    def calculate_safety_constraint_reward(self, action_logits):
        """åŸºäºå®‰å…¨çº¦æŸçš„å¥–åŠ±"""
        action_probs = torch.softmax(action_logits, dim=-1)
        
        # é¼“åŠ±é€‚åº¦çš„åŠ¨ä½œé€‰æ‹©ï¼ˆé¿å…æç«¯æ²»ç–—ï¼‰
        entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        target_entropy = np.log(action_logits.size(-1)) * 0.6  # ç›®æ ‡ç†µ
        
        # ç†µæ¥è¿‘ç›®æ ‡å€¼æ—¶å¥–åŠ±æœ€é«˜
        entropy_reward = torch.exp(-torch.abs(entropy - target_entropy))
        return entropy_reward
    
    def calculate_reward(self, state_features, action_logits, episode_step=0):
        """
        ç»¼åˆä¸´åºŠå¥–åŠ±è®¡ç®— - æ”¹è¿›ç‰ˆï¼Œå…è®¸æ›´å¤§çš„å¥–åŠ±èŒƒå›´å’ŒåŠ¨æ€æ€§
        """
        # å„é¡¹å¥–åŠ±è®¡ç®—
        vital_reward = self.calculate_vital_signs_reward(state_features)
        sofa_reward = self.calculate_sofa_reward(state_features)
        stability_reward = self.calculate_stability_reward(state_features)
        safety_reward = self.calculate_safety_constraint_reward(action_logits)
        
        # åŠ æƒç»„åˆ - ä½¿ç”¨æ–°çš„æƒé‡
        total_reward = (
            self.vital_signs_weight * vital_reward +
            self.sofa_weight * sofa_reward +
            self.stability_weight * stability_reward +
            self.safety_weight * safety_reward
        )
        
        # æ”¹è¿›çš„ç”Ÿå­˜æ—¶é—´å¥–åŠ± - æ›´å…·åŠ¨æ€æ€§
        survival_bonus = torch.full_like(total_reward, min(episode_step * 0.02, 0.3))
        
        # æ·»åŠ éšæœºæ€§å¥–åŠ±ä»¥å¢åŠ æ¢ç´¢
        exploration_bonus = torch.randn_like(total_reward) * 0.05
        
        final_reward = total_reward + survival_bonus + exploration_bonus
        
        # æ‰©å¤§å¥–åŠ±èŒƒå›´ï¼Œç§»é™¤è¿‡äºä¸¥æ ¼çš„é™åˆ¶
        return torch.clamp(final_reward, min=-1.0, max=5.0)

class MedicalSafetyConstraints:
    """åŒ»å­¦å®‰å…¨çº¦æŸç±» - åŸºäºçœŸå®ä¸´åºŠæŒ‡å—ä¼˜åŒ–"""
    
    def __init__(self, num_actions=20):
        self.num_actions = num_actions
        
        # åŸºäºçœŸå®ä¸´åºŠæŒ‡å—çš„å±é™©ç»„åˆ
        self.dangerous_combinations = [
            [1, 2],   # è¡€ç®¡åŠ å‹è¯ + é™å‹è¯ (ç›¸äº’æŠµæ¶ˆ)
            [3, 11],  # åˆ©å°¿å‰‚ + å¤§é‡ç”µè§£è´¨ (ç”µè§£è´¨ç´ªä¹±)
            [17, 18], # é•‡ç—›å‰‚ + é•‡é™å‰‚ (è¿‡åº¦é•‡é™)
        ]
        
        # éœ€è¦è°¨æ…ä½¿ç”¨çš„é«˜é£é™©åŠ¨ä½œ
        self.high_risk_actions = [1, 17, 18, 19]  # è¡€ç®¡åŠ å‹è¯ã€é•‡ç—›ã€é•‡é™ã€æŠ—å‡
        
    def apply_constraints(self, action_logits, medical_state=None):
        """åº”ç”¨åŒ»å­¦å®‰å…¨çº¦æŸ - ä¼˜åŒ–ç‰ˆ"""
        constrained_logits = action_logits.clone()
        action_probs = torch.softmax(action_logits, dim=-1)
        
        # çº¦æŸ1ï¼šé˜²æ­¢å±é™©è¯ç‰©ç»„åˆ
        for dangerous_combo in self.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            high_risk_mask = combo_prob > 0.05  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
            if high_risk_mask.any():
                for action_idx in dangerous_combo:
                    constrained_logits[high_risk_mask, action_idx] -= 1.5
        
        # çº¦æŸ2ï¼šåŸºäºç”Ÿç†çŠ¶æ€çš„æ™ºèƒ½çº¦æŸ
        if medical_state is not None:
            # ä½è¡€å‹çº¦æŸ - ç¦æ­¢é™å‹è¯ï¼Œé¼“åŠ±è¡€ç®¡åŠ å‹è¯
            bp_systolic = medical_state[:, 3]
            low_bp_mask = bp_systolic < 85  # ä¸¥æ ¼çš„ä½è¡€å‹æ ‡å‡†
            if low_bp_mask.any():
                constrained_logits[low_bp_mask, 2] -= 5.0  # ç¦æ­¢é™å‹è¯
                constrained_logits[low_bp_mask, 1] += 0.5  # è½»å¾®é¼“åŠ±è¡€ç®¡åŠ å‹è¯
            
            # é«˜è¡€å‹çº¦æŸ
            high_bp_mask = bp_systolic > 160
            if high_bp_mask.any():
                constrained_logits[high_bp_mask, 1] -= 3.0  # é™åˆ¶è¡€ç®¡åŠ å‹è¯
            
            # å¿ƒåŠ¨è¿‡é€Ÿçº¦æŸ
            heart_rate = medical_state[:, 1]
            high_hr_mask = heart_rate > 130
            if high_hr_mask.any():
                constrained_logits[high_hr_mask, 4] += 0.3  # é¼“åŠ±Î²å—ä½“é˜»æ»å‰‚
            
            # ä½æ°§çº¦æŸ
            spo2 = medical_state[:, 5]
            low_o2_mask = spo2 < 90
            if low_o2_mask.any():
                constrained_logits[low_o2_mask, 5] += 0.5  # é¼“åŠ±æ°§ç–—
        
        return constrained_logits

class ExpertDataLoader:
    """ä¸“å®¶æ•°æ®åŠ è½½å™¨ - ç”¨äºç›‘ç£å­¦ä¹ """
    
    def __init__(self, expert_data_path=None):
        # å¦‚æœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾
        if expert_data_path is None:
            expert_data_path = self._find_expert_data_file()
        
        self.expert_data_path = expert_data_path
        self.expert_actions = None
        self.expert_states = None
    
    def _find_expert_data_file(self):
        """è‡ªåŠ¨æŸ¥æ‰¾ä¸“å®¶æ•°æ®æ–‡ä»¶"""
        current_file_dir = Path(__file__).parent
        possible_paths = [
            current_file_dir / "processed_data" / "expert_data" / "expert_decisions.csv",
            "processed_data/expert_data/expert_decisions.csv",
            "../processed_data/expert_data/expert_decisions.csv", 
            "data_pipeline/processed_data/expert_data/expert_decisions.csv",
            "/Users/lulingwei/Desktop/TUM/sem_2/reinforcement learning/ä»£ç /data_pipeline/processed_data/expert_data/expert_decisions.csv",
            current_file_dir.parent / "processed_data" / "expert_data" / "expert_decisions.csv"
        ]
        
        for path in possible_paths:
            if Path(path).exists():
                print(f"ğŸ‘©â€âš•ï¸ è‡ªåŠ¨æ‰¾åˆ°ä¸“å®¶æ•°æ®: {path}")
                return str(path)
        
        print("âš ï¸ æœªæ‰¾åˆ°ä¸“å®¶æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return None
        
    def load_expert_data(self):
        """åŠ è½½ä¸“å®¶å†³ç­–æ•°æ® - å¢å¼ºé”™è¯¯å¤„ç†"""
        if self.expert_data_path is None:
            print("âš ï¸ æœªæä¾›ä¸“å®¶æ•°æ®è·¯å¾„ï¼Œç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶æ•°æ®")
            self.generate_simulated_expert_data()
        else:
            try:
                print(f"ğŸ“‚ å°è¯•åŠ è½½ä¸“å®¶æ•°æ®: {self.expert_data_path}")
                expert_data = pd.read_csv(self.expert_data_path)
                
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if 'expert_action' not in expert_data.columns:
                    print("âš ï¸ CSVæ–‡ä»¶ç¼ºå°‘'expert_action'åˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                    self.generate_simulated_expert_data()
                    return
                
                # æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
                feature_columns = []
                for col in expert_data.columns:
                    if col != 'expert_action' and col not in ['patient_id', 'timestamp', 'drug_name', 'vital_signs_count']:
                        feature_columns.append(col)
                
                if len(feature_columns) == 0:
                    print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆç‰¹å¾åˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                    self.generate_simulated_expert_data()
                    return
                
                # æå–æ•°æ®
                expert_features = expert_data[feature_columns]
                expert_actions = expert_data['expert_action']
                
                # æ¸…ç†æ•°æ® - ç§»é™¤NaNè¡Œ
                valid_mask = ~(expert_features.isnull().any(axis=1) | expert_actions.isnull())
                expert_features = expert_features[valid_mask]
                expert_actions = expert_actions[valid_mask]
                
                if len(expert_actions) == 0:
                    print("âš ï¸ æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                    self.generate_simulated_expert_data()
                    return
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                self.expert_states = expert_features.values.astype(np.float32)
                self.expert_actions = expert_actions.values.astype(np.int64)
                
                print(f"âœ… åŠ è½½çœŸå®ä¸“å®¶æ•°æ®: {len(self.expert_actions)} ä¸ªæ ·æœ¬, {self.expert_states.shape[1]} ä¸ªç‰¹å¾")
                print(f"   åŠ¨ä½œåˆ†å¸ƒ: {np.bincount(self.expert_actions)}")
                
            except Exception as e:
                print(f"âŒ åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                self.generate_simulated_expert_data()
    
    def generate_simulated_expert_data(self, num_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶å†³ç­–æ•°æ®"""
        # åŸºäºåŒ»å­¦è§„åˆ™ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶å†³ç­–
        np.random.seed(42)
        
        # æ¨¡æ‹ŸçŠ¶æ€ç‰¹å¾ (temperature, heart_rate, bp_sys, bp_dias, spo2, ...)
        states = []
        actions = []
        
        for _ in range(num_samples):
            # ç”Ÿæˆæ‚£è€…çŠ¶æ€
            temp = np.random.normal(37.0, 1.5)  # ä½“æ¸©
            hr = np.random.normal(80, 20)       # å¿ƒç‡
            bp_sys = np.random.normal(120, 25)  # æ”¶ç¼©å‹
            bp_dias = np.random.normal(80, 15)  # èˆ’å¼ å‹
            spo2 = np.random.normal(96, 4)      # æ°§é¥±å’Œåº¦
            
            # å…¶ä»–ç‰¹å¾
            other_features = np.random.normal(0, 1, 7)  # å…¶ä»–12-5=7ä¸ªç‰¹å¾
            state = np.array([temp, hr, 0, bp_sys, bp_dias, spo2] + list(other_features))
            
            # åŸºäºçŠ¶æ€çš„ä¸“å®¶å†³ç­–è§„åˆ™
            action = self.expert_decision_rule(temp, hr, bp_sys, bp_dias, spo2)
            
            states.append(state)
            actions.append(action)
        
        self.expert_states = np.array(states)
        self.expert_actions = np.array(actions)
        print(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶æ•°æ®: {len(self.expert_actions)} ä¸ªæ ·æœ¬")
    
    def expert_decision_rule(self, temp, hr, bp_sys, bp_dias, spo2):
        """åŸºäºåŒ»å­¦çŸ¥è¯†çš„ä¸“å®¶å†³ç­–è§„åˆ™"""
        # å‘çƒ­ -> é™æ¸©è¯ç‰©
        if temp > 38.5:
            return 1  # é™æ¸©è¯ç‰©
        
        # å¿ƒåŠ¨è¿‡é€Ÿ -> Î²å—ä½“é˜»æ»å‰‚
        elif hr > 100:
            return 2  # Î²å—ä½“é˜»æ»å‰‚
        
        # ä½è¡€å‹ -> è¡€ç®¡åŠ å‹è¯
        elif bp_sys < 90:
            return 3  # è¡€ç®¡åŠ å‹è¯
        
        # é«˜è¡€å‹ -> é™å‹è¯
        elif bp_sys > 140:
            return 4  # é™å‹è¯
        
        # ä½æ°§ -> æ°§ç–—
        elif spo2 < 92:
            return 5  # æ°§ç–—
        
        # æ­£å¸¸æƒ…å†µ -> è§‚å¯Ÿ
        else:
            return 0  # è§‚å¯Ÿ/ç»´æŒæ²»ç–—
    
    def get_expert_batch(self, batch_size=32):
        """è·å–ä¸“å®¶æ•°æ®æ‰¹æ¬¡ - å¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            if self.expert_states is None or self.expert_actions is None:
                self.load_expert_data()
            
            if self.expert_states is None or self.expert_actions is None:
                return None, None
                
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if len(self.expert_actions) == 0:
                return None, None
                
            indices = np.random.choice(len(self.expert_actions), batch_size, replace=True)
            
            batch_states = self.expert_states[indices]
            batch_actions = self.expert_actions[indices]
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            if batch_states.dtype == object:
                # å°è¯•ä¿®å¤ object ç±»å‹
                fixed_states = []
                for state in batch_states:
                    if isinstance(state, (list, np.ndarray)):
                        try:
                            fixed_state = np.array(state, dtype=np.float32)
                            if fixed_state.shape == (12,):  # ç¡®ä¿æ­£ç¡®çš„ç‰¹å¾æ•°é‡
                                fixed_states.append(fixed_state)
                            else:
                                # é»˜è®¤å€¼
                                fixed_states.append(np.zeros(12, dtype=np.float32))
                        except:
                            fixed_states.append(np.zeros(12, dtype=np.float32))
                    else:
                        fixed_states.append(np.zeros(12, dtype=np.float32))
                
                batch_states = np.array(fixed_states, dtype=np.float32)
            
            return batch_states, batch_actions
            
        except Exception as e:
            print(f"      ä¸“å®¶æ•°æ®æ‰¹æ¬¡è·å–é”™è¯¯: {e}")
            return None, None

class EnhancedTrainer:
    """å¢å¼ºçš„è®­ç»ƒå™¨ - åŒ…å«åŒ»å­¦å®‰å…¨çº¦æŸå’Œç›‘ç£å­¦ä¹ """
    
    def __init__(self, agent, reward_calculator, lr=5e-5, device='cpu', expert_data_path=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.agent = agent.to(device)
        self.reward_calculator = reward_calculator
        self.device = device
        self.safety_constraints = MedicalSafetyConstraints()
        
        # ä¸“å®¶æ•°æ®åŠ è½½å™¨ - ExpertDataLoaderä¼šè‡ªåŠ¨æŸ¥æ‰¾æ•°æ®
        print("ğŸ” åˆå§‹åŒ–ä¸“å®¶æ•°æ®åŠ è½½å™¨...")
        
        try:
            self.expert_loader = ExpertDataLoader(expert_data_path)
            self.expert_loader.load_expert_data()
            
            if self.expert_loader.expert_states is not None:
                print(f"âœ… ä¸“å®¶æ•°æ®åŠ è½½æˆåŠŸ: {len(self.expert_loader.expert_actions)} ä¸ªæ ·æœ¬")
            else:
                print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿä¸“å®¶æ•°æ®")
                
        except Exception as e:
            print(f"âš ï¸ ä¸“å®¶æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰")
            self.expert_loader = ExpertDataLoader(None)
            self.expert_loader.load_expert_data()
        
        # æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.AdamW(
            self.agent.parameters(), 
            lr=lr, 
            weight_decay=1e-3,  # å¢åŠ æ­£åˆ™åŒ–
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=3, verbose=True
        )
        
        self.mse_loss = nn.MSELoss()
        self.huber_loss = nn.SmoothL1Loss()  # æ›´ç¨³å®šçš„æŸå¤±å‡½æ•°
        self.global_step = 0
        
        # å¢å¼ºçš„å†å²è®°å½•
        self.history = {
            'train_total_loss': [],
            'train_actor_loss': [],
            'train_critic_loss': [],
            'train_expert_loss': [],  # æ–°å¢ï¼šä¸“å®¶æ¨¡ä»¿æŸå¤±
            'train_reward': [],
            'val_loss': [],
            'val_reward': [],
            'learning_rate': [],
            'gradient_norm': [],
            'safety_violations': [],
            'rl_sl_weights': []  # æ–°å¢ï¼šRLå’ŒSLæƒé‡å˜åŒ–
        }
    
    def prepare_batch(self, sequences, batch_size=8):
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡"""
        np.random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch_sequences = sequences[i:i + batch_size]
            
            # å †å ç‰¹å¾
            features = torch.tensor([seq['features'] for seq in batch_sequences], 
                                   dtype=torch.float32, device=self.device)
            
            # åºåˆ—é•¿åº¦
            seq_lengths = [seq['sequence_length'] for seq in batch_sequences]
            
            # æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼
            features = torch.clamp(features, min=-10, max=10)
            features = torch.where(torch.isnan(features), torch.zeros_like(features), features)
            
            yield features, seq_lengths
    
    def train_epoch(self, train_sequences, epoch):
        """å¢å¼ºçš„è®­ç»ƒepoch - åŒ…å«å®‰å…¨çº¦æŸã€RLå’ŒSLæ··åˆè®­ç»ƒ"""
        self.agent.train()
        epoch_actor_losses = []
        epoch_critic_losses = []
        epoch_expert_losses = []  # æ–°å¢ï¼šä¸“å®¶æ¨¡ä»¿æŸå¤±
        epoch_rewards = []
        epoch_safety_violations = []
        epoch_gradient_norms = []
        
        for batch_idx, (features, seq_lengths) in enumerate(self.prepare_batch(train_sequences)):
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - åŒ…å«RLå’ŒSLè¾“å‡º
            (action_logits, state_values, _, attention_weights, 
             rl_action_logits, expert_action_logits) = self.agent(features, seq_lengths)
            
            # åº”ç”¨åŒ»å­¦å®‰å…¨çº¦æŸ
            current_medical_state = features[:, -1, :]
            constrained_action_logits = self.safety_constraints.apply_constraints(
                action_logits, current_medical_state
            )
            
            # è®¡ç®—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            action_probs = torch.softmax(constrained_action_logits, dim=-1)
            
            # è®¡ç®—å¥–åŠ±
            rewards = self.reward_calculator.calculate_reward(
                features, constrained_action_logits, episode_step=self.global_step
            ).to(self.device)
            
            # æ›´ç¨³å®šçš„CriticæŸå¤±è®¡ç®—
            state_values_squeezed = state_values.squeeze()
            if state_values_squeezed.dim() == 0:
                state_values_squeezed = state_values_squeezed.unsqueeze(0)
            
            # ä½¿ç”¨HuberæŸå¤±æ›´ç¨³å®š
            critic_loss = self.huber_loss(state_values_squeezed, rewards)
            
            # ç®€åŒ–advantagesè®¡ç®—
            advantages = rewards - state_values_squeezed.detach()
            # ä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨
            advantages = torch.clamp(advantages, min=-1.0, max=1.0)
            
            # ç®€åŒ–çš„ActoræŸå¤±è®¡ç®— - æ ‡å‡†ç­–ç•¥æ¢¯åº¦
            action_dist = torch.distributions.Categorical(action_probs)
            sampled_actions = action_dist.sample()
            
            # è®¡ç®—é€‰ä¸­åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            log_probs = action_dist.log_prob(sampled_actions)
            
            # å®Œå…¨é‡å†™ActoræŸå¤± - ä½¿ç”¨ç®€å•çš„äº¤å‰ç†µæŸå¤±
            # å°†RLé—®é¢˜è½¬æ¢ä¸ºåˆ†ç±»é—®é¢˜
            # ä½¿ç”¨advantagesä½œä¸ºæƒé‡çš„äº¤å‰ç†µæŸå¤±
            
            # ç”Ÿæˆç›®æ ‡åŠ¨ä½œï¼ˆåŸºäºadvantagesï¼‰
            # æ­£advantages -> é¼“åŠ±å½“å‰åŠ¨ä½œï¼Œè´Ÿadvantages -> æƒ©ç½šå½“å‰åŠ¨ä½œ
            positive_advantages = advantages > 0
            
            # å¯¹æ­£advantagesçš„åŠ¨ä½œä½¿ç”¨æ­£å‘äº¤å‰ç†µ
            positive_loss = torch.zeros_like(log_probs)
            if positive_advantages.any():
                positive_loss[positive_advantages] = -log_probs[positive_advantages] * advantages[positive_advantages].abs()
            
            # å¯¹è´Ÿadvantagesçš„åŠ¨ä½œä½¿ç”¨åå‘äº¤å‰ç†µ
            negative_loss = torch.zeros_like(log_probs)
            if (~positive_advantages).any():
                # æƒ©ç½šå½“å‰åŠ¨ä½œï¼Œé¼“åŠ±å…¶ä»–åŠ¨ä½œ
                negative_loss[~positive_advantages] = advantages[~positive_advantages].abs() * 0.1
            
            # ç»„åˆæŸå¤±
            policy_loss = (positive_loss + negative_loss).mean()
            
            # ç†µæ­£åˆ™åŒ–
            entropy = action_dist.entropy().mean()
            entropy_loss = -0.001 * entropy  # å¾®å¼±ç†µå¥–åŠ±
            
            # æœ€ç»ˆActoræŸå¤±
            actor_loss = policy_loss + entropy_loss
            actor_loss = torch.clamp(actor_loss, min=0.001)  # å¼ºåˆ¶ä¸ºæ­£å€¼
            
            # ç›‘ç£å­¦ä¹ æŸå¤± - æ¨¡ä»¿ä¸“å®¶è¡Œä¸º
            expert_loss = self.calculate_expert_imitation_loss(
                features, expert_action_logits, batch_idx
            )
            
            # ç®€åŒ–å®‰å…¨çº¦æŸæŸå¤±
            safety_violation = self.calculate_safety_violations(action_probs)
            safety_loss = 0.3 * safety_violation
            
            # é‡æ–°è®¾è®¡æ€»æŸå¤± - å¹³è¡¡å„ç»„ä»¶
            total_loss = (
                0.3 * critic_loss + 
                0.3 * actor_loss + 
                0.2 * expert_loss + 
                0.2 * safety_loss
            )
            total_loss = torch.clamp(total_loss, min=0.0)
            
            # æ£€æŸ¥å¼‚å¸¸æŸå¤±å€¼
            if (torch.isnan(total_loss) or torch.isinf(total_loss) or 
                torch.isnan(actor_loss) or torch.isnan(critic_loss) or
                total_loss.item() < 0.0):
                print(f"å¼‚å¸¸æŸå¤±æ£€æµ‹ epoch {epoch}, batch {batch_idx}: "
                      f"total={total_loss.item():.6f}, actor={actor_loss.item():.6f}, "
                      f"critic={critic_loss.item():.6f}. è·³è¿‡...")
                continue
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # éå¸¸ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ªå’Œæ¢¯åº¦çˆ†ç‚¸æ£€æµ‹
            grad_norm_before = torch.nn.utils.clip_grad_norm_(self.agent.parameters(), max_norm=float('inf'))
            
            # å¦‚æœæ¢¯åº¦è¿‡å¤§ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
            if grad_norm_before > 5.0:
                print(f"    æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹: {grad_norm_before:.2f} > 5.0, è·³è¿‡æ‰¹æ¬¡")
                continue
                
            grad_norm = torch.nn.utils.clip_grad_norm_(self.agent.parameters(), max_norm=0.1)
            epoch_gradient_norms.append(grad_norm.item())
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            epoch_actor_losses.append(actor_loss.item())
            epoch_critic_losses.append(critic_loss.item())
            epoch_expert_losses.append(expert_loss.item())
            epoch_rewards.append(rewards.mean().item())
            epoch_safety_violations.append(safety_violation.item())
            
            self.global_step += 1
            
            if batch_idx % 5 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}: "
                      f"Total Loss = {total_loss.item():.6f}, "
                      f"Actor Loss = {actor_loss.item():.6f}, "
                      f"Critic Loss = {critic_loss.item():.6f}, "
                      f"Expert Loss = {expert_loss.item():.6f}, "
                      f"Safety Loss = {safety_loss.item():.6f}, "
                      f"Avg Reward = {rewards.mean().item():.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_actor_loss = np.mean(epoch_actor_losses) if epoch_actor_losses else 0
        avg_critic_loss = np.mean(epoch_critic_losses) if epoch_critic_losses else 0
        avg_expert_loss = np.mean(epoch_expert_losses) if epoch_expert_losses else 0
        avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0
        avg_safety_violations = np.mean(epoch_safety_violations) if epoch_safety_violations else 0
        avg_grad_norm = np.mean(epoch_gradient_norms) if epoch_gradient_norms else 0
        
        # ä½¿ç”¨åŠ æƒæ€»æŸå¤±
        total_avg_loss = 0.3 * avg_critic_loss + 0.3 * avg_actor_loss + 0.2 * avg_expert_loss + 0.2 * avg_safety_violations
        
        # æ›´æ–°å†å²è®°å½•
        self.history['train_actor_loss'].append(avg_actor_loss)
        self.history['train_critic_loss'].append(avg_critic_loss)
        self.history['train_expert_loss'].append(avg_expert_loss)
        self.history['train_total_loss'].append(total_avg_loss)
        self.history['train_reward'].append(avg_reward)
        self.history['safety_violations'].append(avg_safety_violations)
        self.history['gradient_norm'].append(avg_grad_norm)
        self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])
        self.history['rl_sl_weights'].append([self.agent.rl_weight.item(), self.agent.sl_weight.item()])
        
        return total_avg_loss, avg_reward
    
    def calculate_safety_violations(self, action_probs):
        """è®¡ç®—å®‰å…¨è¿è§„æƒ…å†µ - æ›´ä¸¥æ ¼çš„è¯„ä¼°"""
        violations = torch.zeros(action_probs.size(0), device=self.device)
        
        # æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ - æ›´ä¸¥æ ¼
        for dangerous_combo in self.safety_constraints.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            violations += combo_prob * 2.0  # åŠ é‡å±é™©ç»„åˆæƒ©ç½š
        
        # æ£€æŸ¥é«˜é£é™©åŠ¨ä½œ
        high_risk_probs = action_probs[:, self.safety_constraints.high_risk_actions]
        violations += high_risk_probs.sum(dim=1) * 1.0  # æé«˜é«˜é£é™©åŠ¨ä½œæƒ©ç½š
        
        # æ£€æŸ¥åŠ¨ä½œåˆ†å¸ƒé›†ä¸­åº¦ - è¿‡äºé›†ä¸­ä¹Ÿä¸å®‰å…¨
        max_prob = torch.max(action_probs, dim=1)[0]
        violations += torch.clamp(max_prob - 0.8, min=0) * 0.5  # å•ä¸€åŠ¨ä½œæ¦‚ç‡è¿‡é«˜
        
        return violations.mean()
    
    def calculate_expert_imitation_loss(self, current_features, expert_logits, batch_idx):
        """è®¡ç®—ä¸“å®¶æ¨¡ä»¿æŸå¤± - ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        batch_size = current_features.size(0)
        
        try:
            # è·å–ä¸“å®¶æ•°æ®æ‰¹æ¬¡
            expert_states, expert_actions = self.expert_loader.get_expert_batch(batch_size)
            
            # å®‰å…¨çš„æ•°æ®ç±»å‹è½¬æ¢
            if expert_states is None or expert_actions is None:
                # å¦‚æœæ²¡æœ‰ä¸“å®¶æ•°æ®ï¼Œè¿”å›å°æŸå¤±
                return torch.tensor(0.1, device=self.device, requires_grad=True)
            
            # å¤„ç†expert_states - ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
            if isinstance(expert_states, np.ndarray):
                if expert_states.dtype == object:
                    # å¦‚æœæ˜¯objectç±»å‹ï¼Œå°è¯•è½¬æ¢
                    try:
                        expert_states = np.array([np.array(x, dtype=np.float32) for x in expert_states])
                        expert_states = expert_states.astype(np.float32)
                    except:
                        # è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        expert_states = np.random.randn(batch_size, current_features.size(-1)).astype(np.float32)
                else:
                    expert_states = expert_states.astype(np.float32)
            
            # å¤„ç†expert_actions
            if isinstance(expert_actions, np.ndarray):
                expert_actions = expert_actions.astype(np.int64)
            
            # è½¬æ¢ä¸ºå¼ é‡
            expert_states_tensor = torch.tensor(expert_states, dtype=torch.float32, device=self.device)
            expert_actions_tensor = torch.tensor(expert_actions, dtype=torch.long, device=self.device)
            
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
            if expert_states_tensor.size(0) != batch_size:
                expert_states_tensor = expert_states_tensor[:batch_size]
                expert_actions_tensor = expert_actions_tensor[:batch_size]
            
            if expert_actions_tensor.size(0) != batch_size:
                expert_actions_tensor = expert_actions_tensor[:batch_size]
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            expert_loss = nn.CrossEntropyLoss()(expert_logits, expert_actions_tensor)
            
            return expert_loss
            
        except Exception as e:
            print(f"      ä¸“å®¶æ•°æ®å¤„ç†é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤æŸå¤±")
            # è¿”å›ä¸€ä¸ªå°çš„é»˜è®¤æŸå¤±
            return torch.tensor(0.1, device=self.device, requires_grad=True)
    
    def validate(self, val_sequences):
        """å¢å¼ºçš„æ¨¡å‹éªŒè¯ - åŒ…å«ä¸´åºŠæŒ‡æ ‡"""
        self.agent.eval()
        val_losses = []
        val_rewards = []
        clinical_metrics = {
            'sofa_scores': [],
            'mortality_predictions': [],
            'treatment_effectiveness': [],
            'safety_scores': []
        }
        
        with torch.no_grad():
            for features, seq_lengths in self.prepare_batch(val_sequences):
                (action_logits, state_values, _, attention_weights,
                 rl_action_logits, expert_action_logits) = self.agent(features, seq_lengths)
                
                # åº”ç”¨å®‰å…¨çº¦æŸ
                current_medical_state = features[:, -1, :]
                constrained_action_logits = self.safety_constraints.apply_constraints(
                    action_logits, current_medical_state
                )
                
                action_probs = torch.softmax(constrained_action_logits, dim=-1)
                
                rewards = self.reward_calculator.calculate_reward(
                    features, constrained_action_logits, episode_step=self.global_step
                ).to(self.device)
                
                state_values_squeezed = state_values.squeeze()
                if state_values_squeezed.dim() == 0:
                    state_values_squeezed = state_values_squeezed.unsqueeze(0)
                
                critic_loss = self.huber_loss(state_values_squeezed, rewards)
                
                # è®¡ç®—ActoræŸå¤±
                advantages = rewards - state_values_squeezed
                action_dist = torch.distributions.Categorical(action_probs)
                sampled_actions = action_dist.sample()
                log_probs = action_dist.log_prob(sampled_actions)
                actor_loss = -torch.mean(log_probs * advantages)
                
                # è®¡ç®—ä¸“å®¶æ¨¡ä»¿æŸå¤±
                expert_loss = self.calculate_expert_imitation_loss(
                    features, expert_action_logits, 0
                )
                
                total_loss = critic_loss.item() + actor_loss.item() + 0.3 * expert_loss.item()
                
                if not (torch.isnan(torch.tensor(total_loss)) or torch.isinf(torch.tensor(total_loss))):
                    val_losses.append(total_loss)
                    val_rewards.append(rewards.mean().item())
                    
                    # æ”¶é›†ä¸´åºŠæŒ‡æ ‡
                    self.collect_clinical_metrics(features, action_probs, state_values, clinical_metrics)
        
        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')
        avg_val_reward = np.mean(val_rewards) if val_rewards else 0
        
        self.history['val_loss'].append(avg_val_loss)
        self.history['val_reward'].append(avg_val_reward)
        
        # è®¡ç®—ä¸´åºŠæŒ‡æ ‡
        clinical_summary = self.summarize_clinical_metrics(clinical_metrics)
        
        return avg_val_loss, avg_val_reward, clinical_summary
    
    def collect_clinical_metrics(self, features, action_probs, state_values, metrics):
        """æ”¶é›†ä¸´åºŠæŒ‡æ ‡"""
        # SOFAè¯„åˆ†å˜åŒ–
        sofa_scores = features[:, -1, -1]  # æœ€åä¸€ä¸ªç‰¹å¾æ˜¯SOFAè¯„åˆ†
        metrics['sofa_scores'].extend(sofa_scores.cpu().numpy())
        
        # æ­»äº¡é£é™©é¢„æµ‹ï¼ˆåŸºäºçŠ¶æ€å€¼ï¼‰
        mortality_risk = torch.sigmoid(-state_values.squeeze())  # çŠ¶æ€å€¼è¶Šä½ï¼Œæ­»äº¡é£é™©è¶Šé«˜
        metrics['mortality_predictions'].extend(mortality_risk.cpu().numpy())
        
        # æ²»ç–—æœ‰æ•ˆæ€§ï¼ˆåŸºäºåŠ¨ä½œé€‰æ‹©çš„é›†ä¸­åº¦ï¼‰
        action_entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        treatment_effectiveness = 1.0 / (1.0 + action_entropy)  # ç†µè¶Šä½ï¼Œæ²»ç–—è¶Šé›†ä¸­
        metrics['treatment_effectiveness'].extend(treatment_effectiveness.cpu().numpy())
        
        # å®‰å…¨è¯„åˆ†
        safety_scores = self.calculate_safety_scores(action_probs)
        metrics['safety_scores'].extend(safety_scores.cpu().numpy())
    
    def calculate_safety_scores(self, action_probs):
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        safety_scores = torch.ones(action_probs.size(0), device=self.device)
        
        # æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ
        for dangerous_combo in self.safety_constraints.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            safety_scores -= combo_prob  # å±é™©ç»„åˆæ¦‚ç‡è¶Šé«˜ï¼Œå®‰å…¨è¯„åˆ†è¶Šä½
        
        return torch.clamp(safety_scores, min=0.0, max=1.0)
    
    def summarize_clinical_metrics(self, metrics):
        """æ±‡æ€»ä¸´åºŠæŒ‡æ ‡"""
        summary = {}
        
        if metrics['sofa_scores']:
            sofa_scores = np.array(metrics['sofa_scores'])
            summary['avg_sofa_score'] = np.mean(sofa_scores)
            summary['sofa_improvement'] = max(0, 15 - np.mean(sofa_scores)) / 15  # æ ‡å‡†åŒ–æ”¹å–„æŒ‡æ ‡
        
        if metrics['mortality_predictions']:
            mortality_risk = np.array(metrics['mortality_predictions'])
            summary['mortality_risk'] = np.mean(mortality_risk)
        
        if metrics['treatment_effectiveness']:
            effectiveness = np.array(metrics['treatment_effectiveness'])
            summary['treatment_effectiveness'] = np.mean(effectiveness)
        
        if metrics['safety_scores']:
            safety = np.array(metrics['safety_scores'])
            summary['safety_score'] = np.mean(safety)
        
        return summary
    
    def train(self, train_sequences, val_sequences, epochs=30):
        """å¢å¼ºçš„è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆè´¥è¡€ç—‡è®­ç»ƒ...")
        print(f"è®­ç»ƒåºåˆ—: {len(train_sequences)}")
        print(f"éªŒè¯åºåˆ—: {len(val_sequences)}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        
        best_val_reward = -float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_reward = self.train_epoch(train_sequences, epoch)
            
            # éªŒè¯
            val_loss, val_reward, val_metrics = self.validate(val_sequences)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_reward)
            
            print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}:")
            print(f"  Train - Loss: {train_loss:.6f}, Reward: {train_reward:.4f}")
            print(f"  Val - Loss: {val_loss:.6f}, Reward: {val_reward:.4f}")
            print(f"  Safety Violations: {self.history['safety_violations'][-1]:.4f}")
            print(f"  Gradient Norm: {self.history['gradient_norm'][-1]:.4f}")
            print(f"  Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # è¾“å‡ºä¸´åºŠæŒ‡æ ‡
            if val_metrics:
                print(f"  Clinical Metrics - Mortality Risk: {val_metrics['mortality_risk']:.4f}, "
                      f"SOFA Improvement: {val_metrics['sofa_improvement']:.4f}")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_reward > best_val_reward:
                best_val_reward = val_reward
                patience_counter = 0
                torch.save(self.agent.state_dict(), 'models/enhanced_best_sepsis_agent.pth')
                print("  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = f'models/enhanced_checkpoint_epoch_{epoch+1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.agent.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_val_reward': best_val_reward,
                    'history': self.history
                }, checkpoint_path)
                print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        return self.history
    
    def save_training_report(self, filename='enhanced_training_report.txt'):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Enhanced Sepsis DRL Training Report\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Total Epochs: {len(self.history['train_reward'])}\n")
            f.write(f"Best Training Reward: {max(self.history['train_reward']):.4f}\n")
            f.write(f"Best Validation Reward: {max(self.history['val_reward']):.4f}\n")
            
            if self.history['safety_violations']:
                f.write(f"Final Safety Violations: {self.history['safety_violations'][-1]:.4f}\n")
            if self.history['gradient_norm']:
                f.write(f"Final Gradient Norm: {self.history['gradient_norm'][-1]:.4f}\n")
            if self.history['learning_rate']:
                f.write(f"Final Learning Rate: {self.history['learning_rate'][-1]:.2e}\n\n")
            
            f.write("Training Improvements:\n")
            f.write("- Enhanced LSTM architecture with attention mechanism\n")
            f.write("- Clinical reward function based on medical indicators\n")
            f.write("- Medical safety constraints to prevent dangerous combinations\n")
            f.write("- Improved training stability with gradient clipping and regularization\n")
            f.write("- Comprehensive clinical metrics evaluation\n")
        
        print(f"Training report saved to {filename}")
    
    def plot_training_history(self):
        """Generate individual training curve plots for better visualization and saving"""
        # Create plots directory if it doesn't exist
        Path('plots').mkdir(exist_ok=True)
        
        # Plot 1: Total Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_total_loss'], label='Train Total Loss', color='blue', linewidth=2)
        plt.plot(self.history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/total_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 2: Rewards
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_reward'], label='Train Reward', color='purple', linewidth=2)
        plt.plot(self.history['val_reward'], label='Validation Reward', color='brown', linewidth=2)
        plt.title('Clinical Rewards Over Training', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Reward')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/clinical_rewards.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 3: Actor vs Critic vs Expert Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_actor_loss'], label='Actor Loss (RL)', color='green', linewidth=2)
        plt.plot(self.history['train_critic_loss'], label='Critic Loss', color='orange', linewidth=2)
        if self.history['train_expert_loss']:
            plt.plot(self.history['train_expert_loss'], label='Expert Loss (SL)', color='blue', linewidth=2)
        plt.title('Actor vs Critic vs Expert Loss Comparison', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/actor_critic_expert_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 4: Safety Violations
        if self.history['safety_violations']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['safety_violations'], label='Safety Violations', color='red', linewidth=2)
            plt.title('Safety Violations During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Violation Score')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/safety_violations.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 5: Gradient Norm
        if self.history['gradient_norm']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['gradient_norm'], label='Gradient Norm', color='cyan', linewidth=2)
            plt.title('Gradient Norm During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Norm')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/gradient_norm.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 6: Learning Rate Schedule
        if self.history['learning_rate']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['learning_rate'], label='Learning Rate', color='magenta', linewidth=2)
            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.yscale('log')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/learning_rate.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 7: RL vs SL Weight Evolution
        if self.history['rl_sl_weights']:
            plt.figure(figsize=(10, 6))
            rl_weights = [w[0] for w in self.history['rl_sl_weights']]
            sl_weights = [w[1] for w in self.history['rl_sl_weights']]
            plt.plot(rl_weights, label='RL Weight', color='red', linewidth=2)
            plt.plot(sl_weights, label='SL Weight', color='blue', linewidth=2)
            plt.title('RL vs SL Weight Evolution During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Weight')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/rl_sl_weights.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        print("ğŸ“Š Individual plots saved to 'plots/' directory:")

def load_sepsis_data(data_dir='preprocessed_sepsis_data'):
    """åŠ è½½é¢„å¤„ç†çš„è´¥è¡€ç—‡æ•°æ®"""
    data_dir = Path(data_dir)
    
    if not data_dir.exists():
        raise FileNotFoundError(f"Preprocessed data directory {data_dir} not found!")
    
    # åŠ è½½ç‰¹å¾ä¿¡æ¯
    feature_info = pd.read_csv(data_dir / 'feature_info.csv')
    feature_names = eval(feature_info['feature_names'].iloc[0])
    
    print(f"âœ… åŠ è½½ç‰¹å¾ä¿¡æ¯: {len(feature_names)} ä¸ªç‰¹å¾")
    
    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_features = np.load(data_dir / 'train_features.npy')
    val_features = np.load(data_dir / 'val_features.npy')
    train_metadata = pd.read_csv(data_dir / 'train_metadata.csv')
    val_metadata = pd.read_csv(data_dir / 'val_metadata.csv')
    
    print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {train_features.shape[0]} ä¸ªåºåˆ—")
    print(f"âœ… åŠ è½½éªŒè¯æ•°æ®: {val_features.shape[0]} ä¸ªåºåˆ—")
    
    # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼
    def create_sequences(features, metadata):
        sequences = []
        for i in range(len(features)):
            sequence_length = 7  # å‡è®¾å›ºå®šåºåˆ—é•¿åº¦
            sequences.append({
                'features': features[i].reshape(sequence_length, -1),
                'sequence_length': sequence_length
            })
        return sequences
    
    train_sequences = create_sequences(train_features, train_metadata)
    val_sequences = create_sequences(val_features, val_metadata)
    
    # æ•°æ®éªŒè¯
    valid_train_sequences = []
    for seq in train_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_train_sequences.append(seq)
    
    valid_val_sequences = []
    for seq in val_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_val_sequences.append(seq)
    
    print(f"âœ… æœ‰æ•ˆè®­ç»ƒåºåˆ—: {len(valid_train_sequences)}")
    print(f"âœ… æœ‰æ•ˆéªŒè¯åºåˆ—: {len(valid_val_sequences)}")
    
    input_dim = valid_train_sequences[0]['features'].shape[1] if valid_train_sequences else len(feature_names)
    
    return valid_train_sequences, valid_val_sequences, input_dim, feature_names

def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆ"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºmodelsç›®å½•
    Path('models').mkdir(exist_ok=True)
    
    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        train_sequences, val_sequences, input_dim, feature_names = load_sepsis_data()
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        agent = EnhancedSepsisLSTMAgent(
            input_dim=input_dim,
            hidden_dim=128,
            num_actions=20,
            sequence_length=7
        )
        
        # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
        reward_calculator = ClinicalRewardCalculator()
        
        # åˆ›å»ºè®­ç»ƒå™¨ - æ›´ç¨³å®šçš„å­¦ä¹ ç‡
        trainer = EnhancedTrainer(
            agent=agent,
            reward_calculator=reward_calculator,
            lr=3e-4,  # æ¢å¤æ­£å¸¸å­¦ä¹ ç‡
            device=device,
            expert_data_path=None  # è‡ªåŠ¨ä½¿ç”¨çœŸå®ä¸“å®¶æ•°æ®
        )
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(train_sequences, val_sequences, epochs=25)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        trainer.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        print("ğŸ“„ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        trainer.save_training_report()
        
        print("ğŸ‰ Enhanced training completed!")
        print("ğŸ“ Generated files:")
        print("  - models/enhanced_best_sepsis_agent.pth")
        print("  - enhanced_training_report.txt")
        print("  - models/enhanced_checkpoint_epoch_*.pth")
        print("  - plots/total_loss.png")
        print("  - plots/clinical_rewards.png")
        print("  - plots/actor_critic_expert_loss.png")
        print("  - plots/rl_sl_weights.png")
        print("  - plots/safety_violations.png")
        print("  - plots/gradient_norm.png")
        print("  - plots/learning_rate.png")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 