#!/usr/bin/env python3

import os
# è§£å†³macOSä¸Šçš„OpenMPå†²çªé—®é¢˜
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from collections import defaultdict
import pickle
import warnings
import json
import math # æ–°å¢ï¼šç”¨äºæ­£å¼¦ä½ç½®ç¼–ç 

warnings.filterwarnings('ignore')

class SafetyLayer(nn.Module):
    """å¢å¼ºçš„åŒ»å­¦å®‰å…¨å±‚ - ç»Ÿä¸€å¤„ç†å®‰å…¨çº¦æŸ"""
    
    def __init__(self, num_actions=20):
        super().__init__()
        self.num_actions = num_actions
        
        # åŸºäºçœŸå®ä¸´åºŠæŒ‡å—çš„å±é™©ç»„åˆ
        self.dangerous_combinations = [
            [1, 2],   # è¡€ç®¡åŠ å‹è¯ + é™å‹è¯ (ç›¸äº’æŠµæ¶ˆ)
            [3, 11],  # åˆ©å°¿å‰‚ + å¤§é‡ç”µè§£è´¨ (ç”µè§£è´¨ç´Šä¹±)
            [17, 18], # é•‡ç—›å‰‚ + é•‡é™å‰‚ (è¿‡åº¦é•‡é™)
        ]
        
        # éœ€è¦è°¨æ…ä½¿ç”¨çš„é«˜é£é™©åŠ¨ä½œ
        self.high_risk_actions = [1, 17, 18, 19]  # è¡€ç®¡åŠ å‹è¯ã€é•‡ç—›ã€é•‡é™ã€æŠ—å‡
        
    def apply_constraints(self, logits, latest_state):
        """åº”ç”¨åŒ»å­¦å®‰å…¨çº¦æŸ"""
        constrained_logits = logits.clone()
        action_probs = torch.softmax(logits, dim=-1)
        
        # çº¦æŸ1ï¼šé˜²æ­¢å±é™©è¯ç‰©ç»„åˆ
        for dangerous_combo in self.dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            high_risk_mask = combo_prob > 0.05
            if high_risk_mask.any():
                for action_idx in dangerous_combo:
                    constrained_logits[high_risk_mask, action_idx] -= 1.5
        
        # çº¦æŸ2ï¼šåŸºäºç”Ÿç†çŠ¶æ€çš„æ™ºèƒ½çº¦æŸ
        if latest_state is not None:
            # ä½è¡€å‹çº¦æŸ - ç¦æ­¢é™å‹è¯ï¼Œé¼“åŠ±è¡€ç®¡åŠ å‹è¯
            bp_systolic = latest_state[:, 3]
            low_bp_mask = bp_systolic < 85
            if low_bp_mask.any():
                constrained_logits[low_bp_mask, 2] -= 5.0  # ç¦æ­¢é™å‹è¯
                constrained_logits[low_bp_mask, 1] += 0.5  # è½»å¾®é¼“åŠ±è¡€ç®¡åŠ å‹è¯
            
            # é«˜è¡€å‹çº¦æŸ
            high_bp_mask = bp_systolic > 160
            if high_bp_mask.any():
                constrained_logits[high_bp_mask, 1] -= 3.0  # é™åˆ¶è¡€ç®¡åŠ å‹è¯
            
            # å¿ƒåŠ¨è¿‡é€Ÿçº¦æŸ
            heart_rate = latest_state[:, 1]
            high_hr_mask = heart_rate > 130
            if high_hr_mask.any():
                constrained_logits[high_hr_mask, 4] += 0.3  # é¼“åŠ±Î²å—ä½“é˜»æ»å‰‚
            
            # ä½æ°§çº¦æŸ
            spo2 = latest_state[:, 5]
            low_o2_mask = spo2 < 90
            if low_o2_mask.any():
                constrained_logits[low_o2_mask, 5] += 0.5  # é¼“åŠ±æ°§ç–—
        
        return constrained_logits

# âœ¨ æ–°å¢ï¼šå›ºå®šçš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç æ¨¡å—
class SinusoidalPositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor, shape [batch_size, seq_len, embedding_dim]
        """
        x = x + self.pe[:, :x.size(1)]
        return x

class EnhancedSepsisLSTMAgent(nn.Module):
    """å¢å¼ºçš„è´¥è¡€ç—‡LSTMæ™ºèƒ½ä½“ - åŒ…å«åŒ»å­¦çº¦æŸå’Œç›‘ç£å­¦ä¹ æ¨¡ä»¿"""
    
    def __init__(self, input_dim, hidden_dim=128, num_actions=20, sequence_length=7, expert_data_path=None):
        super(EnhancedSepsisLSTMAgent, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_actions = num_actions
        self.sequence_length = sequence_length
        
        # å¤šå±‚LSTMç”¨äºæ›´å¥½çš„æ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=False
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºå…³æ³¨é‡è¦çš„æ—¶é—´æ­¥
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )
        
        # Layer normalization for stability
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        
        # âœ¨ ä¿®æ­£ï¼šä½¿ç”¨å›ºå®šçš„æ­£å¼¦ä½ç½®ç¼–ç 
        self.pos_encoder = SinusoidalPositionalEncoding(hidden_dim, sequence_length)
        
        # âœ¨ æ”¹è¿›çš„Actorç½‘ç»œ - ç®€åŒ–æ¶æ„ï¼Œæ›´ç¨³å®š
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim, 128),  # temporal + medical
            nn.ReLU(),
            nn.Linear(128, num_actions)
        )
        
        # âœ¨ æ”¹è¿›çš„Criticç½‘ç»œ - ç®€åŒ–æ¶æ„ï¼Œæ›´ç¨³å®š
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        # âœ¨ æ”¹è¿›çš„åŒ»å­¦ç‰¹å¾æå–å™¨ - è¾“å‡ºç»´åº¦ä¸hidden_dimä¸€è‡´
        self.medical_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        
        # âœ¨ ä¸“å®¶æ¨¡ä»¿ç½‘ç»œ - ç®€åŒ–æ¶æ„
        self.expert_imitation_net = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_actions)
        )
        
        # âœ¨ ä½¿ç”¨softmaxå½’ä¸€åŒ–çš„èåˆæƒé‡
        self.rl_weight = nn.Parameter(torch.tensor(1.0))
        self.sl_weight = nn.Parameter(torch.tensor(1.0))
        
        # âœ¨ ç»Ÿä¸€çš„å®‰å…¨å±‚
        self.safety_layer = SafetyLayer(num_actions)
        
        # debug flag
        self._printed = False
        
        # ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                module.bias.data.fill_(0.01)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    torch.nn.init.xavier_uniform_(param.data, gain=0.5)
                elif 'weight_hh' in name:
                    torch.nn.init.orthogonal_(param.data, gain=0.5)
                elif 'bias' in name:
                    param.data.fill_(0.01)
    
    def _normalize_weights(self):
        """âœ¨ ä½¿ç”¨softmaxå½’ä¸€åŒ–æƒé‡ï¼Œé¿å…é™¤é›¶"""
        w = torch.softmax(torch.stack([self.rl_weight, self.sl_weight]), dim=0)
        return w[0], w[1]
    
    def forward(self, sequences, sequence_lengths=None, use_expert_imitation=True):
        """å‰å‘ä¼ æ’­ - å¢åŠ ä¸“å®¶æ¨¡ä»¿åŠŸèƒ½"""
        batch_size, T, _ = sequences.shape
        
        # LSTMå¤„ç†æ—¶åºæ•°æ®
        lstm_output, (h_n, c_n) = self.lstm(sequences)
        
        # Layer normalization
        lstm_output = self.layer_norm1(lstm_output)
        
        # âœ¨ ä¿®æ­£ï¼šåº”ç”¨å›ºå®šçš„ä½ç½®ç¼–ç 
        lstm_output_with_pos = self.pos_encoder(lstm_output)
        
        # âœ¨ æ³¨æ„åŠ›æœºåˆ¶ - æ˜¾å¼æå–æƒé‡
        attended_output, attention_weights = self.attention(
            lstm_output_with_pos, lstm_output_with_pos, lstm_output_with_pos,
            need_weights=True, average_attn_weights=False  # ä¿ç•™ [B, heads, T, T]
        )
        
        # ç»“åˆLSTMå’Œæ³¨æ„åŠ›è¾“å‡º (ä¿®æ­£ï¼šæ®‹å·®è¿æ¥åº”ä½¿ç”¨æ·»åŠ äº†ä½ç½®ç¼–ç çš„è¾“å…¥)
        combined_output = lstm_output_with_pos + attended_output
        combined_output = self.layer_norm2(combined_output)
        
        # âœ¨ å–æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥
        if sequence_lengths is not None:
            # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡
            seq_lengths_tensor = torch.tensor(sequence_lengths, device=combined_output.device)
            idx = (seq_lengths_tensor - 1).clamp(0, T-1)
            temporal_context = combined_output[torch.arange(batch_size), idx, :]
        else:
            temporal_context = combined_output[:, -1, :]
        
        # âœ¨ åŒ»å­¦ç‰¹å¾æå–ï¼ˆä½¿ç”¨æœ€æ–°çš„åŒ»å­¦æ•°æ®ï¼‰
        med_feat_raw = sequences[:, -1, :]  # å–æœ€æ–°è§‚æµ‹
        medical_features = self.medical_feature_extractor(med_feat_raw)
        
        # âœ¨ èåˆæ—¶åºå’ŒåŒ»å­¦ç‰¹å¾
        fused_features = torch.cat([temporal_context, medical_features], dim=1)   # [B, 2H]
        
        # âœ¨ Actorè¾“å‡º
        rl_action_logits = self.actor(fused_features)
        expert_action_logits = self.expert_imitation_net(fused_features)
        
        # âœ¨ ä½¿ç”¨æ”¹è¿›çš„æƒé‡èåˆ
        if use_expert_imitation:
            w_rl, w_sl = self._normalize_weights()
            fused_action_logits = w_rl * rl_action_logits + w_sl * expert_action_logits
        else:
            fused_action_logits = rl_action_logits
        
        # âœ¨ ä½¿ç”¨ç»Ÿä¸€çš„å®‰å…¨å±‚
        constrained_action_logits = self.safety_layer.apply_constraints(fused_action_logits, med_feat_raw)
        
        # âœ¨ Criticè¾“å‡º
        state_values = self.critic(fused_features)
        
        # âœ¨ debug æ‰“å°ä¸€æ¬¡å½¢çŠ¶
        if not self._printed:
            print("âœ¨ Enhanced Model Debug Info:")
            print(f"   - attention_weights shape: {attention_weights.shape}")   # æœŸæœ› [B, heads, T, T]
            print(f"   - fused_features shape: {fused_features.shape}")
            self._printed = True
        
        # âœ¨ è¿”å›æ›´æœ‰æ„ä¹‰çš„è¾“å‡ºï¼ŒåŒ…æ‹¬èåˆåçš„logits
        return {
            'constrained_logits': constrained_action_logits,
            'fused_logits': fused_action_logits,  # æ–°å¢ï¼šèåˆåä½†æœªçº¦æŸçš„logits
            'state_values': state_values,
            'temporal_features': combined_output,
            'attention_weights': attention_weights,
            'rl_logits': rl_action_logits,
            'expert_logits': expert_action_logits,
            'temporal_context': temporal_context  # æ–°å¢ï¼šæ³¨æ„åŠ›åŠ æƒçš„ä¸Šä¸‹æ–‡
        }

class ClinicalRewardCalculator:
    """ä¸´åºŠç›¸å…³çš„å¥–åŠ±è®¡ç®—å™¨ - åŸºäºåŒ»å­¦æŒ‡æ ‡çš„å¥–åŠ±"""
    
    def __init__(self):
        # é‡æ–°è®¾è®¡çš„ä¸´åºŠæŒ‡æ ‡æƒé‡ - æ›´åŠ æ³¨é‡SOFAè¯„åˆ†å’Œç”Ÿå‘½ä½“å¾æ”¹å–„
        self.vital_signs_weight = 0.4  # å¢åŠ ç”Ÿå‘½ä½“å¾æƒé‡
        self.sofa_weight = 0.5         # å¢åŠ SOFAè¯„åˆ†æƒé‡
        self.stability_weight = 0.15   # é€‚å½“é™ä½ç¨³å®šæ€§æƒé‡
        self.safety_weight = 0.05      # é™ä½å®‰å…¨æƒé‡ï¼Œé¿å…è¿‡åº¦ä¿å®ˆ
        
        # æ­£å¸¸å€¼èŒƒå›´ (åŸºäºä¸´åºŠæ ‡å‡†) - æ‰©å¤§ä¸€äº›èŒƒå›´ä»¥å¢åŠ æ•æ„Ÿåº¦
        self.normal_ranges = {
            'temperature': (35.5, 38.0),   # ç¨å¾®æ‰©å¤§æ¸©åº¦èŒƒå›´
            'heart_rate': (50, 110),       # æ‰©å¤§å¿ƒç‡èŒƒå›´
            'respiratory_rate': (10, 24),  # æ‰©å¤§å‘¼å¸é¢‘ç‡èŒƒå›´
            'arterial_blood_pressure_systolic': (85, 150),
            'arterial_blood_pressure_diastolic': (55, 95),
            'o2_saturation_pulseoxymetry': (92, 100),  # ç¨å¾®é™ä½æ°§é¥±å’Œåº¦ä¸‹é™
            'glucose_(serum)': (60, 200),  # æ‰©å¤§è¡€ç³–èŒƒå›´
            'gcs_-_eye_opening': (3, 4),
            'gcs_-_motor_response': (4, 6),  # æ‰©å¤§è¿åŠ¨ååº”èŒƒå›´
            'gcs_-_verbal_response': (3, 5)  # æ‰©å¤§è¨€è¯­ååº”èŒƒå›´
        }
        
    def calculate_vital_signs_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾çš„å¥–åŠ±è®¡ç®— - æ”¹è¿›ç‰ˆ"""
        batch_size = features.size(0)
        rewards = torch.zeros(batch_size)
        
        # ç‰¹å¾åç§°æ˜ å°„åˆ°ç´¢å¼•
        feature_names = ['temperature', 'heart_rate', 'respiratory_rate', 
                        'arterial_blood_pressure_systolic', 'arterial_blood_pressure_diastolic',
                        'o2_saturation_pulseoxymetry', 'glucose_(serum)', 'gcs_-_eye_opening',
                        'gcs_-_motor_response', 'gcs_-_verbal_response', 'age', 'sofa_score']
        
        # è®¡ç®—æ—¶åºæ”¹å–„å¥–åŠ± - å…³æ³¨ç”Ÿå‘½ä½“å¾çš„æ”¹å–„è¶‹åŠ¿
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            current_values = features[:, -1, :10]  # å½“å‰æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            previous_values = features[:, -2, :10]  # å‰ä¸€æ—¶é—´æ­¥çš„ç”Ÿå‘½ä½“å¾
            
            improvement_reward = 0.0
            for i, feature_name in enumerate(feature_names[:10]):
                if feature_name in self.normal_ranges:
                    normal_min, normal_max = self.normal_ranges[feature_name]
                    normal_center = (normal_min + normal_max) / 2
                    
                    # è®¡ç®—å½“å‰å’Œä¹‹å‰è·ç¦»æ­£å¸¸ä¸­å¿ƒçš„è·ç¦»
                    current_distance = torch.abs(current_values[:, i] - normal_center)
                    previous_distance = torch.abs(previous_values[:, i] - normal_center)
                    
                    # æ”¹å–„å¥–åŠ± - è·ç¦»æ­£å¸¸å€¼æ›´è¿‘è·å¾—æ­£å¥–åŠ±
                    improvement = previous_distance - current_distance
                    normalized_improvement = improvement / (normal_max - normal_min)
                    improvement_reward += normalized_improvement * 0.3  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.3å¥–åŠ±
            
            rewards += improvement_reward
        
        # å½“å‰çŠ¶æ€çš„æ­£å¸¸æ€§å¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„è®¡ç®—æ–¹å¼
        current_state_reward = 0.0
        for i, feature_name in enumerate(feature_names[:10]):
            if feature_name in self.normal_ranges:
                normal_min, normal_max = self.normal_ranges[feature_name]
                values = features[:, -1, i]  # å½“å‰æ—¶é—´æ­¥çš„å€¼
                
                # ä½¿ç”¨åˆ†æ®µçº¿æ€§å‡½æ•°è€ŒéæŒ‡æ•°å‡½æ•°ï¼Œå¢åŠ æ•æ„Ÿåº¦
                in_range_mask = (values >= normal_min) & (values <= normal_max)
                below_range_mask = values < normal_min
                above_range_mask = values > normal_max
                
                # åœ¨æ­£å¸¸èŒƒå›´å†…ï¼šæ»¡åˆ†
                feature_reward = torch.zeros_like(values)
                feature_reward[in_range_mask] = 1.0
                
                # åœ¨èŒƒå›´å¤–ï¼šçº¿æ€§é€’å‡ï¼Œè€ŒéæŒ‡æ•°é€’å‡
                range_width = normal_max - normal_min
                feature_reward[below_range_mask] = torch.clamp(
                    1.0 - (normal_min - values[below_range_mask]) / range_width, 
                    min=0.0, max=1.0
                )
                feature_reward[above_range_mask] = torch.clamp(
                    1.0 - (values[above_range_mask] - normal_max) / range_width, 
                    min=0.0, max=1.0
                )
                
                current_state_reward += feature_reward * 0.15  # æ¯ä¸ªç‰¹å¾æœ€å¤šè´¡çŒ®0.15å¥–åŠ±
        
        rewards += current_state_reward
        return rewards
    
    def calculate_sofa_reward(self, features):
        """åŸºäºSOFAè¯„åˆ†çš„å¥–åŠ± - æ”¹è¿›ç‰ˆï¼Œæ›´æ³¨é‡æ”¹å–„è¶‹åŠ¿"""
        current_sofa = features[:, -1, -1]  # å½“å‰SOFAè¯„åˆ†
        
        # åŸºç¡€SOFAå¥–åŠ± - ä½¿ç”¨æ›´æ•æ„Ÿçš„åˆ†æ®µå‡½æ•°
        base_sofa_reward = torch.zeros_like(current_sofa)
        
        # ä¼˜ç§€çŠ¶æ€ (SOFA 0-6): é«˜å¥–åŠ±
        excellent_mask = current_sofa <= 6
        base_sofa_reward[excellent_mask] = 2.0 - current_sofa[excellent_mask] / 6.0
        
        # ä¸­ç­‰çŠ¶æ€ (SOFA 7-12): ä¸­ç­‰å¥–åŠ±
        moderate_mask = (current_sofa > 6) & (current_sofa <= 12)
        base_sofa_reward[moderate_mask] = 1.0 - (current_sofa[moderate_mask] - 6) / 12.0
        
        # ä¸¥é‡çŠ¶æ€ (SOFA 13-18): ä½å¥–åŠ±
        severe_mask = (current_sofa > 12) & (current_sofa <= 18)
        base_sofa_reward[severe_mask] = 0.5 - (current_sofa[severe_mask] - 12) / 12.0
        
        # å±é‡çŠ¶æ€ (SOFA >18): æä½å¥–åŠ±
        critical_mask = current_sofa > 18
        base_sofa_reward[critical_mask] = 0.1
        
        # SOFAæ”¹å–„å¥–åŠ± - å…³æ³¨è¯„åˆ†çš„æ”¹å–„
        improvement_reward = torch.zeros_like(current_sofa)
        if features.size(1) >= 2:  # è‡³å°‘æœ‰2ä¸ªæ—¶é—´æ­¥
            previous_sofa = features[:, -2, -1]
            sofa_improvement = previous_sofa - current_sofa  # è¯„åˆ†é™ä½æ˜¯å¥½äº‹
            
            # æ”¹å–„å¥–åŠ±ï¼šæ¯é™ä½1åˆ†SOFAè¯„åˆ†è·å¾—0.5å¥–åŠ±
            improvement_reward = torch.clamp(sofa_improvement * 0.5, min=-1.0, max=2.0)
        
        total_sofa_reward = base_sofa_reward + improvement_reward
        return torch.clamp(total_sofa_reward, min=0.0, max=3.0)
    
    def calculate_stability_reward(self, features):
        """åŸºäºç”Ÿå‘½ä½“å¾ç¨³å®šæ€§çš„å¥–åŠ±"""
        if features.size(1) < 2:  # åºåˆ—é•¿åº¦å°äº2ï¼Œæ— æ³•è®¡ç®—ç¨³å®šæ€§
            return torch.zeros(features.size(0))
        
        # è®¡ç®—ç”Ÿå‘½ä½“å¾çš„å˜åŒ–ç‡
        vital_signs = features[:, :, :10]  # å‰10ä¸ªç‰¹å¾æ˜¯ç”Ÿå‘½ä½“å¾
        changes = torch.abs(vital_signs[:, 1:] - vital_signs[:, :-1])
        
        # ç¨³å®šæ€§å¥–åŠ±ï¼šå˜åŒ–è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
        stability = torch.exp(-changes.mean(dim=(1, 2)))
        return stability
    
    def calculate_safety_constraint_reward(self, action_logits):
        """åŸºäºå®‰å…¨çº¦æŸçš„å¥–åŠ±"""
        action_probs = torch.softmax(action_logits, dim=-1)
        
        # é¼“åŠ±é€‚åº¦çš„åŠ¨ä½œé€‰æ‹©ï¼ˆé¿å…æç«¯æ²»ç–—ï¼‰
        entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        target_entropy = np.log(action_logits.size(-1)) * 0.6  # ç›®æ ‡ç†µ
        
        # ç†µæ¥è¿‘ç›®æ ‡å€¼æ—¶å¥–åŠ±æœ€é«˜
        entropy_reward = torch.exp(-torch.abs(entropy - target_entropy))
        return entropy_reward
    
    def calculate_reward(self, state_features, action_logits, episode_step=0):
        """
        ç»¼åˆä¸´åºŠå¥–åŠ±è®¡ç®— - **ä¿®æ­£ç‰ˆ**: ç§»é™¤äº†ä¸ç¨³å®šçš„éšæœºæ¢ç´¢å¥–åŠ±
        """
        # å„é¡¹å¥–åŠ±è®¡ç®—
        vital_reward = self.calculate_vital_signs_reward(state_features)
        sofa_reward = self.calculate_sofa_reward(state_features)
        stability_reward = self.calculate_stability_reward(state_features)
        safety_reward = self.calculate_safety_constraint_reward(action_logits)
        
        # åŠ æƒç»„åˆ - ä½¿ç”¨æ–°çš„æƒé‡
        total_reward = (
            self.vital_signs_weight * vital_reward +
            self.sofa_weight * sofa_reward +
            self.stability_weight * stability_reward +
            self.safety_weight * safety_reward
        )
        
        # æ”¹è¿›çš„ç”Ÿå­˜æ—¶é—´å¥–åŠ± - æ›´å…·åŠ¨æ€æ€§
        survival_bonus = torch.full_like(total_reward, min(episode_step * 0.02, 0.3))
        
        # **ä¿®æ­£**: ç§»é™¤äº†éšæœºæ¢ç´¢å¥–åŠ±
        # exploration_bonus = torch.randn_like(total_reward) * 0.05
        
        final_reward = total_reward + survival_bonus
        
        # æ‰©å¤§å¥–åŠ±èŒƒå›´ï¼Œç§»é™¤è¿‡äºä¸¥æ ¼çš„é™åˆ¶
        return torch.clamp(final_reward, min=-1.0, max=5.0)

class ExpertDataLoader:
    """ä¸“å®¶æ•°æ®åŠ è½½å™¨ - ç”¨äºç›‘ç£å­¦ä¹ """
    
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_actions = None
        self.expert_states = None
        
    def load_expert_data(self):
        """
        åŠ è½½ä¸“å®¶å†³ç­–æ•°æ® - âœ¨ ä¿®æ­£ç‰ˆ: å¢å¼ºäº†è§£æCSVä¸­å­—ç¬¦ä¸²åˆ—è¡¨çš„é²æ£’æ€§
        """
        if self.expert_data_path is None or not Path(self.expert_data_path).exists():
            print(f"âš ï¸ ä¸“å®¶æ•°æ®æ–‡ä»¶æœªåœ¨ '{self.expert_data_path}' æ‰¾åˆ°ã€‚å°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ã€‚")
            self.generate_simulated_expert_data()
            return

        print(f"âœ… æ‰¾åˆ°ä¸“å®¶æ•°æ®æ–‡ä»¶: {self.expert_data_path}")
        try:
            expert_df = pd.read_csv(self.expert_data_path)
            
            self.expert_actions = expert_df['expert_action'].values.astype(np.int64)
            
            states_list = []
            # éå† 'state_features' åˆ—ä¸­çš„æ¯ä¸€ä¸ªå­—ç¬¦ä¸²
            for state_str in expert_df['state_features']:
                try:
                    # 1. å°† 'nan' æ›¿æ¢ä¸º 'null'ï¼Œä½¿å…¶æˆä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼
                    # 2. ç§»é™¤å¤šä½™çš„å¼•å·ï¼Œä»¥é˜²ä¸‡ä¸€
                    cleaned_str = state_str.replace('nan', 'null').strip('"')
                    
                    # 3. ä½¿ç”¨jsonåŠ è½½å­—ç¬¦ä¸²ä¸ºPythonåˆ—è¡¨
                    parsed_list = json.loads(cleaned_str)
                    
                    # 4. å°†åˆ—è¡¨ä¸­çš„ 'null' æ›¿æ¢ä¸º 0.0ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æµ®ç‚¹æ•°
                    state_row = [float(x) if x is not None else 0.0 for x in parsed_list]
                    states_list.append(state_row)
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"   - âš ï¸ è§£æè¡Œå¤±è´¥: '{state_str[:50]}...'. é”™è¯¯: {e}. ä½¿ç”¨é›¶å‘é‡å¡«å……ã€‚")
                    # å¦‚æœæŸä¸€è¡Œè§£æå¤±è´¥ï¼Œç”¨ä¸€ä¸ªé›¶å‘é‡å¡«å……ä»¥ä¿æŒæ•°æ®å¯¹é½
                    # è¿™é‡Œçš„ç»´åº¦éœ€è¦ä¸æˆåŠŸè§£æçš„è¡Œä¸€è‡´ï¼Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªå ä½ç¬¦
                    states_list.append(None) 
            
            # ç¡®å®šç‰¹å¾ç»´åº¦ (ä»ç¬¬ä¸€ä¸ªæˆåŠŸè§£æçš„è¡Œ)
            feature_dim = 0
            for row in states_list:
                if row is not None:
                    feature_dim = len(row)
                    break
            
            if feature_dim == 0:
                raise ValueError("æœªèƒ½ä»ä¸“å®¶æ•°æ®ä¸­ç¡®å®šç‰¹å¾ç»´åº¦ã€‚")

            # å¡«å……å¤±è´¥çš„è¡Œ
            final_states_list = [row if row is not None else [0.0] * feature_dim for row in states_list]

            self.expert_states = np.array(final_states_list, dtype=np.float32)
            
            print(f"âœ… æˆåŠŸåŠ è½½å¹¶è§£æä¸“å®¶æ•°æ®: {len(self.expert_actions)} ä¸ªæ ·æœ¬")
            print(f"   - ä¸“å®¶çŠ¶æ€å½¢çŠ¶: {self.expert_states.shape}")
            print(f"   - ä¸“å®¶åŠ¨ä½œå½¢çŠ¶: {self.expert_actions.shape}")

        except Exception as e:
            print(f"âŒ åŠ è½½ä¸“å®¶æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}ã€‚å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€‚")
            import traceback
            traceback.print_exc()
            self.generate_simulated_expert_data()
    
    def generate_simulated_expert_data(self, num_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶å†³ç­–æ•°æ®"""
        # åŸºäºåŒ»å­¦è§„åˆ™ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶å†³ç­–
        np.random.seed(42)
        
        # æ¨¡æ‹ŸçŠ¶æ€ç‰¹å¾ (temperature, heart_rate, bp_sys, bp_dias, spo2, ...)
        states = []
        actions = []
        
        for _ in range(num_samples):
            # ç”Ÿæˆæ‚£è€…çŠ¶æ€
            temp = np.random.normal(37.0, 1.5)  # ä½“æ¸©
            hr = np.random.normal(80, 20)       # å¿ƒç‡
            bp_sys = np.random.normal(120, 25)  # æ”¶ç¼©å‹
            bp_dias = np.random.normal(80, 15)  # èˆ’å¼ å‹
            spo2 = np.random.normal(96, 4)      # æ°§é¥±å’Œåº¦
            
            # å…¶ä»–ç‰¹å¾
            other_features = np.random.normal(0, 1, 7)  # å…¶ä»–12-5=7ä¸ªç‰¹å¾
            state = np.array([temp, hr, 0, bp_sys, bp_dias, spo2] + list(other_features))
            
            # åŸºäºçŠ¶æ€çš„ä¸“å®¶å†³ç­–è§„åˆ™
            action = self.expert_decision_rule(temp, hr, bp_sys, bp_dias, spo2)
            
            states.append(state)
            actions.append(action)
        
        self.expert_states = np.array(states, dtype=np.float32)
        self.expert_actions = np.array(actions, dtype=np.int64)
        print(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿä¸“å®¶æ•°æ®: {len(self.expert_actions)} ä¸ªæ ·æœ¬")
        print(f"   æ¨¡æ‹ŸçŠ¶æ€å½¢çŠ¶: {self.expert_states.shape}, æ•°æ®ç±»å‹: {self.expert_states.dtype}")
        print(f"   æ¨¡æ‹ŸåŠ¨ä½œå½¢çŠ¶: {self.expert_actions.shape}, æ•°æ®ç±»å‹: {self.expert_actions.dtype}")
    
    def expert_decision_rule(self, temp, hr, bp_sys, bp_dias, spo2):
        """åŸºäºåŒ»å­¦çŸ¥è¯†çš„ä¸“å®¶å†³ç­–è§„åˆ™"""
        # å‘çƒ­ -> é™æ¸©è¯ç‰©
        if temp > 38.5:
            return 1  # é™æ¸©è¯ç‰©
        
        # å¿ƒåŠ¨è¿‡é€Ÿ -> Î²å—ä½“é˜»æ»å‰‚
        elif hr > 100:
            return 2  # Î²å—ä½“é˜»æ»å‰‚
        
        # ä½è¡€å‹ -> è¡€ç®¡åŠ å‹è¯
        elif bp_sys < 90:
            return 3  # è¡€ç®¡åŠ å‹è¯
        
        # é«˜è¡€å‹ -> é™å‹è¯
        elif bp_sys > 140:
            return 4  # é™å‹è¯
        
        # ä½æ°§ -> æ°§ç–—
        elif spo2 < 92:
            return 5  # æ°§ç–—
        
        # æ­£å¸¸æƒ…å†µ -> è§‚å¯Ÿ
        else:
            return 0  # è§‚å¯Ÿ/ç»´æŒæ²»ç–—
    
    def get_expert_batch(self, batch_size=32):
        """è·å–ä¸“å®¶æ•°æ®æ‰¹æ¬¡"""
        if self.expert_states is None:
            self.load_expert_data()
        
        indices = np.random.choice(len(self.expert_actions), batch_size, replace=True)
        
        # ç¡®ä¿è¿”å›çš„æ•°æ®ç±»å‹æ­£ç¡®
        batch_states = self.expert_states[indices].astype(np.float32)
        batch_actions = self.expert_actions[indices].astype(np.int64)
        
        return batch_states, batch_actions

class EnhancedTrainer:
    """å¢å¼ºçš„è®­ç»ƒå™¨ - åŒ…å«åŒ»å­¦å®‰å…¨çº¦æŸå’Œç›‘ç£å­¦ä¹ """
    
    def __init__(self, agent, reward_calculator, lr=5e-5, device='cpu', expert_data_path=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.agent = agent.to(device)
        self.reward_calculator = reward_calculator
        self.device = device
        # âœ¨ ä½¿ç”¨æ¨¡å‹å†…ç½®çš„å®‰å…¨å±‚
        # self.safety_constraints = self.agent.safety_layer
        
        # ä¸“å®¶æ•°æ®åŠ è½½å™¨ - ä¼˜å…ˆå°è¯•çœŸå®æ•°æ®
        if expert_data_path is None:
            # å°è¯•åŠ è½½çœŸå®ä¸“å®¶æ•°æ®
            real_expert_path = "data_pipeline/processed_data/expert_data/expert_decisions.csv"
            if Path(real_expert_path).exists():
                print(f"ğŸ‘©â€âš•ï¸ æ‰¾åˆ°çœŸå®ä¸“å®¶æ•°æ®: {real_expert_path}")
                expert_data_path = real_expert_path
            else:
                print("âš ï¸ æœªæ‰¾åˆ°çœŸå®ä¸“å®¶æ•°æ®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        
        self.expert_loader = ExpertDataLoader(expert_data_path)
        self.expert_loader.load_expert_data()
        
        # æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = optim.AdamW(
            self.agent.parameters(), 
            lr=lr, 
            weight_decay=1e-3,  # å¢åŠ æ­£åˆ™åŒ–
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=3
        )
        
        self.huber_loss = nn.SmoothL1Loss()  # æ›´ç¨³å®šçš„æŸå¤±å‡½æ•°
        self.global_step = 0
        
        # å¢å¼ºçš„å†å²è®°å½•
        self.history = {
            'train_total_loss': [],
            'train_actor_loss': [],
            'train_critic_loss': [],
            'train_expert_loss': [],
            'train_reward': [],
            'val_loss': [],
            'val_reward': [],
            'learning_rate': [],
            'gradient_norm': [],
            'safety_violations': [],
            'rl_sl_weights': [],
            'val_clinical_metrics': [] # æ–°å¢ï¼šä¿å­˜éªŒè¯æ—¶çš„ä¸´åºŠæŒ‡æ ‡
        }
    
    def prepare_batch(self, sequences, batch_size=8):
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡"""
        np.random.shuffle(sequences)
        
        for i in range(0, len(sequences), batch_size):
            batch_sequences = sequences[i:i + batch_size]
            
            # å †å ç‰¹å¾
            features = torch.tensor([seq['features'] for seq in batch_sequences], 
                                   dtype=torch.float32, device=self.device)
            
            # åºåˆ—é•¿åº¦
            seq_lengths = [seq['sequence_length'] for seq in batch_sequences]
            
            # æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼
            features = torch.clamp(features, min=-10, max=10)
            features = torch.where(torch.isnan(features), torch.zeros_like(features), features)
            
            yield features, seq_lengths
    
    def train_epoch(self, train_sequences, epoch):
        """
        å¢å¼ºçš„è®­ç»ƒepoch - **ä¿®æ­£ç‰ˆ**: ä½¿ç”¨æ ‡å‡†çš„Actor-CriticæŸå¤±å‡½æ•°
        """
        self.agent.train()
        epoch_actor_losses = []
        epoch_critic_losses = []
        epoch_expert_losses = []
        epoch_rewards = []
        epoch_safety_violations = []
        epoch_gradient_norms = []
        
        for batch_idx, (features, seq_lengths) in enumerate(self.prepare_batch(train_sequences)):
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - ä¿®å¤ï¼šå¤„ç†æ–°çš„å­—å…¸è¿”å›æ ¼å¼
            outputs = self.agent(features, seq_lengths)
            action_logits = outputs['constrained_logits']
            state_values = outputs['state_values']
            expert_action_logits = outputs['expert_logits']
            
            # âœ¨ è®¡ç®—å¥–åŠ± - ä½¿ç”¨å·²çº¦æŸçš„logits
            rewards = self.reward_calculator.calculate_reward(
                features, action_logits, episode_step=self.global_step
            ).to(self.device)
            
            # --- Critic Loss è®¡ç®— ---
            state_values = state_values.squeeze()
            # ä½¿ç”¨HuberæŸå¤±ä»¥å¢åŠ ç¨³å®šæ€§
            critic_loss = self.huber_loss(state_values, rewards)
            
            # --- Actor Loss è®¡ç®— ---
            # è®¡ç®—Advantage
            advantages = (rewards - state_values).detach() # ä½¿ç”¨.detach()æ¥é˜»æ­¢æ¢¯åº¦æµå‘Critic
            
            # æ ‡å‡†åŒ–Advantage
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # è®¡ç®—åŠ¨ä½œåˆ†å¸ƒå’Œç†µ
            action_dist = torch.distributions.Categorical(logits=action_logits)
            log_probs = action_dist.log_prob(action_dist.sample())
            entropy_loss = -0.01 * action_dist.entropy().mean() # ç†µæ­£åˆ™åŒ–ï¼Œé¼“åŠ±æ¢ç´¢
            
            # **ä¿®æ­£**: æ ‡å‡†çš„ActoræŸå¤± (ç­–ç•¥æ¢¯åº¦æŸå¤±)
            actor_loss = -(log_probs * advantages).mean()
            
            # --- ä¸“å®¶æ¨¡ä»¿æŸå¤± (Supervised Learning) ---
            expert_loss = self.calculate_expert_imitation_loss(
                features, expert_action_logits, batch_idx
            )
            
            # --- å®‰å…¨çº¦æŸæŸå¤± ---
            safety_violation = self.calculate_safety_violations(torch.softmax(action_logits, dim=-1))
            safety_loss = 0.1 * safety_violation
            
            # --- æ€»æŸå¤± ---
            # **ä¿®æ­£**: å¹³è¡¡å„é¡¹æŸå¤±ï¼Œç§»é™¤ä¸å¿…è¦çš„clamp
            total_loss = critic_loss + actor_loss + entropy_loss + 0.5 * expert_loss + safety_loss
            
            # æ£€æŸ¥å¼‚å¸¸æŸå¤±å€¼
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"å¼‚å¸¸æŸå¤±æ£€æµ‹ epoch {epoch}, batch {batch_idx}: "
                      f"total={total_loss.item():.6f}, actor={actor_loss.item():.6f}, "
                      f"critic={critic_loss.item():.6f}. è·³è¿‡...")
                continue
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # **ä¿®æ­£**: ä½¿ç”¨æ›´åˆç†çš„æ¢¯åº¦è£å‰ªå€¼
            grad_norm = torch.nn.utils.clip_grad_norm_(self.agent.parameters(), max_norm=1.0)
            epoch_gradient_norms.append(grad_norm.item())
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            epoch_actor_losses.append(actor_loss.item())
            epoch_critic_losses.append(critic_loss.item())
            epoch_expert_losses.append(expert_loss.item())
            epoch_rewards.append(rewards.mean().item())
            epoch_safety_violations.append(safety_violation.item())
            
            self.global_step += 1
            
            if batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}: "
                      f"Total Loss = {total_loss.item():.4f}, "
                      f"Actor Loss = {actor_loss.item():.4f}, "
                      f"Critic Loss = {critic_loss.item():.4f}, "
                      f"Expert Loss = {expert_loss.item():.4f}, "
                      f"Avg Reward = {rewards.mean().item():.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_actor_loss = np.mean(epoch_actor_losses) if epoch_actor_losses else 0
        avg_critic_loss = np.mean(epoch_critic_losses) if epoch_critic_losses else 0
        avg_expert_loss = np.mean(epoch_expert_losses) if epoch_expert_losses else 0
        avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0
        avg_safety_violations = np.mean(epoch_safety_violations) if epoch_safety_violations else 0
        avg_grad_norm = np.mean(epoch_gradient_norms) if epoch_gradient_norms else 0
        
        total_avg_loss = avg_critic_loss + avg_actor_loss + 0.5 * avg_expert_loss + 0.1 * avg_safety_violations
        
        # æ›´æ–°å†å²è®°å½•
        self.history['train_actor_loss'].append(avg_actor_loss)
        self.history['train_critic_loss'].append(avg_critic_loss)
        self.history['train_expert_loss'].append(avg_expert_loss)
        self.history['train_total_loss'].append(total_avg_loss)
        self.history['train_reward'].append(avg_reward)
        self.history['safety_violations'].append(avg_safety_violations)
        self.history['gradient_norm'].append(avg_grad_norm)
        self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])
        self.history['rl_sl_weights'].append([self.agent.rl_weight.item(), self.agent.sl_weight.item()])
        
        return total_avg_loss, avg_reward
    
    def calculate_safety_violations(self, action_probs):
        """âœ¨ è®¡ç®—å®‰å…¨è¿è§„æƒ…å†µ - ä½¿ç”¨æ¨¡å‹å†…ç½®å®‰å…¨å±‚çš„è§„åˆ™"""
        violations = torch.zeros(action_probs.size(0), device=self.device)
        
        # âœ¨ ä½¿ç”¨æ¨¡å‹å†…ç½®å®‰å…¨å±‚çš„å±é™©ç»„åˆè§„åˆ™
        dangerous_combinations = [
            [1, 2],   # è¡€ç®¡åŠ å‹è¯ + é™å‹è¯ (ç›¸äº’æŠµæ¶ˆ)
            [3, 11],  # åˆ©å°¿å‰‚ + å¤§é‡ç”µè§£è´¨ (ç”µè§£è´¨ç´Šä¹±)
            [17, 18], # é•‡ç—›å‰‚ + é•‡é™å‰‚ (è¿‡åº¦é•‡é™)
        ]
        high_risk_actions = [1, 17, 18, 19]  # è¡€ç®¡åŠ å‹è¯ã€é•‡ç—›ã€é•‡é™ã€æŠ—å‡
        
        # æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ - æ›´ä¸¥æ ¼
        for dangerous_combo in dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            violations += combo_prob * 2.0  # åŠ é‡å±é™©ç»„åˆæƒ©ç½š
        
        # æ£€æŸ¥é«˜é£é™©åŠ¨ä½œ
        high_risk_probs = action_probs[:, high_risk_actions]
        violations += high_risk_probs.sum(dim=1) * 1.0  # æé«˜é«˜é£é™©åŠ¨ä½œæƒ©ç½š
        
        # æ£€æŸ¥åŠ¨ä½œåˆ†å¸ƒé›†ä¸­åº¦ - è¿‡äºé›†ä¸­ä¹Ÿä¸å®‰å…¨
        max_prob = torch.max(action_probs, dim=1)[0]
        violations += torch.clamp(max_prob - 0.8, min=0) * 0.5  # å•ä¸€åŠ¨ä½œæ¦‚ç‡è¿‡é«˜
        
        return violations.mean()
    
    def calculate_expert_imitation_loss(self, current_features, expert_logits, batch_idx):
        """è®¡ç®—ä¸“å®¶æ¨¡ä»¿æŸå¤±"""
        batch_size = current_features.size(0)
        
        # è·å–ä¸“å®¶æ•°æ®æ‰¹æ¬¡
        expert_states, expert_actions = self.expert_loader.get_expert_batch(batch_size)
        
        # å®‰å…¨åœ°è½¬æ¢ä¸ºå¼ é‡ - å¤„ç†objectæ•°ç»„é—®é¢˜
        try:
            # ç¡®ä¿expert_statesæ˜¯æ•°å€¼æ•°ç»„
            if expert_states.dtype == np.object_:
                # å¦‚æœæ˜¯objectæ•°ç»„ï¼Œå°è¯•è½¬æ¢ä¸ºfloatæ•°ç»„
                expert_states_list = []
                for state in expert_states:
                    if isinstance(state, (list, np.ndarray)):
                        expert_states_list.append(np.array(state, dtype=np.float32))
                    else:
                        # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œåˆ›å»ºå›ºå®šé•¿åº¦çš„æ•°ç»„
                        expert_states_list.append(np.zeros(current_features.size(-1), dtype=np.float32))
                expert_states = np.array(expert_states_list, dtype=np.float32)
            
            # ç¡®ä¿å½¢çŠ¶åŒ¹é…
            if expert_states.shape[1] != current_features.size(-1):
                # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…å½“å‰ç‰¹å¾ç»´åº¦
                target_dim = current_features.size(-1)
                if expert_states.shape[1] < target_dim:
                    # å¡«å……zeros
                    padding = np.zeros((expert_states.shape[0], target_dim - expert_states.shape[1]))
                    expert_states = np.concatenate([expert_states, padding], axis=1)
                else:
                    # æˆªæ–­
                    expert_states = expert_states[:, :target_dim]
            
            expert_states = torch.tensor(expert_states, dtype=torch.float32, device=self.device)
            expert_actions = torch.tensor(expert_actions, dtype=torch.long, device=self.device)
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            expert_loss = nn.CrossEntropyLoss()(expert_logits, expert_actions)
            
        except Exception as e:
            print(f"âš ï¸ ä¸“å®¶æ•°æ®è½¬æ¢é”™è¯¯: {e}, ä½¿ç”¨é›¶æŸå¤±")
            # è¿”å›é›¶æŸå¤±ä»¥é¿å…è®­ç»ƒä¸­æ–­
            expert_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return expert_loss
    
    def validate(self, val_sequences):
        """å¢å¼ºçš„æ¨¡å‹éªŒè¯ - åŒ…å«ä¸´åºŠæŒ‡æ ‡"""
        self.agent.eval()
        val_losses = []
        val_rewards = []
        clinical_metrics = {
            'sofa_scores': [],
            'mortality_predictions': [],
            'treatment_effectiveness': [],
            'safety_scores': []
        }
        
        with torch.no_grad():
            for features, seq_lengths in self.prepare_batch(val_sequences):
                # ä¿®å¤ï¼šå¤„ç†æ–°çš„å­—å…¸è¿”å›æ ¼å¼
                outputs = self.agent(features, seq_lengths, use_expert_imitation=False)
                action_logits = outputs['constrained_logits']
                state_values = outputs['state_values']
                
                # âœ¨ å®‰å…¨çº¦æŸå·²åœ¨æ¨¡å‹å†…éƒ¨å¤„ç†
                action_probs = torch.softmax(action_logits, dim=-1)
                
                rewards = self.reward_calculator.calculate_reward(
                    features, action_logits, episode_step=self.global_step
                ).to(self.device)
                
                state_values_squeezed = state_values.squeeze()
                
                critic_loss = self.huber_loss(state_values_squeezed, rewards)
                
                total_loss = critic_loss.item()
                
                if not (np.isnan(total_loss) or np.isinf(total_loss)):
                    val_losses.append(total_loss)
                    val_rewards.append(rewards.mean().item())
                    
                    # æ”¶é›†ä¸´åºŠæŒ‡æ ‡
                    self.collect_clinical_metrics(features, action_probs, state_values, clinical_metrics)
        
        avg_val_loss = np.mean(val_losses) if val_losses else float('inf')
        avg_val_reward = np.mean(val_rewards) if val_rewards else 0
        
        self.history['val_loss'].append(avg_val_loss)
        self.history['val_reward'].append(avg_val_reward)
        
        # è®¡ç®—ä¸´åºŠæŒ‡æ ‡
        clinical_summary = self.summarize_clinical_metrics(clinical_metrics)
        
        # æ–°å¢ï¼šå°†è®¡ç®—å‡ºçš„ä¸´åºŠæŒ‡æ ‡ä¿å­˜åˆ°historyä¸­
        self.history['val_clinical_metrics'].append(clinical_summary)
        
        return avg_val_loss, avg_val_reward, clinical_summary
    
    def collect_clinical_metrics(self, features, action_probs, state_values, metrics):
        """æ”¶é›†ä¸´åºŠæŒ‡æ ‡"""
        # SOFAè¯„åˆ†å˜åŒ–
        sofa_scores = features[:, -1, -1]  # æœ€åä¸€ä¸ªç‰¹å¾æ˜¯SOFAè¯„åˆ†
        metrics['sofa_scores'].extend(sofa_scores.cpu().numpy())
        
        # æ­»äº¡é£é™©é¢„æµ‹ï¼ˆåŸºäºçŠ¶æ€å€¼ï¼‰
        mortality_risk = torch.sigmoid(-state_values.squeeze())  # çŠ¶æ€å€¼è¶Šä½ï¼Œæ­»äº¡é£é™©è¶Šé«˜
        metrics['mortality_predictions'].extend(mortality_risk.cpu().numpy())
        
        # æ²»ç–—æœ‰æ•ˆæ€§ï¼ˆåŸºäºåŠ¨ä½œé€‰æ‹©çš„é›†ä¸­åº¦ï¼‰
        action_entropy = -torch.sum(action_probs * torch.log(action_probs + 1e-8), dim=-1)
        treatment_effectiveness = 1.0 / (1.0 + action_entropy)  # ç†µè¶Šä½ï¼Œæ²»ç–—è¶Šé›†ä¸­
        metrics['treatment_effectiveness'].extend(treatment_effectiveness.cpu().numpy())
        
        # å®‰å…¨è¯„åˆ†
        safety_scores = self.calculate_safety_scores(action_probs)
        metrics['safety_scores'].extend(safety_scores.cpu().numpy())
    
    def calculate_safety_scores(self, action_probs):
        """âœ¨ è®¡ç®—å®‰å…¨è¯„åˆ† - ä½¿ç”¨å†…ç½®å®‰å…¨å±‚è§„åˆ™"""
        safety_scores = torch.ones(action_probs.size(0), device=self.device)
        
        # âœ¨ æ£€æŸ¥å±é™©åŠ¨ä½œç»„åˆ
        dangerous_combinations = [
            [1, 2],   # è¡€ç®¡åŠ å‹è¯ + é™å‹è¯ (ç›¸äº’æŠµæ¶ˆ)
            [3, 11],  # åˆ©å°¿å‰‚ + å¤§é‡ç”µè§£è´¨ (ç”µè§£è´¨ç´Šä¹±)
            [17, 18], # é•‡ç—›å‰‚ + é•‡é™å‰‚ (è¿‡åº¦é•‡é™)
        ]
        
        for dangerous_combo in dangerous_combinations:
            combo_prob = torch.prod(action_probs[:, dangerous_combo], dim=1)
            safety_scores -= combo_prob  # å±é™©ç»„åˆæ¦‚ç‡è¶Šé«˜ï¼Œå®‰å…¨è¯„åˆ†è¶Šä½
        
        return torch.clamp(safety_scores, min=0.0, max=1.0)
    
    def summarize_clinical_metrics(self, metrics):
        """æ±‡æ€»ä¸´åºŠæŒ‡æ ‡"""
        summary = {}
        
        if metrics['sofa_scores']:
            sofa_scores = np.array(metrics['sofa_scores'])
            summary['avg_sofa_score'] = np.mean(sofa_scores)
            summary['sofa_improvement'] = max(0, 15 - np.mean(sofa_scores)) / 15  # æ ‡å‡†åŒ–æ”¹å–„æŒ‡æ ‡
        
        if metrics['mortality_predictions']:
            mortality_risk = np.array(metrics['mortality_predictions'])
            summary['mortality_risk'] = np.mean(mortality_risk)
        
        if metrics['treatment_effectiveness']:
            effectiveness = np.array(metrics['treatment_effectiveness'])
            summary['treatment_effectiveness'] = np.mean(effectiveness)
        
        if metrics['safety_scores']:
            safety = np.array(metrics['safety_scores'])
            summary['safety_score'] = np.mean(safety)
        
        return summary
    
    def train(self, train_sequences, val_sequences, epochs=30):
        """å¢å¼ºçš„è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆè´¥è¡€ç—‡è®­ç»ƒ...")
        print(f"è®­ç»ƒåºåˆ—: {len(train_sequences)}")
        print(f"éªŒè¯åºåˆ—: {len(val_sequences)}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        
        best_val_reward = -float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_reward = self.train_epoch(train_sequences, epoch)
            
            # éªŒè¯
            val_loss, val_reward, val_metrics = self.validate(val_sequences)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_reward)
            
            print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}:")
            print(f"  Train - Loss: {train_loss:.4f}, Reward: {train_reward:.4f}")
            print(f"  Val - Loss: {val_loss:.4f}, Reward: {val_reward:.4f}")
            print(f"  Safety Violations: {self.history['safety_violations'][-1]:.4f}")
            print(f"  Gradient Norm: {self.history['gradient_norm'][-1]:.4f}")
            print(f"  Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # è¾“å‡ºä¸´åºŠæŒ‡æ ‡
            if val_metrics:
                print(f"  Clinical Metrics - Mortality Risk: {val_metrics.get('mortality_risk', 0):.4f}, "
                      f"SOFA Improvement: {val_metrics.get('sofa_improvement', 0):.4f}")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_reward > best_val_reward:
                best_val_reward = val_reward
                patience_counter = 0
                torch.save(self.agent.state_dict(), 'models/enhanced_best_sepsis_agent.pth')
                print("  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = f'models/enhanced_checkpoint_epoch_{epoch+1}.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.agent.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_val_reward': best_val_reward,
                    'history': self.history
                }, checkpoint_path)
                print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # æ–°å¢ï¼šåœ¨è®­ç»ƒç»“æŸåä¿å­˜å®Œæ•´çš„å†å²è®°å½•
        self.save_full_history()
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        return self.history
    
    def save_training_report(self, filename='enhanced_training_report.txt'):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("Enhanced Sepsis DRL Training Report\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Total Epochs: {len(self.history['train_reward'])}\n")
            f.write(f"Best Training Reward: {max(self.history['train_reward']):.4f}\n")
            f.write(f"Best Validation Reward: {max(self.history['val_reward']):.4f}\n")
            
            if self.history['safety_violations']:
                f.write(f"Final Safety Violations: {self.history['safety_violations'][-1]:.4f}\n")
            if self.history['gradient_norm']:
                f.write(f"Final Gradient Norm: {self.history['gradient_norm'][-1]:.4f}\n")
            if self.history['learning_rate']:
                f.write(f"Final Learning Rate: {self.history['learning_rate'][-1]:.2e}\n\n")
            
            f.write("Training Improvements:\n")
            f.write("- Enhanced LSTM architecture with attention mechanism\n")
            f.write("- Clinical reward function based on medical indicators\n")
            f.write("- Medical safety constraints to prevent dangerous combinations\n")
            f.write("- Improved training stability with gradient clipping and regularization\n")
            f.write("- Comprehensive clinical metrics evaluation\n")
        
        print(f"Training report saved to {filename}")

    def save_full_history(self, filename='training_history.json'):
        """æ–°å¢ï¼šä¿å­˜å®Œæ•´çš„è®­ç»ƒå†å²åˆ°JSONæ–‡ä»¶"""
        # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        history_to_save = {}
        for key, value in self.history.items():
            if isinstance(value, list) and len(value) > 0:
                # å¤„ç†åŒ…å«numpyæ•°å€¼æˆ–å¼ é‡çš„åˆ—è¡¨
                if isinstance(value[0], (np.generic, np.ndarray)):
                    history_to_save[key] = [item.item() if hasattr(item, 'item') else item for item in value]
                elif isinstance(value[0], torch.Tensor):
                     history_to_save[key] = [item.item() for item in value]
                elif isinstance(value[0], dict): # å¤„ç†å­—å…¸åˆ—è¡¨
                    history_to_save[key] = [{k: (v.item() if hasattr(v, 'item') else v) for k, v in item.items()} for item in value]
                else:
                    history_to_save[key] = value
            else:
                history_to_save[key] = value
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history_to_save, f, indent=4)
        print(f"  ğŸ’¾ å®Œæ•´çš„è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {filename}")
    
    def plot_training_history(self):
        """Generate individual training curve plots for better visualization and saving"""
        # Create plots directory if it doesn't exist
        Path('plots').mkdir(exist_ok=True)
        
        # Plot 1: Total Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_total_loss'], label='Train Total Loss', color='blue', linewidth=2)
        plt.plot(self.history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/total_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 2: Rewards
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_reward'], label='Train Reward', color='purple', linewidth=2)
        plt.plot(self.history['val_reward'], label='Validation Reward', color='brown', linewidth=2)
        plt.title('Clinical Rewards Over Training', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Reward')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/clinical_rewards.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 3: Actor vs Critic vs Expert Loss
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['train_actor_loss'], label='Actor Loss (RL)', color='green', linewidth=2)
        plt.plot(self.history['train_critic_loss'], label='Critic Loss', color='orange', linewidth=2)
        if self.history['train_expert_loss']:
            plt.plot(self.history['train_expert_loss'], label='Expert Loss (SL)', color='blue', linewidth=2)
        plt.title('Actor vs Critic vs Expert Loss Comparison', fontsize=14, fontweight='bold')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('plots/actor_critic_expert_loss.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Plot 4: Safety Violations
        if self.history['safety_violations']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['safety_violations'], label='Safety Violations', color='red', linewidth=2)
            plt.title('Safety Violations During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Violation Score')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/safety_violations.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 5: Gradient Norm
        if self.history['gradient_norm']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['gradient_norm'], label='Gradient Norm', color='cyan', linewidth=2)
            plt.title('Gradient Norm During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Norm')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/gradient_norm.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 6: Learning Rate Schedule
        if self.history['learning_rate']:
            plt.figure(figsize=(10, 6))
            plt.plot(self.history['learning_rate'], label='Learning Rate', color='magenta', linewidth=2)
            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.yscale('log')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/learning_rate.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Plot 7: RL vs SL Weight Evolution
        if self.history['rl_sl_weights']:
            plt.figure(figsize=(10, 6))
            rl_weights = [w[0] for w in self.history['rl_sl_weights']]
            sl_weights = [w[1] for w in self.history['rl_sl_weights']]
            plt.plot(rl_weights, label='RL Weight', color='red', linewidth=2)
            plt.plot(sl_weights, label='SL Weight', color='blue', linewidth=2)
            plt.title('RL vs SL Weight Evolution During Training', fontsize=14, fontweight='bold')
            plt.xlabel('Epoch')
            plt.ylabel('Weight')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('plots/rl_sl_weights.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        print("ğŸ“Š Individual plots saved to 'plots/' directory:")

def load_sepsis_data(data_dir='preprocessed_sepsis_data'):
    """åŠ è½½é¢„å¤„ç†çš„è´¥è¡€ç—‡æ•°æ®"""
    data_dir = Path(data_dir)
    
    if not data_dir.exists():
        raise FileNotFoundError(f"Preprocessed data directory {data_dir} not found!")
    
    # åŠ è½½ç‰¹å¾ä¿¡æ¯
    feature_info = pd.read_csv(data_dir / 'feature_info.csv')
    feature_names = eval(feature_info['feature_names'].iloc[0])
    
    print(f"âœ… åŠ è½½ç‰¹å¾ä¿¡æ¯: {len(feature_names)} ä¸ªç‰¹å¾")
    
    # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_features = np.load(data_dir / 'train_features.npy')
    val_features = np.load(data_dir / 'val_features.npy')
    train_metadata = pd.read_csv(data_dir / 'train_metadata.csv')
    val_metadata = pd.read_csv(data_dir / 'val_metadata.csv')
    
    print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {train_features.shape[0]} ä¸ªåºåˆ—")
    print(f"âœ… åŠ è½½éªŒè¯æ•°æ®: {val_features.shape[0]} ä¸ªåºåˆ—")
    
    # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼
    def create_sequences(features, metadata):
        sequences = []
        for i in range(len(features)):
            # ä¿®æ­£: è¿™é‡Œçš„åºåˆ—é•¿åº¦åº”è¯¥æ˜¯åŠ¨æ€çš„ï¼Œä½†å½“å‰æ•°æ®é¢„å¤„ç†æ˜¯å›ºå®šçš„
            # å¦‚æœæ‚¨çš„æ•°æ®æ˜¯å˜é•¿çš„ï¼Œéœ€è¦åœ¨è¿™é‡Œè¿›è¡Œç›¸åº”ä¿®æ”¹
            sequence_length = 7  # å‡è®¾å›ºå®šåºåˆ—é•¿åº¦
            sequences.append({
                'features': features[i].reshape(sequence_length, -1),
                'sequence_length': sequence_length
            })
        return sequences
    
    train_sequences = create_sequences(train_features, train_metadata)
    val_sequences = create_sequences(val_features, val_metadata)
    
    # æ•°æ®éªŒè¯
    valid_train_sequences = []
    for seq in train_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_train_sequences.append(seq)
    
    valid_val_sequences = []
    for seq in val_sequences:
        if (seq['features'].shape[0] > 0 and 
            not np.isnan(seq['features']).all() and
            seq['sequence_length'] > 0):
            valid_val_sequences.append(seq)
    
    print(f"âœ… æœ‰æ•ˆè®­ç»ƒåºåˆ—: {len(valid_train_sequences)}")
    print(f"âœ… æœ‰æ•ˆéªŒè¯åºåˆ—: {len(valid_val_sequences)}")
    
    input_dim = valid_train_sequences[0]['features'].shape[1] if valid_train_sequences else len(feature_names)
    
    return valid_train_sequences, valid_val_sequences, input_dim, feature_names

def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆ"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºmodelsç›®å½•
    Path('models').mkdir(exist_ok=True)
    
    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        train_sequences, val_sequences, input_dim, feature_names = load_sepsis_data()
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        agent = EnhancedSepsisLSTMAgent(
            input_dim=input_dim,
            hidden_dim=128,
            num_actions=20,
            sequence_length=7
        )
        
        # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
        reward_calculator = ClinicalRewardCalculator()
        
        # åˆ›å»ºè®­ç»ƒå™¨ - æ›´ç¨³å®šçš„å­¦ä¹ ç‡
        trainer = EnhancedTrainer(
            agent=agent,
            reward_calculator=reward_calculator,
            lr=3e-4,  # æ¢å¤æ­£å¸¸å­¦ä¹ ç‡
            device=device,
            expert_data_path=None  # è‡ªåŠ¨ä½¿ç”¨çœŸå®ä¸“å®¶æ•°æ®
        )
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(train_sequences, val_sequences, epochs=25)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
        trainer.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        print("ğŸ“„ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        trainer.save_training_report()
        
        print("ğŸ‰ Enhanced training completed!")
        print("ğŸ“ Generated files:")
        print("  - models/enhanced_best_sepsis_agent.pth")
        print("  - enhanced_training_report.txt")
        print("  - training_history.json (æ–°å¢)") # æ–°å¢
        print("  - models/enhanced_checkpoint_epoch_*.pth")
        print("  - plots/total_loss.png")
        print("  - plots/clinical_rewards.png")
        print("  - plots/actor_critic_expert_loss.png")
        print("  - plots/rl_sl_weights.png")
        print("  - plots/safety_violations.png")
        print("  - plots/gradient_norm.png")
        print("  - plots/learning_rate.png")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
