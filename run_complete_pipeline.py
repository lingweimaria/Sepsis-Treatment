#!/usr/bin/env python3
"""
å®Œæ•´çš„è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿
æ•´åˆæ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤ï¼Œä»åŸå§‹MIMIC-IIIæ•°æ®åˆ°æœ€ç»ˆè®­ç»ƒæ•°æ®
"""

import os
import sys
import subprocess
from pathlib import Path
import pandas as pd
import numpy as np
import json
from datetime import datetime

class SepsisDataPipeline:
    """è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, base_dir=None):
        if base_dir is None:
            self.base_dir = Path(__file__).parent
        else:
            self.base_dir = Path(base_dir)
        
        self.raw_data_dir = self.base_dir / "raw_data" / "mimic3_original"
        self.processed_data_dir = self.base_dir / "processed_data"
        self.scripts_dir = self.base_dir / "preprocessing_scripts"
        
        self.log_file = self.base_dir / f"pipeline_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def check_prerequisites(self):
        """æ£€æŸ¥å…ˆå†³æ¡ä»¶"""
        self.log("ğŸ” æ£€æŸ¥å…ˆå†³æ¡ä»¶...")
        
        # æ£€æŸ¥ç›®å½•ç»“æ„
        required_dirs = [
            self.raw_data_dir,
            self.processed_data_dir,
            self.scripts_dir
        ]
        
        for dir_path in required_dirs:
            if not dir_path.exists():
                self.log(f"âŒ ç¼ºå°‘ç›®å½•: {dir_path}")
                return False
            else:
                self.log(f"âœ… ç›®å½•å­˜åœ¨: {dir_path}")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶
        required_files = [
            "time series variables(vital signs)_mimic3.csv",
            "static variables(demographics)_mimic3.csv",
            "static variables(weight_height)_mimic3.csv",
            "top 1000 medications_mimic3.csv",
            "time series variables(output)_mimic3.csv"
        ]
        
        for file_name in required_files:
            file_path = self.raw_data_dir / file_name
            if not file_path.exists():
                self.log(f"âŒ ç¼ºå°‘åŸå§‹æ•°æ®æ–‡ä»¶: {file_name}")
                return False
            else:
                file_size = file_path.stat().st_size / (1024 * 1024)  # MB
                self.log(f"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨: {file_name} ({file_size:.1f} MB)")
        
        # æ£€æŸ¥å¤„ç†è„šæœ¬
        required_scripts = [
            "sepsis_data_preprocessing.py",
            "enhanced_sofa_calculator.py"
        ]
        
        for script_name in required_scripts:
            script_path = self.scripts_dir / script_name
            if not script_path.exists():
                self.log(f"âŒ ç¼ºå°‘å¤„ç†è„šæœ¬: {script_name}")
                return False
            else:
                self.log(f"âœ… è„šæœ¬å­˜åœ¨: {script_name}")
        
        self.log("âœ… æ‰€æœ‰å…ˆå†³æ¡ä»¶æ£€æŸ¥é€šè¿‡")
        return True
    
    def run_basic_preprocessing(self):
        """è¿è¡ŒåŸºç¡€æ•°æ®é¢„å¤„ç†"""
        self.log("ğŸ”„ å¼€å§‹åŸºç¡€æ•°æ®é¢„å¤„ç†...")
        
        try:
            script_path = self.scripts_dir / "sepsis_data_preprocessing.py"
            
            # è¿è¡Œé¢„å¤„ç†è„šæœ¬
            result = subprocess.run([
                sys.executable, str(script_path)
            ], capture_output=True, text=True, cwd=str(self.base_dir.parent))
            
            if result.returncode == 0:
                self.log("âœ… åŸºç¡€æ•°æ®é¢„å¤„ç†å®Œæˆ")
                self.log(f"è¾“å‡º: {result.stdout}")
                return True
            else:
                self.log(f"âŒ åŸºç¡€æ•°æ®é¢„å¤„ç†å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.log(f"âŒ åŸºç¡€æ•°æ®é¢„å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def run_sofa_calculation(self):
        """è¿è¡ŒSOFAè¯„åˆ†è®¡ç®—"""
        self.log("ğŸ”„ å¼€å§‹SOFAè¯„åˆ†è®¡ç®—...")
        
        try:
            script_path = self.scripts_dir / "enhanced_sofa_calculator.py"
            
            # è¿è¡ŒSOFAè®¡ç®—è„šæœ¬
            result = subprocess.run([
                sys.executable, str(script_path)
            ], capture_output=True, text=True, cwd=str(self.base_dir.parent))
            
            if result.returncode == 0:
                self.log("âœ… SOFAè¯„åˆ†è®¡ç®—å®Œæˆ")
                self.log(f"è¾“å‡º: {result.stdout}")
                return True
            else:
                self.log(f"âŒ SOFAè¯„åˆ†è®¡ç®—å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.log(f"âŒ SOFAè¯„åˆ†è®¡ç®—å¼‚å¸¸: {e}")
            return False
    
    def generate_expert_data(self):
        """ç”Ÿæˆä¸“å®¶å†³ç­–æ•°æ®ç”¨äºç›‘ç£å­¦ä¹ """
        self.log("ğŸ”„ ç”Ÿæˆä¸“å®¶å†³ç­–æ•°æ®...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒæ•°æ®
            final_data_dir = self.processed_data_dir / "final"
            train_features_path = final_data_dir / "train_features.npy"
            
            if not train_features_path.exists():
                self.log("âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆä¸“å®¶æ•°æ®")
                return False
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            train_features = np.load(train_features_path)
            self.log(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {train_features.shape}")
            
            # ç”Ÿæˆä¸“å®¶å†³ç­–æ•°æ®
            expert_data = []
            for i, sequence in enumerate(train_features):
                # é‡å¡‘åºåˆ—æ•°æ®
                if len(sequence.shape) == 1:
                    # å‡è®¾åºåˆ—é•¿åº¦ä¸º7
                    sequence = sequence.reshape(7, -1)
                
                # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€
                final_state = sequence[-1]
                
                # åŸºäºåŒ»å­¦è§„åˆ™ç”Ÿæˆä¸“å®¶å†³ç­–
                expert_action = self.generate_expert_action(final_state)
                
                # ç»„åˆçŠ¶æ€å’ŒåŠ¨ä½œ
                expert_sample = {
                    'state_features': final_state.tolist(),
                    'expert_action': expert_action,
                    'sequence_id': i
                }
                expert_data.append(expert_sample)
            
            # ä¿å­˜ä¸“å®¶æ•°æ®
            expert_data_dir = self.processed_data_dir / "expert_data"
            expert_data_dir.mkdir(exist_ok=True)
            
            expert_df = pd.DataFrame(expert_data)
            expert_data_path = expert_data_dir / "expert_decisions.csv"
            expert_df.to_csv(expert_data_path, index=False)
            
            self.log(f"âœ… ä¸“å®¶æ•°æ®å·²ä¿å­˜: {expert_data_path}")
            self.log(f"   ä¸“å®¶æ ·æœ¬æ•°é‡: {len(expert_data)}")
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ç”Ÿæˆä¸“å®¶æ•°æ®å¼‚å¸¸: {e}")
            return False
    
    def generate_expert_action(self, state_features):
        """åŸºäºåŒ»å­¦è§„åˆ™ç”Ÿæˆä¸“å®¶åŠ¨ä½œ"""
        # å‡è®¾çŠ¶æ€ç‰¹å¾çš„é¡ºåºï¼š[temp, hr, resp_rate, bp_sys, bp_dias, spo2, ...]
        if len(state_features) < 6:
            return 0  # é»˜è®¤è§‚å¯ŸåŠ¨ä½œ
        
        temp = state_features[0]      # ä½“æ¸©
        hr = state_features[1]        # å¿ƒç‡
        bp_sys = state_features[3]    # æ”¶ç¼©å‹
        bp_dias = state_features[4]   # èˆ’å¼ å‹
        spo2 = state_features[5]      # æ°§é¥±å’Œåº¦
        
        # ä¸“å®¶å†³ç­–è§„åˆ™
        if temp > 38.5:
            return 1  # é™æ¸©æ²»ç–—
        elif hr > 100:
            return 2  # å¿ƒç‡æ§åˆ¶
        elif bp_sys < 90:
            return 3  # å‡å‹æ²»ç–—
        elif bp_sys > 140:
            return 4  # é™å‹æ²»ç–—
        elif spo2 < 92:
            return 5  # æ°§ç–—
        else:
            return 0  # è§‚å¯Ÿ/ç»´æŒæ²»ç–—

    def validate_output_data(self):
        """éªŒè¯è¾“å‡ºæ•°æ®"""
        self.log("ğŸ” éªŒè¯è¾“å‡ºæ•°æ®...")
        
        validation_results = {}
        
        # æ£€æŸ¥æœ€ç»ˆè®­ç»ƒæ•°æ®
        final_data_dir = self.processed_data_dir / "final"
        if final_data_dir.exists():
            required_files = [
                "train_features.npy",
                "val_features.npy", 
                "train_metadata.csv",
                "val_metadata.csv",
                "feature_info.csv"
            ]
            
            for file_name in required_files:
                file_path = final_data_dir / file_name
                if file_path.exists():
                    if file_name.endswith('.npy'):
                        try:
                            data = np.load(file_path)
                            validation_results[file_name] = {
                                'exists': True,
                                'shape': data.shape,
                                'dtype': str(data.dtype)
                            }
                            self.log(f"âœ… {file_name}: shape={data.shape}, dtype={data.dtype}")
                        except Exception as e:
                            validation_results[file_name] = {'exists': True, 'error': str(e)}
                            self.log(f"âŒ {file_name}: è¯»å–é”™è¯¯ - {e}")
                    elif file_name.endswith('.csv'):
                        try:
                            data = pd.read_csv(file_path)
                            validation_results[file_name] = {
                                'exists': True,
                                'shape': data.shape,
                                'columns': list(data.columns)
                            }
                            self.log(f"âœ… {file_name}: shape={data.shape}")
                        except Exception as e:
                            validation_results[file_name] = {'exists': True, 'error': str(e)}
                            self.log(f"âŒ {file_name}: è¯»å–é”™è¯¯ - {e}")
                else:
                    validation_results[file_name] = {'exists': False}
                    self.log(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_name}")
        
        # æ£€æŸ¥SOFAæ•°æ®
        sofa_data_dir = self.processed_data_dir / "complete_sofa_data"
        if sofa_data_dir.exists():
            sofa_dataset_path = sofa_data_dir / "enhanced_sofa_dataset.csv"
            if sofa_dataset_path.exists():
                try:
                    sofa_data = pd.read_csv(sofa_dataset_path)
                    validation_results['enhanced_sofa_dataset.csv'] = {
                        'exists': True,
                        'shape': sofa_data.shape,
                        'columns': list(sofa_data.columns)
                    }
                    self.log(f"âœ… SOFAæ•°æ®é›†: shape={sofa_data.shape}")
                    self.log(f"   ç‰¹å¾: {list(sofa_data.columns)}")
                except Exception as e:
                    validation_results['enhanced_sofa_dataset.csv'] = {'exists': True, 'error': str(e)}
                    self.log(f"âŒ SOFAæ•°æ®é›†è¯»å–é”™è¯¯: {e}")
            else:
                validation_results['enhanced_sofa_dataset.csv'] = {'exists': False}
                self.log("âŒ ç¼ºå°‘SOFAæ•°æ®é›†æ–‡ä»¶")
        
        # æ£€æŸ¥ä¸“å®¶æ•°æ®
        expert_data_dir = self.processed_data_dir / "expert_data"
        if expert_data_dir.exists():
            expert_decisions_path = expert_data_dir / "expert_decisions.csv"
            if expert_decisions_path.exists():
                try:
                    expert_data = pd.read_csv(expert_decisions_path)
                    validation_results['expert_decisions.csv'] = {
                        'exists': True,
                        'shape': expert_data.shape,
                        'columns': list(expert_data.columns),
                        'action_distribution': expert_data['expert_action'].value_counts().to_dict()
                    }
                    self.log(f"âœ… ä¸“å®¶å†³ç­–æ•°æ®: shape={expert_data.shape}")
                    self.log(f"   åŠ¨ä½œåˆ†å¸ƒ: {expert_data['expert_action'].value_counts().to_dict()}")
                except Exception as e:
                    validation_results['expert_decisions.csv'] = {'exists': True, 'error': str(e)}
                    self.log(f"âŒ ä¸“å®¶æ•°æ®è¯»å–é”™è¯¯: {e}")
            else:
                validation_results['expert_decisions.csv'] = {'exists': False}
                self.log("âŒ ç¼ºå°‘ä¸“å®¶å†³ç­–æ•°æ®æ–‡ä»¶")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        validation_report_path = self.base_dir / "data_validation_report.json"
        with open(validation_report_path, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False)
        
        self.log(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {validation_report_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®æ–‡ä»¶ç¼ºå¤±
        critical_files = ["train_features.npy", "enhanced_sofa_dataset.csv"]
        missing_critical = [f for f in critical_files if not validation_results.get(f, {}).get('exists', False)]
        
        # ä¸“å®¶æ•°æ®æ˜¯å¯é€‰çš„ï¼Œä½†ä¼šå‘å‡ºè­¦å‘Š
        if not validation_results.get('expert_decisions.csv', {}).get('exists', False):
            self.log("âš ï¸ ç¼ºå°‘ä¸“å®¶å†³ç­–æ•°æ®ï¼Œæ··åˆRL+SLè®­ç»ƒå°†ä½¿ç”¨æ¨¡æ‹Ÿä¸“å®¶æ•°æ®")
        
        if missing_critical:
            self.log(f"âŒ ç¼ºå°‘å…³é”®æ–‡ä»¶: {missing_critical}")
            return False
        else:
            self.log("âœ… æ‰€æœ‰å…³é”®æ–‡ä»¶éªŒè¯é€šè¿‡")
            return True
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        self.log("ğŸ“Š ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        report_content = []
        report_content.append("# è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿æ€»ç»“æŠ¥å‘Š")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        
        # æ•°æ®æ–‡ä»¶ç»Ÿè®¡
        report_content.append("## æ•°æ®æ–‡ä»¶ç»Ÿè®¡")
        
        # åŸå§‹æ•°æ®
        report_content.append("### åŸå§‹æ•°æ®")
        if self.raw_data_dir.exists():
            for file_path in self.raw_data_dir.glob("*.csv"):
                size_mb = file_path.stat().st_size / (1024 * 1024)
                report_content.append(f"- {file_path.name}: {size_mb:.1f} MB")
        
        # å¤„ç†åæ•°æ®
        report_content.append("### å¤„ç†åæ•°æ®")
        
        # æœ€ç»ˆè®­ç»ƒæ•°æ®
        final_data_dir = self.processed_data_dir / "final"
        if final_data_dir.exists():
            report_content.append("#### æœ€ç»ˆè®­ç»ƒæ•°æ®")
            for file_path in final_data_dir.glob("*"):
                if file_path.is_file():
                    if file_path.suffix == '.npy':
                        try:
                            data = np.load(file_path)
                            report_content.append(f"- {file_path.name}: shape={data.shape}")
                        except:
                            report_content.append(f"- {file_path.name}: è¯»å–å¤±è´¥")
                    elif file_path.suffix == '.csv':
                        try:
                            data = pd.read_csv(file_path)
                            report_content.append(f"- {file_path.name}: {data.shape[0]} è¡Œ, {data.shape[1]} åˆ—")
                        except:
                            report_content.append(f"- {file_path.name}: è¯»å–å¤±è´¥")
        
        # SOFAæ•°æ®
        sofa_data_dir = self.processed_data_dir / "complete_sofa_data"
        if sofa_data_dir.exists():
            report_content.append("#### SOFAè¯„åˆ†æ•°æ®")
            sofa_dataset_path = sofa_data_dir / "enhanced_sofa_dataset.csv"
            if sofa_dataset_path.exists():
                try:
                    sofa_data = pd.read_csv(sofa_dataset_path)
                    report_content.append(f"- enhanced_sofa_dataset.csv: {sofa_data.shape[0]} è¡Œ, {sofa_data.shape[1]} åˆ—")
                    
                    # SOFAè¯„åˆ†ç»Ÿè®¡
                    if 'sofa_score' in sofa_data.columns:
                        sofa_stats = sofa_data['sofa_score'].describe()
                        report_content.append(f"  - SOFAè¯„åˆ†èŒƒå›´: {sofa_stats['min']:.1f} - {sofa_stats['max']:.1f}")
                        report_content.append(f"  - SOFAè¯„åˆ†å‡å€¼: {sofa_stats['mean']:.1f}")
                except:
                    report_content.append("- enhanced_sofa_dataset.csv: è¯»å–å¤±è´¥")
        
        # ä¸“å®¶å†³ç­–æ•°æ®
        expert_data_dir = self.processed_data_dir / "expert_data"
        if expert_data_dir.exists():
            report_content.append("#### ä¸“å®¶å†³ç­–æ•°æ®")
            expert_decisions_path = expert_data_dir / "expert_decisions.csv"
            if expert_decisions_path.exists():
                try:
                    expert_data = pd.read_csv(expert_decisions_path)
                    report_content.append(f"- expert_decisions.csv: {expert_data.shape[0]} è¡Œ, {expert_data.shape[1]} åˆ—")
                    
                    # ä¸“å®¶åŠ¨ä½œåˆ†å¸ƒç»Ÿè®¡
                    if 'expert_action' in expert_data.columns:
                        action_counts = expert_data['expert_action'].value_counts()
                        report_content.append("  - åŠ¨ä½œåˆ†å¸ƒ:")
                        for action, count in action_counts.items():
                            percentage = (count / len(expert_data)) * 100
                            report_content.append(f"    - åŠ¨ä½œ{action}: {count} ({percentage:.1f}%)")
                except:
                    report_content.append("- expert_decisions.csv: è¯»å–å¤±è´¥")
        
        # å¤„ç†è„šæœ¬
        report_content.append("## ä½¿ç”¨çš„å¤„ç†è„šæœ¬")
        if self.scripts_dir.exists():
            for script_path in self.scripts_dir.glob("*.py"):
                report_content.append(f"- {script_path.name}")
        
        # æ•°æ®æµç¨‹
        report_content.append("## æ•°æ®å¤„ç†æµç¨‹")
        report_content.append("1. åŸå§‹MIMIC-IIIæ•°æ®å¯¼å…¥")
        report_content.append("2. åŸºç¡€æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—")
        report_content.append("3. SOFAè¯„åˆ†è®¡ç®—")
        report_content.append("4. ä¸“å®¶å†³ç­–æ•°æ®ç”Ÿæˆ (ç”¨äºæ··åˆRL+SLè®­ç»ƒ)")
        report_content.append("5. ç‰¹å¾å·¥ç¨‹å’Œåºåˆ—æ„å»º")
        report_content.append("6. è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²")
        report_content.append("7. æ•°æ®éªŒè¯å’Œè´¨é‡æ£€æŸ¥")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.base_dir / "pipeline_summary_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        self.log(f"ğŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        self.log("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿")
        
        # 1. æ£€æŸ¥å…ˆå†³æ¡ä»¶
        if not self.check_prerequisites():
            self.log("âŒ å…ˆå†³æ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 2. è¿è¡ŒåŸºç¡€é¢„å¤„ç†
        if not self.run_basic_preprocessing():
            self.log("âŒ åŸºç¡€é¢„å¤„ç†å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 3. è¿è¡ŒSOFAè®¡ç®—
        if not self.run_sofa_calculation():
            self.log("âŒ SOFAè®¡ç®—å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 4. ç”Ÿæˆä¸“å®¶å†³ç­–æ•°æ®
        if not self.generate_expert_data():
            self.log("âš ï¸ ä¸“å®¶æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œä½†æµæ°´çº¿ç»§ç»­")
        
        # 5. éªŒè¯è¾“å‡ºæ•°æ®
        if not self.validate_output_data():
            self.log("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œä½†æµæ°´çº¿ç»§ç»­")
        
        # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        self.log("ğŸ‰ å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        self.log(f"ğŸ“„ è¯¦ç»†æ—¥å¿—: {self.log_file}")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = Path(__file__).parent
    
    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = SepsisDataPipeline(current_dir)
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("\nâœ… æ•°æ®å¤„ç†æµæ°´çº¿æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {pipeline.processed_data_dir}")
        print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {pipeline.log_file}")
    else:
        print("\nâŒ æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå¤±è´¥!")
        print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: {pipeline.log_file}")
    
    return success

if __name__ == "__main__":
    main()