#!/usr/bin/env python3
"""
å®Œæ•´çš„è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿
æ•´åˆæ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤ï¼Œä»åŸå§‹MIMIC-IIIæ•°æ®åˆ°æœ€ç»ˆè®­ç»ƒæ•°æ®
"""

import os
import sys
import subprocess
from pathlib import Path
import pandas as pd
import numpy as np
import json
from datetime import datetime

class SepsisDataPipeline:
    """è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, base_dir=None):
        if base_dir is None:
            self.base_dir = Path(__file__).parent
        else:
            self.base_dir = Path(base_dir)
        
        self.raw_data_dir = self.base_dir / "raw_data" / "mimic3_original"
        self.processed_data_dir = self.base_dir / "processed_data"
        self.scripts_dir = self.base_dir / "preprocessing_scripts"
        
        self.log_file = self.base_dir / f"pipeline_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def check_prerequisites(self):
        """æ£€æŸ¥å…ˆå†³æ¡ä»¶"""
        self.log("ğŸ” æ£€æŸ¥å…ˆå†³æ¡ä»¶...")
        
        # æ£€æŸ¥ç›®å½•ç»“æ„
        required_dirs = [
            self.raw_data_dir,
            self.processed_data_dir,
            self.scripts_dir
        ]
        
        for dir_path in required_dirs:
            if not dir_path.exists():
                self.log(f"âŒ ç¼ºå°‘ç›®å½•: {dir_path}")
                return False
            else:
                self.log(f"âœ… ç›®å½•å­˜åœ¨: {dir_path}")
        
        # æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶
        required_files = [
            "time series variables(vital signs)_mimic3.csv",
            "static variables(demographics)_mimic3.csv",
            "static variables(weight_height)_mimic3.csv",
            "top 1000 medications_mimic3.csv",
            "time series variables(output)_mimic3.csv"
        ]
        
        for file_name in required_files:
            file_path = self.raw_data_dir / file_name
            if not file_path.exists():
                self.log(f"âŒ ç¼ºå°‘åŸå§‹æ•°æ®æ–‡ä»¶: {file_name}")
                return False
            else:
                file_size = file_path.stat().st_size / (1024 * 1024)  # MB
                self.log(f"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨: {file_name} ({file_size:.1f} MB)")
        
        # æ£€æŸ¥å¤„ç†è„šæœ¬
        required_scripts = [
            "sepsis_data_preprocessing.py",
            "enhanced_sofa_calculator.py"
        ]
        
        for script_name in required_scripts:
            script_path = self.scripts_dir / script_name
            if not script_path.exists():
                self.log(f"âŒ ç¼ºå°‘å¤„ç†è„šæœ¬: {script_name}")
                return False
            else:
                self.log(f"âœ… è„šæœ¬å­˜åœ¨: {script_name}")
        
        self.log("âœ… æ‰€æœ‰å…ˆå†³æ¡ä»¶æ£€æŸ¥é€šè¿‡")
        return True
    
    def run_basic_preprocessing(self):
        """è¿è¡ŒåŸºç¡€æ•°æ®é¢„å¤„ç†"""
        self.log("ğŸ”„ å¼€å§‹åŸºç¡€æ•°æ®é¢„å¤„ç†...")
        
        try:
            script_path = self.scripts_dir / "sepsis_data_preprocessing.py"
            
            # è¿è¡Œé¢„å¤„ç†è„šæœ¬
            result = subprocess.run([
                sys.executable, str(script_path)
            ], capture_output=True, text=True, cwd=str(self.base_dir.parent))
            
            if result.returncode == 0:
                self.log("âœ… åŸºç¡€æ•°æ®é¢„å¤„ç†å®Œæˆ")
                self.log(f"è¾“å‡º: {result.stdout}")
                return True
            else:
                self.log(f"âŒ åŸºç¡€æ•°æ®é¢„å¤„ç†å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.log(f"âŒ åŸºç¡€æ•°æ®é¢„å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def run_sofa_calculation(self):
        """è¿è¡ŒSOFAè¯„åˆ†è®¡ç®—"""
        self.log("ğŸ”„ å¼€å§‹SOFAè¯„åˆ†è®¡ç®—...")
        
        try:
            script_path = self.scripts_dir / "enhanced_sofa_calculator.py"
            
            # è¿è¡ŒSOFAè®¡ç®—è„šæœ¬
            result = subprocess.run([
                sys.executable, str(script_path)
            ], capture_output=True, text=True, cwd=str(self.base_dir.parent))
            
            if result.returncode == 0:
                self.log("âœ… SOFAè¯„åˆ†è®¡ç®—å®Œæˆ")
                self.log(f"è¾“å‡º: {result.stdout}")
                return True
            else:
                self.log(f"âŒ SOFAè¯„åˆ†è®¡ç®—å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            self.log(f"âŒ SOFAè¯„åˆ†è®¡ç®—å¼‚å¸¸: {e}")
            return False
    
    def validate_output_data(self):
        """éªŒè¯è¾“å‡ºæ•°æ®"""
        self.log("ğŸ” éªŒè¯è¾“å‡ºæ•°æ®...")
        
        validation_results = {}
        
        # æ£€æŸ¥æœ€ç»ˆè®­ç»ƒæ•°æ®
        final_data_dir = self.processed_data_dir / "final"
        if final_data_dir.exists():
            required_files = [
                "train_features.npy",
                "val_features.npy", 
                "train_metadata.csv",
                "val_metadata.csv",
                "feature_info.csv"
            ]
            
            for file_name in required_files:
                file_path = final_data_dir / file_name
                if file_path.exists():
                    if file_name.endswith('.npy'):
                        try:
                            data = np.load(file_path)
                            validation_results[file_name] = {
                                'exists': True,
                                'shape': data.shape,
                                'dtype': str(data.dtype)
                            }
                            self.log(f"âœ… {file_name}: shape={data.shape}, dtype={data.dtype}")
                        except Exception as e:
                            validation_results[file_name] = {'exists': True, 'error': str(e)}
                            self.log(f"âŒ {file_name}: è¯»å–é”™è¯¯ - {e}")
                    elif file_name.endswith('.csv'):
                        try:
                            data = pd.read_csv(file_path)
                            validation_results[file_name] = {
                                'exists': True,
                                'shape': data.shape,
                                'columns': list(data.columns)
                            }
                            self.log(f"âœ… {file_name}: shape={data.shape}")
                        except Exception as e:
                            validation_results[file_name] = {'exists': True, 'error': str(e)}
                            self.log(f"âŒ {file_name}: è¯»å–é”™è¯¯ - {e}")
                else:
                    validation_results[file_name] = {'exists': False}
                    self.log(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_name}")
        
        # æ£€æŸ¥SOFAæ•°æ®
        sofa_data_dir = self.processed_data_dir / "complete_sofa_data"
        if sofa_data_dir.exists():
            sofa_dataset_path = sofa_data_dir / "enhanced_sofa_dataset.csv"
            if sofa_dataset_path.exists():
                try:
                    sofa_data = pd.read_csv(sofa_dataset_path)
                    validation_results['enhanced_sofa_dataset.csv'] = {
                        'exists': True,
                        'shape': sofa_data.shape,
                        'columns': list(sofa_data.columns)
                    }
                    self.log(f"âœ… SOFAæ•°æ®é›†: shape={sofa_data.shape}")
                    self.log(f"   ç‰¹å¾: {list(sofa_data.columns)}")
                except Exception as e:
                    validation_results['enhanced_sofa_dataset.csv'] = {'exists': True, 'error': str(e)}
                    self.log(f"âŒ SOFAæ•°æ®é›†è¯»å–é”™è¯¯: {e}")
            else:
                validation_results['enhanced_sofa_dataset.csv'] = {'exists': False}
                self.log("âŒ ç¼ºå°‘SOFAæ•°æ®é›†æ–‡ä»¶")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        validation_report_path = self.base_dir / "data_validation_report.json"
        with open(validation_report_path, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, indent=2, ensure_ascii=False)
        
        self.log(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {validation_report_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®æ–‡ä»¶ç¼ºå¤±
        critical_files = ["train_features.npy", "enhanced_sofa_dataset.csv"]
        missing_critical = [f for f in critical_files if not validation_results.get(f, {}).get('exists', False)]
        
        if missing_critical:
            self.log(f"âŒ ç¼ºå°‘å…³é”®æ–‡ä»¶: {missing_critical}")
            return False
        else:
            self.log("âœ… æ‰€æœ‰å…³é”®æ–‡ä»¶éªŒè¯é€šè¿‡")
            return True
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        self.log("ğŸ“Š ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        report_content = []
        report_content.append("# è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿æ€»ç»“æŠ¥å‘Š")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("")
        
        # æ•°æ®æ–‡ä»¶ç»Ÿè®¡
        report_content.append("## æ•°æ®æ–‡ä»¶ç»Ÿè®¡")
        
        # åŸå§‹æ•°æ®
        report_content.append("### åŸå§‹æ•°æ®")
        if self.raw_data_dir.exists():
            for file_path in self.raw_data_dir.glob("*.csv"):
                size_mb = file_path.stat().st_size / (1024 * 1024)
                report_content.append(f"- {file_path.name}: {size_mb:.1f} MB")
        
        # å¤„ç†åæ•°æ®
        report_content.append("### å¤„ç†åæ•°æ®")
        
        # æœ€ç»ˆè®­ç»ƒæ•°æ®
        final_data_dir = self.processed_data_dir / "final"
        if final_data_dir.exists():
            report_content.append("#### æœ€ç»ˆè®­ç»ƒæ•°æ®")
            for file_path in final_data_dir.glob("*"):
                if file_path.is_file():
                    if file_path.suffix == '.npy':
                        try:
                            data = np.load(file_path)
                            report_content.append(f"- {file_path.name}: shape={data.shape}")
                        except:
                            report_content.append(f"- {file_path.name}: è¯»å–å¤±è´¥")
                    elif file_path.suffix == '.csv':
                        try:
                            data = pd.read_csv(file_path)
                            report_content.append(f"- {file_path.name}: {data.shape[0]} è¡Œ, {data.shape[1]} åˆ—")
                        except:
                            report_content.append(f"- {file_path.name}: è¯»å–å¤±è´¥")
        
        # SOFAæ•°æ®
        sofa_data_dir = self.processed_data_dir / "complete_sofa_data"
        if sofa_data_dir.exists():
            report_content.append("#### SOFAè¯„åˆ†æ•°æ®")
            sofa_dataset_path = sofa_data_dir / "enhanced_sofa_dataset.csv"
            if sofa_dataset_path.exists():
                try:
                    sofa_data = pd.read_csv(sofa_dataset_path)
                    report_content.append(f"- enhanced_sofa_dataset.csv: {sofa_data.shape[0]} è¡Œ, {sofa_data.shape[1]} åˆ—")
                    
                    # SOFAè¯„åˆ†ç»Ÿè®¡
                    if 'sofa_score' in sofa_data.columns:
                        sofa_stats = sofa_data['sofa_score'].describe()
                        report_content.append(f"  - SOFAè¯„åˆ†èŒƒå›´: {sofa_stats['min']:.1f} - {sofa_stats['max']:.1f}")
                        report_content.append(f"  - SOFAè¯„åˆ†å‡å€¼: {sofa_stats['mean']:.1f}")
                except:
                    report_content.append("- enhanced_sofa_dataset.csv: è¯»å–å¤±è´¥")
        
        # å¤„ç†è„šæœ¬
        report_content.append("## ä½¿ç”¨çš„å¤„ç†è„šæœ¬")
        if self.scripts_dir.exists():
            for script_path in self.scripts_dir.glob("*.py"):
                report_content.append(f"- {script_path.name}")
        
        # æ•°æ®æµç¨‹
        report_content.append("## æ•°æ®å¤„ç†æµç¨‹")
        report_content.append("1. åŸå§‹MIMIC-IIIæ•°æ®å¯¼å…¥")
        report_content.append("2. åŸºç¡€æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—")
        report_content.append("3. SOFAè¯„åˆ†è®¡ç®—")
        report_content.append("4. ç‰¹å¾å·¥ç¨‹å’Œåºåˆ—æ„å»º")
        report_content.append("5. è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²")
        report_content.append("6. æ•°æ®éªŒè¯å’Œè´¨é‡æ£€æŸ¥")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.base_dir / "pipeline_summary_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        self.log(f"ğŸ“„ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        self.log("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´è´¥è¡€ç—‡æ•°æ®å¤„ç†æµæ°´çº¿")
        
        # 1. æ£€æŸ¥å…ˆå†³æ¡ä»¶
        if not self.check_prerequisites():
            self.log("âŒ å…ˆå†³æ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 2. è¿è¡ŒåŸºç¡€é¢„å¤„ç†
        if not self.run_basic_preprocessing():
            self.log("âŒ åŸºç¡€é¢„å¤„ç†å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 3. è¿è¡ŒSOFAè®¡ç®—
        if not self.run_sofa_calculation():
            self.log("âŒ SOFAè®¡ç®—å¤±è´¥ï¼Œæµæ°´çº¿åœæ­¢")
            return False
        
        # 4. éªŒè¯è¾“å‡ºæ•°æ®
        if not self.validate_output_data():
            self.log("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œä½†æµæ°´çº¿ç»§ç»­")
        
        # 5. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        self.log("ğŸ‰ å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        self.log(f"ğŸ“„ è¯¦ç»†æ—¥å¿—: {self.log_file}")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = Path(__file__).parent
    
    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = SepsisDataPipeline(current_dir)
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    success = pipeline.run_complete_pipeline()
    
    if success:
        print("\nâœ… æ•°æ®å¤„ç†æµæ°´çº¿æˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ æ•°æ®ç›®å½•: {pipeline.processed_data_dir}")
        print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶: {pipeline.log_file}")
    else:
        print("\nâŒ æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå¤±è´¥!")
        print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: {pipeline.log_file}")
    
    return success

if __name__ == "__main__":
    main()