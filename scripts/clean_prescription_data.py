#!/usr/bin/env python3
"""
æ¸…æ´—å¤„æ–¹æ•°æ®ä¸­çš„æ—¶é—´å¼‚å¸¸
"""

import pandas as pd
import numpy as np
import json

def clean_prescription_data():
    """æ¸…æ´—å¤„æ–¹æ•°æ®çš„æ—¶é—´å¼‚å¸¸"""
    
    print("=" * 60)
    print("å¤„æ–¹æ•°æ®æ—¶é—´æ¸…æ´—")
    print("=" * 60)
    
    # è¯»å–åŸå§‹æ•°æ®
    print("\nğŸ“¥ è¯»å–åŸå§‹æ•°æ®...")
    pres_data = pd.read_csv('data/raw/top 1000 medications_mimic3.csv', 
                           usecols=['HADM_ID', 'STARTDATE', 'ENDDATE', 'DRUG'],
                           dtype={'HADM_ID': 'int32'},
                           parse_dates=['STARTDATE', 'ENDDATE'])
    
    print(f"åŸå§‹è®°å½•æ•°: {len(pres_data):,}")
    
    # è®°å½•æ¸…æ´—ç»Ÿè®¡
    cleaning_stats = {
        'original_count': len(pres_data),
        'removed_counts': {}
    }
    
    # 1. ç§»é™¤å¼‚å¸¸å¹´ä»½æ•°æ®
    print("\nğŸ§¹ æ­¥éª¤1: ç§»é™¤å¼‚å¸¸å¹´ä»½æ•°æ®")
    
    # ç§»é™¤2100å¹´å‰çš„æ•°æ®
    early_records = pres_data[pres_data['STARTDATE'].dt.year < 2100]
    cleaning_stats['removed_counts']['early_records'] = len(early_records)
    pres_data = pres_data[pres_data['STARTDATE'].dt.year >= 2100]
    print(f"ç§»é™¤2100å¹´å‰çš„è®°å½•: {len(early_records):,}")
    
    # ç§»é™¤2200å¹´åçš„æ•°æ®
    late_records = pres_data[pres_data['STARTDATE'].dt.year > 2200]
    cleaning_stats['removed_counts']['late_records'] = len(late_records)
    pres_data = pres_data[pres_data['STARTDATE'].dt.year <= 2200]
    print(f"ç§»é™¤2200å¹´åçš„è®°å½•: {len(late_records):,}")
    
    # 2. ç§»é™¤è´ŸæŒç»­æ—¶é—´è®°å½•
    print("\nğŸ§¹ æ­¥éª¤2: ç§»é™¤è´ŸæŒç»­æ—¶é—´è®°å½•")
    negative_duration = pres_data[pres_data['ENDDATE'] < pres_data['STARTDATE']]
    cleaning_stats['removed_counts']['negative_duration'] = len(negative_duration)
    pres_data = pres_data[pres_data['ENDDATE'] >= pres_data['STARTDATE']]
    print(f"ç§»é™¤è´ŸæŒç»­æ—¶é—´è®°å½•: {len(negative_duration):,}")
    
    # 3. ç§»é™¤æé•¿æŒç»­æ—¶é—´è®°å½• (>1å¹´)
    print("\nğŸ§¹ æ­¥éª¤3: ç§»é™¤æé•¿æŒç»­æ—¶é—´è®°å½•")
    pres_data['duration_days'] = (pres_data['ENDDATE'] - pres_data['STARTDATE']).dt.days
    extreme_duration = pres_data[pres_data['duration_days'] > 365]
    cleaning_stats['removed_counts']['extreme_duration'] = len(extreme_duration)
    pres_data = pres_data[pres_data['duration_days'] <= 365]
    print(f"ç§»é™¤è¶…è¿‡1å¹´çš„è®°å½•: {len(extreme_duration):,}")
    
    # 4. ç§»é™¤æçŸ­æŒç»­æ—¶é—´è®°å½• (0å°æ—¶)
    print("\nğŸ§¹ æ­¥éª¤4: ç§»é™¤æçŸ­æŒç»­æ—¶é—´è®°å½•")
    zero_duration = pres_data[pres_data['duration_days'] == 0]
    cleaning_stats['removed_counts']['zero_duration'] = len(zero_duration)
    pres_data = pres_data[pres_data['duration_days'] > 0]
    print(f"ç§»é™¤0æŒç»­æ—¶é—´è®°å½•: {len(zero_duration):,}")
    
    # ç§»é™¤ä¸´æ—¶è®¡ç®—åˆ—
    pres_data = pres_data.drop(columns=['duration_days'])
    
    # 5. æ¸…æ´—ç»“æœç»Ÿè®¡
    print("\nğŸ“Š æ¸…æ´—ç»“æœç»Ÿè®¡:")
    cleaning_stats['cleaned_count'] = len(pres_data)
    cleaning_stats['total_removed'] = sum(cleaning_stats['removed_counts'].values())
    cleaning_stats['retention_rate'] = len(pres_data) / cleaning_stats['original_count']
    
    print(f"åŸå§‹è®°å½•: {cleaning_stats['original_count']:,}")
    print(f"æ¸…æ´—åè®°å½•: {cleaning_stats['cleaned_count']:,}")
    print(f"ç§»é™¤è®°å½•: {cleaning_stats['total_removed']:,}")
    print(f"ä¿ç•™ç‡: {cleaning_stats['retention_rate']*100:.1f}%")
    
    # è¯¦ç»†ç§»é™¤ç»Ÿè®¡
    print("\nç§»é™¤è®°å½•è¯¦æƒ…:")
    for category, count in cleaning_stats['removed_counts'].items():
        print(f"  {category}: {count:,}")
    
    # 6. éªŒè¯æ¸…æ´—åæ•°æ®è´¨é‡
    print("\nâœ… éªŒè¯æ¸…æ´—åæ•°æ®è´¨é‡:")
    pres_data['duration_hours'] = (pres_data['ENDDATE'] - pres_data['STARTDATE']).dt.total_seconds() / 3600
    
    print(f"æ—¶é—´èŒƒå›´: {pres_data['STARTDATE'].min()} åˆ° {pres_data['ENDDATE'].max()}")
    print(f"æŒç»­æ—¶é—´ç»Ÿè®¡:")
    print(f"  å¹³å‡: {pres_data['duration_hours'].mean():.1f}å°æ—¶")
    print(f"  ä¸­ä½æ•°: {pres_data['duration_hours'].median():.1f}å°æ—¶")
    print(f"  æœ€çŸ­: {pres_data['duration_hours'].min():.1f}å°æ—¶")
    print(f"  æœ€é•¿: {pres_data['duration_hours'].max():.1f}å°æ—¶")
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¼‚å¸¸
    print(f"\nå¼‚å¸¸æ£€æŸ¥:")
    print(f"  è´ŸæŒç»­æ—¶é—´: {(pres_data['duration_hours'] < 0).sum()}")
    print(f"  è¶…è¿‡1å¹´: {(pres_data['duration_hours'] > 365*24).sum()}")
    print(f"  å¹´ä»½èŒƒå›´: {pres_data['STARTDATE'].dt.year.min()} - {pres_data['STARTDATE'].dt.year.max()}")
    
    # 7. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    print("\nğŸ’¾ ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")
    output_path = 'data/processed/cleaned_prescriptions.csv'
    pres_data.to_csv(output_path, index=False)
    print(f"æ¸…æ´—åæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜æ¸…æ´—ç»Ÿè®¡
    stats_path = 'data/processed/prescription_cleaning_stats.json'
    with open(stats_path, 'w') as f:
        json.dump(cleaning_stats, f, indent=2)
    print(f"æ¸…æ´—ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_path}")
    
    print("\n" + "=" * 60)
    print("å¤„æ–¹æ•°æ®æ¸…æ´—å®Œæˆï¼")
    print("=" * 60)
    
    return pres_data, cleaning_stats

if __name__ == "__main__":
    import os
    os.chdir('/Users/lulingwei/Desktop/TUM/sem_2/reinforcement learning/ä»£ç /sepsis_rl_system')
    
    cleaned_data, stats = clean_prescription_data()
    
    print("\nå»ºè®®:")
    print("1. ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®é‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ®é›†")
    print("2. æ¸…æ´—åçš„æ•°æ®æ—¶é—´èŒƒå›´æ›´åˆç†ï¼Œåº”è¯¥èƒ½å‡å°‘è¿‡æ»¤ç‡")
    print("3. å¯ä»¥è€ƒè™‘åœ¨create_training_dataset.pyä¸­ä½¿ç”¨cleaned_prescriptions.csv")