#!/usr/bin/env python3
"""
æ•°æ®é›†åˆå¹¶è„šæœ¬
å°† sofa.csv å’Œ final_df.csv è¿›è¡Œåˆå¹¶
"""

import pandas as pd
import numpy as np

# æ–‡ä»¶è·¯å¾„
SOFA_CSV = "/Users/lulingwei/Desktop/TUM/sem_2/reinforcement learning/ä»£ç /sepsis_rl_system/data/raw/sofa.csv"
FINAL_DF_CSV = "/Users/lulingwei/Desktop/TUM/sem_2/reinforcement learning/ä»£ç /sepsis_rl_system/data/raw/final_df.csv"
OUTPUT_CSV = "/Users/lulingwei/Desktop/TUM/sem_2/reinforcement learning/ä»£ç /sepsis_rl_system/data/processed/merged_dataset.csv"

print("=" * 60)
print("å¼€å§‹æ•°æ®é›†åˆå¹¶æµç¨‹")
print("=" * 60)

# æ­¥éª¤1ï¼šè¯»å–æ•°æ®
print("\nğŸ“¥ æ­¥éª¤1ï¼šè¯»å–æ•°æ®")
print("æ­£åœ¨è¯»å– sofa.csv...")
sofa_df = pd.read_csv(SOFA_CSV)
print(f"sofa.csv å½¢çŠ¶: {sofa_df.shape}")
print(f"sofa.csv åˆ—å: {list(sofa_df.columns)}")

print("\næ­£åœ¨è¯»å– final_df.csv...")
final_df = pd.read_csv(FINAL_DF_CSV, parse_dates=['admittime'])
print(f"final_df.csv å½¢çŠ¶: {final_df.shape}")
print(f"final_df.csv åˆ—å: {list(final_df.columns)}")

# æ­¥éª¤2ï¼šæ•°æ®æ¸…ç†
print("\nğŸ§¹ æ­¥éª¤2ï¼šæ•°æ®æ¸…ç†")

# 2.1 æ¸…ç† sofa æ•°æ®
print("æ¸…ç† sofa æ•°æ®...")
sofa_clean = sofa_df.copy()
# ç¡®ä¿å…³é”®åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
sofa_clean['hadm_id'] = sofa_clean['hadm_id'].astype('int64')
sofa_clean['time_step'] = sofa_clean['time_step'].astype('int64')
print(f"sofa æ¸…ç†åå½¢çŠ¶: {sofa_clean.shape}")

# 2.2 æ¸…ç† final_df æ•°æ®
print("æ¸…ç† final_df æ•°æ®...")
final_clean = final_df.copy()
# ç¡®ä¿å…³é”®åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
final_clean['hadm_id'] = final_clean['hadm_id'].astype('int64')
final_clean['time_step'] = final_clean['time_step'].astype('int64')
print(f"final_df æ¸…ç†åå½¢çŠ¶: {final_clean.shape}")

# æ­¥éª¤3ï¼šåˆ†ç¦»é™æ€æ•°æ®å’Œæ—¶åºæ•°æ®
print("\nğŸ”„ æ­¥éª¤3ï¼šåˆ†ç¦»é™æ€æ•°æ®å’Œæ—¶åºæ•°æ®")

# 3.1 è¯†åˆ«é™æ€åˆ—ï¼ˆåœ¨final_dfä¸­ï¼Œå¯¹äºåŒä¸€ä¸ªhadm_idï¼Œè¿™äº›åˆ—çš„å€¼åº”è¯¥æ˜¯ç›¸åŒçš„ï¼‰
# åªä¿ç•™ä¸æ²»ç–—ç›¸å…³çš„é™æ€ç‰¹å¾ï¼ŒåŒ…å«admittimeç”¨äºæ—¶é—´å¯¹é½
static_columns = [
    'subject_id', 'gender', 'age', 'weight_kg', 
    'height_cm', 'hospital_expire_flag', 'admittime'
]

# 3.2 æå–é™æ€æ•°æ®
print("æå–é™æ€æ•°æ®...")
static_data = final_clean[['hadm_id'] + static_columns].drop_duplicates(subset=['hadm_id'])
print(f"é™æ€æ•°æ®å½¢çŠ¶: {static_data.shape}")
print(f"é™æ€æ•°æ®åˆ—å: {list(static_data.columns)}")

# 3.3 ç‰¹å¾æ¸…ç†å’Œç¼–ç 
print("\nğŸ”§ æ­¥éª¤3.3ï¼šç‰¹å¾æ¸…ç†å’Œç¼–ç ")

# åˆ é™¤ä¸æ²»ç–—æ— å…³çš„ç‰¹å¾ (ä¿ç•™admittimeç”¨äºæ—¶é—´å¯¹é½)
columns_to_remove = ['religion', 'language', 'marital_status', 'ethnicity']
print(f"åˆ é™¤ä¸æ²»ç–—æ— å…³çš„ç‰¹å¾: {columns_to_remove}")
print("ä¿ç•™admittimeå­—æ®µç”¨äºåç»­æ—¶é—´å¯¹é½")
existing_cols_to_remove = [col for col in columns_to_remove if col in final_clean.columns]
if existing_cols_to_remove:
    final_clean = final_clean.drop(columns=existing_cols_to_remove)
    print(f"å·²åˆ é™¤åˆ—: {existing_cols_to_remove}")
    print(f"final_cleanæ¸…ç†åå½¢çŠ¶: {final_clean.shape}")
else:
    print("æœªæ‰¾åˆ°éœ€è¦åˆ é™¤çš„åˆ—")

print("å¯¹genderè¿›è¡ŒäºŒè¿›åˆ¶ç¼–ç ...")
print(f"Genderç¼–ç å‰åˆ†å¸ƒ: {static_data['gender'].value_counts().to_dict()}")
# å¯¹genderè¿›è¡ŒäºŒè¿›åˆ¶ç¼–ç  (M=1, F=0)
static_data['gender'] = static_data['gender'].map({'M': 1, 'F': 0})
print(f"Genderç¼–ç ååˆ†å¸ƒ: {static_data['gender'].value_counts().to_dict()}")

# æ£€æŸ¥æ˜¯å¦æœ‰æœªæˆåŠŸç¼–ç çš„æ•°æ®
gender_missing = static_data['gender'].isnull().sum()
if gender_missing > 0:
    print(f"âš ï¸  è­¦å‘Š: æœ‰ {gender_missing} ä¸ªgenderå€¼æœªèƒ½æˆåŠŸç¼–ç ")

print("âœ… ç‰¹å¾æ¸…ç†å’Œç¼–ç å®Œæˆ")

# 3.4 è¯†åˆ«æ—¶åºåˆ—
final_temporal_columns = [col for col in final_clean.columns if col not in static_columns]
sofa_temporal_columns = [col for col in sofa_clean.columns]

print(f"final_df æ—¶åºåˆ—æ•°é‡: {len(final_temporal_columns)}")
print(f"sofa æ—¶åºåˆ—æ•°é‡: {len(sofa_temporal_columns)}")

# æ­¥éª¤4ï¼šå‡†å¤‡æ—¶åºæ•°æ®åˆå¹¶
print("\nğŸ”— æ­¥éª¤4ï¼šå‡†å¤‡æ—¶åºæ•°æ®åˆå¹¶")

# 4.1 ä»final_dfä¸­æå–æ—¶åºæ•°æ®
final_temporal = final_clean[final_temporal_columns].copy()
print(f"final_df æ—¶åºæ•°æ®å½¢çŠ¶: {final_temporal.shape}")

# 4.2 sofaæ•°æ®å·²ç»æ˜¯æ—¶åºæ•°æ®
sofa_temporal = sofa_clean.copy()
print(f"sofa æ—¶åºæ•°æ®å½¢çŠ¶: {sofa_temporal.shape}")

# æ­¥éª¤5ï¼šåˆå¹¶æ—¶åºæ•°æ®
print("\nğŸ”„ æ­¥éª¤5ï¼šåˆå¹¶æ—¶åºæ•°æ®")

# æ£€æŸ¥é‡å çš„åˆ—ï¼ˆé™¤äº†hadm_idå’Œtime_stepï¼‰
common_cols = set(final_temporal.columns) & set(sofa_temporal.columns)
merge_keys = {'hadm_id', 'time_step'}
overlap_cols = common_cols - merge_keys
if overlap_cols:
    print(f"âš ï¸  å‘ç°é‡å åˆ—: {overlap_cols}")
    # å¯¹é‡å åˆ—æ·»åŠ åç¼€ä»¥åŒºåˆ†
    sofa_temporal = sofa_temporal.rename(columns={col: f"{col}_sofa" for col in overlap_cols})

# æ‰§è¡Œå·¦è¿æ¥ï¼ˆä»¥final_dfä¸ºä¸»ï¼‰
print("æ‰§è¡Œæ—¶åºæ•°æ®åˆå¹¶...")
merged_temporal = final_temporal.merge(
    sofa_temporal, 
    on=['hadm_id', 'time_step'], 
    how='left',
    suffixes=('', '_sofa')
)
print(f"åˆå¹¶åæ—¶åºæ•°æ®å½¢çŠ¶: {merged_temporal.shape}")

# æ­¥éª¤6ï¼šä¸é™æ€æ•°æ®è¿æ¥
print("\nğŸ”— æ­¥éª¤6ï¼šä¸é™æ€æ•°æ®è¿æ¥")

# å°†åˆå¹¶åçš„æ—¶åºæ•°æ®ä¸é™æ€æ•°æ®è¿›è¡Œå·¦è¿æ¥
final_merged = merged_temporal.merge(
    static_data,
    on='hadm_id',
    how='left'
)
print(f"æœ€ç»ˆåˆå¹¶æ•°æ®å½¢çŠ¶: {final_merged.shape}")

# æ­¥éª¤7ï¼šæ•°æ®è´¨é‡æ£€æŸ¥
print("\nğŸ” æ­¥éª¤7ï¼šæ•°æ®è´¨é‡æ£€æŸ¥")

print("æ£€æŸ¥åˆå¹¶åçš„æ•°æ®è´¨é‡...")
print(f"æ€»è¡Œæ•°: {len(final_merged)}")
print(f"å”¯ä¸€hadm_idæ•°é‡: {final_merged['hadm_id'].nunique()}")
print(f"time_stepèŒƒå›´: {final_merged['time_step'].min()} åˆ° {final_merged['time_step'].max()}")

# æ£€æŸ¥ç¼ºå¤±å€¼
missing_counts = final_merged.isnull().sum()
high_missing = missing_counts[missing_counts > len(final_merged) * 0.5]
if not high_missing.empty:
    print(f"âš ï¸  é«˜ç¼ºå¤±ç‡åˆ— (>50%): {high_missing.to_dict()}")

# æ­¥éª¤8ï¼šä¿å­˜ç»“æœ
print("\nğŸ’¾ æ­¥éª¤8ï¼šä¿å­˜ç»“æœ")

# åˆ›å»ºè¾“å‡ºç›®å½•
import os
os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

# ä¿å­˜åˆå¹¶åçš„æ•°æ®
final_merged.to_csv(OUTPUT_CSV, index=False)
print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {OUTPUT_CSV}")

# ä¿å­˜æ•°æ®æ‘˜è¦
summary_file = OUTPUT_CSV.replace('.csv', '_summary.txt')
with open(summary_file, 'w') as f:
    f.write("æ•°æ®åˆå¹¶æ‘˜è¦\n")
    f.write("=" * 50 + "\n\n")
    f.write(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {final_merged.shape}\n")
    f.write(f"åˆ—æ•°: {len(final_merged.columns)}\n")
    f.write(f"è¡Œæ•°: {len(final_merged)}\n")
    f.write(f"å”¯ä¸€hadm_idæ•°é‡: {final_merged['hadm_id'].nunique()}\n")
    f.write(f"time_stepèŒƒå›´: {final_merged['time_step'].min()} åˆ° {final_merged['time_step'].max()}\n\n")
    f.write("ç‰¹å¾æ¸…ç†è¯´æ˜:\n")
    f.write("- å·²åˆ é™¤ä¸æ²»ç–—æ— å…³çš„ç‰¹å¾: religion, language, marital_status, ethnicity\n")
    f.write("- å·²å¯¹genderè¿›è¡ŒäºŒè¿›åˆ¶ç¼–ç  (M=1, F=0)\n")
    f.write("- ä¿ç•™çš„é™æ€ç‰¹å¾: subject_id, gender, age, weight_kg, height_cm, hospital_expire_flag, admittime\n")
    f.write("- admittimeå­—æ®µä¿ç•™ç”¨äºåç»­è¯ç‰©æ•°æ®æ—¶é—´å¯¹é½\n\n")
    f.write("åˆ—ååˆ—è¡¨:\n")
    for i, col in enumerate(final_merged.columns, 1):
        f.write(f"{i:3d}. {col}\n")
    f.write("\nç¼ºå¤±å€¼ç»Ÿè®¡:\n")
    for col, count in missing_counts.items():
        if count > 0:
            f.write(f"{col}: {count} ({count/len(final_merged)*100:.1f}%)\n")

print(f"âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")

print("\n" + "=" * 60)
print("æ•°æ®åˆå¹¶å®Œæˆï¼")
print("=" * 60)